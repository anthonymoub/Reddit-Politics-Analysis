{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469a0cbd-1377-4671-9fb8-37401cc0af7a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d1254c-0d13-4fa8-b86a-7602c80c8a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.10.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - openjdk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.08.22 |       h06a4308_0         123 KB\n",
      "    certifi-2023.11.17         |  py310h06a4308_0         158 KB\n",
      "    openjdk-11.0.13            |       h87a67e3_0       341.0 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       341.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  openjdk            pkgs/main/linux-64::openjdk-11.0.13-h87a67e3_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2023.7.2~ --> pkgs/main::ca-certificates-2023.08.22-h06a4308_0 \n",
      "  certifi            conda-forge/noarch::certifi-2023.7.22~ --> pkgs/main/linux-64::certifi-2023.11.17-py310h06a4308_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ca-certificates-2023 | 123 KB    |                                       |   0% \n",
      "certifi-2023.11.17   | 158 KB    |                                       |   0% \u001b[A\n",
      "\n",
      "ca-certificates-2023 | 123 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | 1                                     |   0% \u001b[A\u001b[A\n",
      "certifi-2023.11.17   | 158 KB    | ##################################### | 100% \u001b[A\n",
      "certifi-2023.11.17   | 158 KB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #                                     |   3% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #9                                    |   5% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##9                                   |   8% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ###7                                  |  10% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ####8                                 |  13% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #####8                                |  16% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ######8                               |  18% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #######8                              |  21% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #########                             |  25% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##########7                           |  29% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ############2                         |  33% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #############8                        |  38% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ###############5                      |  42% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #################2                    |  47% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##################8                   |  51% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ####################5                 |  56% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ######################1               |  60% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #######################7              |  64% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #########################5            |  69% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ###########################1          |  73% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ############################8         |  78% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##############################5       |  83% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ################################1     |  87% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #################################8    |  91% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ###################################3  |  95% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ####################################7 |  99% \u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyspark==3.4.0\n",
      "  Using cached pyspark-3.4.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.4.0)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting spark-nlp==5.1.3\n",
      "  Obtaining dependency information for spark-nlp==5.1.3 from https://files.pythonhosted.org/packages/cd/7d/bc0eca4c9ec4c9c1d9b28c42c2f07942af70980a7d912d0aceebf8db32dd/spark_nlp-5.1.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached spark_nlp-5.1.3-py2.py3-none-any.whl.metadata (53 kB)\n",
      "Using cached spark_nlp-5.1.3-py2.py3-none-any.whl (537 kB)\n",
      "Installing collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install openjdk -y\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.4.0\n",
    "\n",
    "# install spark-nlp\n",
    "%pip install spark-nlp==5.1.3\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aace35-7213-4a8f-9fe4-ad5073484158",
   "metadata": {},
   "source": [
    "# Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009680b3-9eac-4e4e-aec9-e8c071c1687d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6d8ece39-5cae-4e0e-aad3-a1bb4dd7d1e2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 537ms :: artifacts dl 42ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6d8ece39-5cae-4e0e-aad3-a1bb4dd7d1e2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/25ms)\n",
      "23/11/30 02:50:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3432de79-c9ee-4971-a671-6335316bdefe",
   "metadata": {},
   "source": [
    "# Load in Comments Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d696120d-15b2-4bdf-81dc-85ff0535c1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/29 19:19:23 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Tegveer's S3 -- DO NOT CHANGE\n",
    "s3_directory_comms = f\"s3a://sagemaker-us-east-1-433974840707/project/nlp_cleaned_comments/\"\n",
    "\n",
    "# Read all the Parquet files in the directory into a DataFrame\n",
    "df_comments = spark.read.parquet(s3_directory_comms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5e318-8337-4bca-80d6-291b5d121f73",
   "metadata": {},
   "source": [
    "## Pre-process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0130e8c9-1b25-4068-a1ad-c0a843e7f03c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change `controversiality` data type from int to str, get year and month cols from `created_utc`, drop [removed] and [deleted] comments \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df_comments = df_comments.withColumn(\"controversiality\", \n",
    "                                                    df_comments[\"controversiality\"] \n",
    "                                                    .cast(StringType())) \\\n",
    "                                         .withColumn('year', F.year('created_utc')) \\\n",
    "                                         .withColumn('month', F.month('created_utc')) \\\n",
    "                                         .withColumn ('day', F.dayofmonth(\"created_utc\")) \\\n",
    "                                         .withColumn('distinguished', F.when(df_comments['distinguished'] == 'moderator', 'yes')\n",
    "                                                     .otherwise(df_comments['distinguished']))\n",
    "df_comments = df_comments.fillna({'distinguished': \"no\"})\n",
    "\n",
    "pattern = r\"\\[removed\\]|\\[deleted\\]\"\n",
    "\n",
    "df_comments = df_comments.filter(~(F.col(\"body\").rlike(pattern) | F.col(\"author\").rlike(pattern)))\n",
    "\n",
    "# select required cols for ML\n",
    "\n",
    "df_comments = df_comments.select(\"controversiality\", \"distinguished\", \"subreddit\", \"year\", \"month\", \"day\", \"gilded\", \"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4906f32-9a9f-41d8-92db-3cc6d1849801",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:========================================================>(63 + 1) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in sampled and filtered df:  13242001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of records in sampled and filtered df: \", df_comments.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36973a06-bb9f-4815-8513-0bceae3f88c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:========================================================>(63 + 1) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|controversiality|   count|\n",
      "+----------------+--------+\n",
      "|               0|12311780|\n",
      "|               1|  930221|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_comments.groupby('controversiality').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fe71bd7-89d3-4c11-b8a0-1037309e53d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- controversiality: string (nullable = true)\n",
      " |-- distinguished: string (nullable = false)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- gilded: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_comments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f10d5f-09ec-4074-9f39-86cff13bb30e",
   "metadata": {},
   "source": [
    "## Adding `weight` column "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9abcb-d2ca-486d-831b-4e5c18d09bad",
   "metadata": {},
   "source": [
    "https://danvatterott.com/blog/2019/11/18/balancing-model-weights-in-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "699e6eff-1446-4773-8fb5-47c7071638c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:========================================================>(63 + 1) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0.5377776812126273, '1': 7.117663974474882}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_collect = df_comments.select(\"controversiality\").groupBy(\"controversiality\").count().collect()\n",
    "unique_y = [x[\"controversiality\"] for x in y_collect]\n",
    "total_y = sum([x[\"count\"] for x in y_collect])\n",
    "unique_y_count = len(y_collect)\n",
    "bin_count = [x[\"count\"] for x in y_collect]\n",
    "\n",
    "class_weights_spark = {i: ii for i, ii in zip(unique_y, total_y / (unique_y_count * np.array(bin_count)))}\n",
    "print(class_weights_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a6e1623-93e9-4cd0-964b-5189c06ccd76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pyspark/sql/column.py:458: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "mapping_expr = F.create_map([F.lit(x) for x in chain(*class_weights_spark.items())])\n",
    "\n",
    "df_comments = df_comments.withColumn(\"weight\", mapping_expr.getItem(F.col(\"controversiality\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78798fd-9641-425b-a2ab-a8585e802127",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save DataFrame to Tegveer's S3\n",
    "s3_bucket = f\"s3a://sagemaker-us-east-1-433974840707/project/ml_comments/\"\n",
    "\n",
    "# Write DataFrame to S3 in Parquet format\n",
    "df_comments.write.mode(\"overwrite\").parquet(s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49ad7f5-827b-47a6-b794-63f2a77730cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE cleaned/\n",
      "                           PRE comments/\n",
      "                           PRE ml/\n",
      "                           PRE ml_comments/\n",
      "                           PRE nlp/\n",
      "                           PRE nlp_cleaned_comments/\n",
      "                           PRE nlp_cleaned_submissions/\n",
      "                           PRE sentiment/\n",
      "                           PRE submissions/\n",
      "2023-11-16 19:15:27  708534094 spark-nlp-assembly-5.1.3.jar\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "! aws s3 ls s3://sagemaker-us-east-1-433974840707/project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bdb6208-f5e8-4175-ba30-c45e8f56ca9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 19:26:24          0 _SUCCESS\n",
      "2023-11-29 19:25:43     594198 part-00000-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:44     589362 part-00001-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:44     585372 part-00002-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:45     594971 part-00003-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:46     589538 part-00004-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:46     589704 part-00005-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:47     588680 part-00006-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:48     588935 part-00007-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:48     596708 part-00008-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:49     592220 part-00009-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:50     592149 part-00010-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:50     590467 part-00011-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:51     592791 part-00012-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:52     596045 part-00013-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:52     591944 part-00014-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:53     598898 part-00015-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:53     589267 part-00016-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:54     595070 part-00017-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:55     598616 part-00018-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:55     595925 part-00019-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:56     591923 part-00020-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:57     590291 part-00021-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:57     594049 part-00022-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:58     585946 part-00023-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:59     592030 part-00024-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:25:59     592298 part-00025-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:00     590378 part-00026-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:01     596147 part-00027-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:01     591614 part-00028-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:02     589628 part-00029-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:02     592963 part-00030-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:03     597448 part-00031-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:04     598247 part-00032-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:04     596369 part-00033-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:05     588168 part-00034-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:06     593327 part-00035-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:06     590642 part-00036-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:07     592382 part-00037-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:08     592080 part-00038-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:09     586510 part-00039-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:09     593508 part-00040-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:10     594475 part-00041-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:10     594784 part-00042-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:11     589972 part-00043-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:12     598985 part-00044-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:12     594861 part-00045-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:13     593997 part-00046-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:13     594913 part-00047-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:14     590775 part-00048-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:15     588999 part-00049-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:15     593737 part-00050-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:16     590286 part-00051-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:16     592259 part-00052-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:17     588594 part-00053-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:17     594006 part-00054-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:18     595317 part-00055-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:19     592238 part-00056-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:19     590498 part-00057-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:20     591853 part-00058-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:21     594221 part-00059-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:21     593457 part-00060-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:22     595649 part-00061-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:22     586560 part-00062-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n",
      "2023-11-29 19:26:23     589307 part-00063-8f86faa0-d114-46be-9005-e82cbea8abbd-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "! aws s3 ls s3://sagemaker-us-east-1-433974840707/project/ml_comments/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb003fb5-e22c-44a1-b8ad-ad502181d640",
   "metadata": {},
   "source": [
    "# Preparing features for text classification - Comments Dataset Only (as Submissions doesn't have `controversiality`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2803bdc-cae2-410c-aa3f-6c788437753f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./controversial_processing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./controversial_processing.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "import json\n",
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, NaiveBayes, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--ml_model\", type=str, help=\"Model used for Classification\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "    logger.info(f\"args={args}\")\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Spark ML\")\\\n",
    "        .config(\"spark.driver.memory\",\"16G\")\\\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"sparknlp version: {sparknlp.version()}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "    \n",
    "    # get model\n",
    "    ml_model = args.ml_model\n",
    "    \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True)\n",
    "    \n",
    "    stringIndexer_controversiality = StringIndexer(inputCol=\"controversiality\", outputCol=\"controversiality_str\")\n",
    "    stringIndexer_distinguished = StringIndexer(inputCol=\"distinguished\", outputCol=\"distinguished_ix\")\n",
    "    stringIndexer_subreddit = StringIndexer(inputCol=\"subreddit\", outputCol=\"subreddit_ix\")\n",
    "    \n",
    "    logger.info(f\"Applying stringIndexer_controversiality to the dataframe\")\n",
    "    indexed = stringIndexer_controversiality.fit(df).transform(df)\n",
    "\n",
    "    logger.info(f\"Applying stringIndexer_distinguished to the new dataframe `indexed`\")\n",
    "    indexed = stringIndexer_distinguished.fit(indexed).transform(indexed)\n",
    "\n",
    "    logger.info(f\"Applying stringIndexer_subreddit to the new dataframe `indexed`\")\n",
    "    indexed = stringIndexer_subreddit.fit(indexed).transform(indexed)\n",
    "    \n",
    "    logger.info(f\"One-hot Encoding\")\n",
    "    onehot_subreddit = OneHotEncoder(inputCol=\"subreddit_ix\", outputCol=\"subreddit_vec\")\n",
    "    \n",
    "    logger.info(f\"Vector assembling features\")\n",
    "    vectorAssembler_features = VectorAssembler(inputCols=['distinguished_ix', 'year', 'month', 'day', 'score', 'gilded', 'subreddit_ix'], # spark processing job throws algorithmerror when subreddit_vec column is included in VectorAssembler  \n",
    "                                               outputCol= 'features')\n",
    "    \n",
    "    logger.info(f\"Creating new vectorized features df\")\n",
    "    features_vectorized = vectorAssembler_features.transform(indexed) # note this is a new df\n",
    "    \n",
    "    \n",
    "    logger.info(f\"Creating {ml_model}\")\n",
    "    if ml_model == \"rf\":\n",
    "        pipeline_model = RandomForestClassifier(labelCol=\"controversiality_str\", featuresCol=\"features\", numTrees=30, weightCol=\"weight\")\n",
    "    elif ml_model == \"lr\":\n",
    "        pipeline_model = LogisticRegression(labelCol=\"controversiality_str\", featuresCol=\"features\", maxIter=10, weightCol=\"weight\")\n",
    "    elif ml_model == \"gbt\":\n",
    "        pipeline_model = GBTClassifier(labelCol=\"controversiality_str\", featuresCol=\"features\", maxIter=10, weightCol=\"weight\")\n",
    "    elif ml_model == \"svm\":\n",
    "        pipeline_model = LinearSVC(labelCol=\"controversiality_str\", featuresCol=\"features\", maxIter=10, weightCol=\"weight\")\n",
    "    \n",
    "    logger.info(f\"Creating Label Converter\")\n",
    "    labelConverter = IndexToString(inputCol=\"prediction\", \n",
    "                               outputCol=\"predictedControversiality\", \n",
    "                               labels=[\"0\", \"1\"])\n",
    "    \n",
    "    logger.info(f\"Creating Pipeline\")\n",
    "    pipeline_model = Pipeline(stages=\n",
    "                              [stringIndexer_controversiality,\n",
    "                               stringIndexer_distinguished,  \n",
    "                               stringIndexer_subreddit,\n",
    "                               onehot_subreddit,\n",
    "                               vectorAssembler_features, \n",
    "                               pipeline_model, \n",
    "                               labelConverter]\n",
    "                             )\n",
    "\n",
    "    logger.info(f\"Split data into train, test, and validation\")\n",
    "    train_data, test_data = df.randomSplit([0.75, 0.25], 24)\n",
    "    logger.info(\"Number of training records: \" + str(train_data.count()))\n",
    "    logger.info(\"Number of testing records : \" + str(test_data.count()))\n",
    "    \n",
    "    logger.info(f\"going to fit pipeline on train dataframe\")\n",
    "    model = pipeline_model.fit(train_data)\n",
    "    \n",
    "    # save the model\n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_key_prefix}\"\n",
    "    logger.info(f\"going to save model in {s3_path}\")\n",
    "    model.save(f\"{s3_path}/{ml_model}.model\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b26a40ae-99fc-4295-89b2-6811e49a9408",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project-ml\",\n",
    "    image_uri=f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark:latest\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=7200,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "output_prefix = f\"project/ml_updated\"\n",
    "output_prefix_logs = f\"spark_logs/ml_updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "462e256b-4451-47f5-8ad7-c8789f8d275b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Comments for model rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-ml-2023-11-30-04-06-37-362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-ml-2023-11-30-04-16-32-586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Comments for model lr\n",
      "...........................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-ml-2023-11-30-04-26-37-647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Comments for model gbt\n",
      ".........................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-ml-2023-11-30-04-36-32-377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Comments for model svm\n",
      "................................................................................................!"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "models = ['rf', 'lr', 'gbt', 'svm']\n",
    "\n",
    "for model in models:\n",
    "    # comments\n",
    "    print(f\"Working on Comments for model {model}\")\n",
    "    spark_processor.run(\n",
    "        submit_app=\"./controversial_processing.py\",\n",
    "        arguments=[\n",
    "            \"--ml_model\",\n",
    "            model,\n",
    "            \"--s3_dataset_path\",\n",
    "            f\"s3://sagemaker-us-east-1-433974840707/project/ml_comments/\",\n",
    "            \"--s3_output_bucket\",\n",
    "            \"sagemaker-us-east-1-433974840707\",\n",
    "            \"--s3_output_key_prefix\",\n",
    "            f\"{output_prefix}/{model}/\",\n",
    "        ],\n",
    "        spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "        logs=False,\n",
    "    )\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fedac61a-d9c5-47e0-b648-b27289da4509",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE cleaned/\n",
      "                           PRE comments/\n",
      "                           PRE ml/\n",
      "                           PRE ml_comments/\n",
      "                           PRE nlp/\n",
      "                           PRE nlp_cleaned_comments/\n",
      "                           PRE nlp_cleaned_submissions/\n",
      "                           PRE sentiment/\n",
      "                           PRE submissions/\n",
      "2023-11-16 19:15:27  708534094 spark-nlp-assembly-5.1.3.jar\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "! aws s3 ls s3://sagemaker-us-east-1-433974840707/project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19c1fcc2-de09-473e-8d0d-260959dd5c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE gbt/\n",
      "                           PRE lr/\n",
      "                           PRE rf/\n",
      "                           PRE svm/\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "! aws s3 ls s3://sagemaker-us-east-1-433974840707/project/ml/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a4267-f9b7-41a6-b1c8-e18ce261228a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
