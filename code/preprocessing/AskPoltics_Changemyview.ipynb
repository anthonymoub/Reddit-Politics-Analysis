{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing large datasets with Apache Spark and Amazon SageMaker\n",
    "\n",
    "***This notebook run on `Data Science 3.0 - Python 3` kernel on a `ml.t3.large` instance***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Processing Jobs are used  to analyze data and evaluate machine learning models on Amazon SageMaker. With Processing, you can use a simplified, managed experience on SageMaker to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. You can also use the Amazon SageMaker Processing APIs during the experimentation phase and after the code is deployed in production to evaluate performance.\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/Processing-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding diagram shows how Amazon SageMaker spins up a Processing job. Amazon SageMaker takes your script, copies your data from Amazon Simple Storage Service (Amazon S3), and then pulls a processing container. The processing container image can either be an Amazon SageMaker built-in image or a custom image that you provide. The underlying infrastructure for a Processing job is fully managed by Amazon SageMaker. Cluster resources are provisioned for the duration of your job, and cleaned up when a job completes. The output of the Processing job is stored in the Amazon S3 bucket you specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our workflow for processing large amounts of data with SageMaker\n",
    "\n",
    "We can divide our workflow into two steps:\n",
    "    \n",
    "1. Work with a small subset of the data with Spark running in local model in a SageMaker Studio Notebook.\n",
    "\n",
    "1. Once we are able to work with the small subset of data we can provide the same code (as a Python script rather than a series of interactive steps) to SageMaker Processing which launched a Spark cluster, runs out code and terminates the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook...\n",
    "\n",
    "We will analyze the [Pushshift Reddit dataset](https://arxiv.org/pdf/2001.08435.pdf) to be used for the project and then we will run a SageMaker Processing Job to filter out the comments and submissions from subreddits of interest. The filtered data will be stored in your account's s3 bucket and it is this filtered data that you will be using for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We need an available Java installation to run pyspark. The easiest way to do this is to install JDK and set the proper paths using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.9.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.9.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "IOStream.flush timed out\n",
      "Requirement already satisfied: pyspark==3.3.0 in /opt/conda/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.10/site-packages (from pyspark==3.3.0) (0.10.9.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "# Setup - Run only once per Kernel App\n",
    "%conda install openjdk -y\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.3.0\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilize S3 Data within local PySpark\n",
    "* By specifying the `hadoop-aws` jar in our Spark config we're able to access S3 datasets using the s3a file prefix. \n",
    "* Since we've already authenticated ourself to SageMaker Studio , we can use our assumed SageMaker ExecutionRole for any S3 reads/writes by setting the credential provider as `ContainerCredentialsProvider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Error while sending or receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "INFO:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/clientserver.py\", line 506, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data into a Spark Dataframe\n",
    "Note that we will be using the \"s3a\" adapter (read more [here](https://aws.amazon.com/blogs/opensource/community-collaboration-the-s3a-story)). S3A enables Hadoop to directly read and write Amazon S3 objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process S3 data with SageMaker Processing Job `PySparkProcessor`\n",
    "\n",
    "We are going to move the above processing code in a Python file and then submit that file to SageMaker Processing Job's [`PySparkProcessor`](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#pysparkprocessor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./process_askpolitics_changemyview.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./process_askpolitics_changemyview.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path_commments\", type=str, help=\"Path of dataset in S3 for reddit comments\")\n",
    "    parser.add_argument(\"--s3_dataset_path_submissions\", type=str, help=\"Path of dataset in S3 for reddit submissions\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n",
    "    parser.add_argument(\"--col_name_for_filtering\", type=str, help=\"Name of the column to filter\")\n",
    "    # parser.add_argument(\"--subreddits\", type=str, help=\"comma separate list of subreddits of interest\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "    logger.info(f\"spark version = {spark.version}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path_commments}\")\n",
    "    comments = spark.read.parquet(args.s3_dataset_path_commments, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    logger.info(f\"going to read {args.s3_dataset_path_submissions}\")\n",
    "    submissions = spark.read.parquet(args.s3_dataset_path_submissions, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # filter the dataframe to only keep the subreddits of interest\n",
    "    subreddits = [s.strip() for s in args.subreddits.split(\",\")]\n",
    "    submissions_filtered = submissions.where(col(\"subreddit\").isin(subreddits)) # removing lower()\n",
    "    comments_filtered = comments.where(col(\"subreddit\").isin(subreddits)) # removing lower()\n",
    "    \n",
    "    # save the filtered dataframes so that these files can now be used for future analysis\n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/comments\"\n",
    "    logger.info(f\"going to write comments for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the comments_filtered dataframe is {comments_filtered.count():,}x{len(comments_filtered.columns)}\")\n",
    "    comments_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/submissions\"\n",
    "    logger.info(f\"going to write submissions for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the submissions_filtered dataframe is {submissions_filtered.count():,}x{len(submissions_filtered.columns)}\")\n",
    "    submissions_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now submit this code to SageMaker Processing Job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below was repeated manually (because of the issues we were facing) for different combinations of year and month to gatherall the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eric's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "import time\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=7200,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "col_name_for_filtering = \"subreddit\"\n",
    "\n",
    "# modify this comma separated list to choose the subreddits of interest\n",
    "subreddits = \"changemyview, Ask_Politics\" \n",
    "configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\"spark.executor.memory\": \"12g\", \"spark.executor.cores\": \"4\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "years = [2021, 2022, 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-10-30-18-53-01-767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Comments for year 2021\n",
      ".......................................................................................................................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-10-30-19-12-11-270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Submissions for year 2021\n",
      ".............................................................................................................................................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-10-30-19-33-12-183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Comments for year 2022\n",
      "....................................................................................................................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-10-30-19-52-06-375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Submissions for year 2022\n",
      "......................................................................................................................................................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-10-30-20-13-52-119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Comments for year 2023\n",
      "...................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-10-30-20-24-37-382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Submissions for year 2023\n",
      ".................................................................................................................................!"
     ]
    }
   ],
   "source": [
    "for year in years:    \n",
    "    # comments\n",
    "    print(f\"Working on Comments for year {year}\")\n",
    "    \n",
    "    s3_dataset_path_commments = f\"s3://bigdatateaching/reddit-parquet/comments/year={year}/month=*/*.parquet\" \n",
    "    output_prefix_data_comments = f\"project/comments/year={year}\"\n",
    "\n",
    "    spark_processor.run(\n",
    "        submit_app=\"./process_askpolitics_changemyview.py\",\n",
    "        arguments=[\n",
    "            \"--s3_dataset_path\",\n",
    "            s3_dataset_path_commments,\n",
    "            \"--s3_output_bucket\",\n",
    "            bucket,\n",
    "            \"--s3_output_prefix\",\n",
    "            output_prefix_data_comments,\n",
    "            \"--col_name_for_filtering\",\n",
    "            col_name_for_filtering,\n",
    "            \"--values_to_keep\",\n",
    "            subreddits,\n",
    "        ],\n",
    "        spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "        logs=False,\n",
    "        configuration=configuration\n",
    "    )\n",
    "\n",
    "    time.sleep(60)\n",
    "    \n",
    "    # submissions\n",
    "    print(f\"Working on Submissions for year {year}\")\n",
    "    \n",
    "    s3_dataset_path_submissions = f\"s3://bigdatateaching/reddit-parquet/submissions/year={year}/month=*/*.parquet\"\n",
    "    output_prefix_data_submissions = f\"project/submissions/year={year}\"\n",
    "\n",
    "    spark_processor.run(\n",
    "        submit_app=\"./process_askpolitics_changemyview.py\",\n",
    "        arguments=[\n",
    "             \"--s3_dataset_path\",\n",
    "            s3_dataset_path_submissions,\n",
    "            \"--s3_output_bucket\",\n",
    "            bucket,\n",
    "            \"--s3_output_prefix\",\n",
    "            output_prefix_data_submissions,\n",
    "            \"--col_name_for_filtering\",\n",
    "            col_name_for_filtering,\n",
    "            \"--values_to_keep\",\n",
    "            subreddits,\n",
    "        ],\n",
    "        spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "        logs=False,\n",
    "        configuration=configuration\n",
    "    )\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-10-29-22-26-44-898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................!CPU times: user 3.4 s, sys: 350 ms, total: 3.75 s\n",
      "Wall time: 9min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=6,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=7200,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_dataset_path_commments = \"s3://bigdatateaching/reddit-parquet/comments/year=2022/month=9/*.parquet\" \n",
    "s3_dataset_path_submissions = \"s3://bigdatateaching/reddit-parquet/submissions/year=2022/month=9/*.parquet\" \n",
    "output_prefix_data = \"project\"\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "# modify this comma separated list to choose the subreddits of interest\n",
    "subreddits = \"changemyview, Ask_Politics\"\n",
    "    \n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"./process_askpolitics_changemyview.py\",\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path_commments\",\n",
    "        s3_dataset_path_commments,\n",
    "        \"--s3_dataset_path_submissions\",\n",
    "        s3_dataset_path_submissions,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_prefix\",\n",
    "        output_prefix_data,\n",
    "        \"--subreddits\",\n",
    "        subreddits,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the filtered data\n",
    "\n",
    "Now that we have filtered the data to only keep submissions and comments from subreddits of interest. Let us read data from the s3 path where we saved the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "CPU times: user 363 ms, sys: 0 ns, total: 363 ms\n",
      "Wall time: 610 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=6,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=7200,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "output_prefix_data = \"project\"\n",
    "output_prefix_logs = f\"spark_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading comments from s3a://sagemaker-us-east-1-711387073580/project/comments\n",
      "23/10/30 14:41:04 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 137,855x21\n",
      "CPU times: user 33.8 ms, sys: 900 µs, total: 34.7 ms\n",
      "Wall time: 1min 6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data}/comments\"\n",
    "print(f\"reading comments from {s3_path}\")\n",
    "comments = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|   subreddit| count|\n",
      "+------------+------+\n",
      "|changemyview|137444|\n",
      "|Ask_Politics|   411|\n",
      "+------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check counts (ensuring all needed subreddits exist)\n",
    "comments.groupBy('subreddit').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- can_gild: boolean (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: timestamp (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_submitter: boolean (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- retrieved_on: timestamp (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+--------------------+----------+---------+-------+-------------------+\n",
      "|   subreddit|           author|                body| parent_id|  link_id|     id|        created_utc|\n",
      "+------------+-----------------+--------------------+----------+---------+-------+-------------------+\n",
      "|Conservative|   Thrownaway1211|         2nd dumbest|t1_gjyohnx|t3_l19aok|gjzhisd|2021-01-20 20:44:29|\n",
      "|Conservative|        [deleted]|           [deleted]|t1_gjzfpyh|t3_l1hhgw|gjzhite|2021-01-20 20:44:30|\n",
      "|Conservative|        [deleted]|           [removed]|t1_gjzdkd4|t3_l1dlf1|gjzhiuy|2021-01-20 20:44:30|\n",
      "|Conservative|        premer777|God helps those w...|t1_gjzd3i6|t3_l19aok|gjzhivc|2021-01-20 20:44:30|\n",
      "|Conservative|    Barnyard_Rich|&gt; This country...|t1_gjzax9z|t3_l1g3b9|gjzhiwu|2021-01-20 20:44:31|\n",
      "|Conservative|     sailor-jackn|We’re not just ge...|t1_gjzb8mv|t3_l1dlf1|gjzhixn|2021-01-20 20:44:31|\n",
      "|Conservative|        [deleted]|You just might be...|t1_gjzgw3m|t3_l1fxyh|gjzhiyc|2021-01-20 20:44:31|\n",
      "|Conservative|     lulskadoodle|These are all goo...| t3_l19aok|t3_l19aok|gjzhj0m|2021-01-20 20:44:32|\n",
      "| Libertarian| No_Consequences_|This is not what ...|t1_gjxrxoy|t3_l0zgze|gjzhj1n|2021-01-20 20:44:32|\n",
      "|Conservative|        [deleted]|           [deleted]|t1_gjzh1a0|t3_l1h8v7|gjzhj1u|2021-01-20 20:44:32|\n",
      "| Libertarian|        [deleted]|           [deleted]|t1_gjzhcc4|t3_l0oyxu|gjzhj5a|2021-01-20 20:44:34|\n",
      "|Conservative|    AutoModerator|Looking for debat...| t3_l1hv88|t3_l1hv88|gjzhj5u|2021-01-20 20:44:34|\n",
      "|Conservative|        [deleted]|Please stop sayin...|t1_gjyhdqz|t3_l199d1|gjzhj7r|2021-01-20 20:44:34|\n",
      "|Conservative|        [deleted]|           [removed]| t3_l1d0r7|t3_l1d0r7|gjzhj7w|2021-01-20 20:44:34|\n",
      "|Conservative|    DanPlaysMusic|Why do you suppor...|t1_gjzel12|t3_l1dlf1|gjzhj9s|2021-01-20 20:44:35|\n",
      "| Libertarian|    iushciuweiush|Ignorance is blis...|t1_gjz5cl0|t3_l1efor|gjzhjan|2021-01-20 20:44:35|\n",
      "|Conservative|          mk21dvr|You forgot the \"/s\".|t1_gjzejva|t3_l1eoiy|gjzhjb5|2021-01-20 20:44:36|\n",
      "|Conservative|        [deleted]|           [removed]|t1_gjz9dbo|t3_l1e03j|gjzhjeq|2021-01-20 20:44:37|\n",
      "|Conservative|    CastleBravo45|There are problem...|t1_gjzh695|t3_l1d0r7|gjzhjgz|2021-01-20 20:44:38|\n",
      "|Conservative|KilgoreTroutsAnus|The point of Trum...|t1_gjzb2la|t3_l1ftsv|gjzhjh1|2021-01-20 20:44:38|\n",
      "+------------+-----------------+--------------------+----------+---------+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display a subset of columns\n",
    "comments.select(\"subreddit\", \"author\", \"body\", \"parent_id\", \"link_id\", \"id\", \"created_utc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading submissions from s3a://sagemaker-us-east-1-711387073580/project/submissions\n",
      "23/10/30 14:42:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the submissions dataframe is 2,108x68\n",
      "CPU times: user 14.5 ms, sys: 3.77 ms, total: 18.3 ms\n",
      "Wall time: 21.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data}/submissions\"\n",
    "print(f\"reading submissions from {s3_path}\")\n",
    "submissions = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   subreddit|count|\n",
      "+------------+-----+\n",
      "|changemyview| 2025|\n",
      "|Ask_Politics|   83|\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check counts (ensuring all needed subreddits exist)\n",
    "submissions.groupBy('subreddit').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- adserver_click_url: string (nullable = true)\n",
      " |-- adserver_imp_pixel: string (nullable = true)\n",
      " |-- archived: boolean (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- brand_safe: boolean (nullable = true)\n",
      " |-- contest_mode: boolean (nullable = true)\n",
      " |-- created_utc: timestamp (nullable = true)\n",
      " |-- crosspost_parent: string (nullable = true)\n",
      " |-- crosspost_parent_list: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- approved_at_utc: string (nullable = true)\n",
      " |    |    |-- approved_by: string (nullable = true)\n",
      " |    |    |-- archived: boolean (nullable = true)\n",
      " |    |    |-- author: string (nullable = true)\n",
      " |    |    |-- author_flair_css_class: string (nullable = true)\n",
      " |    |    |-- author_flair_text: string (nullable = true)\n",
      " |    |    |-- banned_at_utc: string (nullable = true)\n",
      " |    |    |-- banned_by: string (nullable = true)\n",
      " |    |    |-- brand_safe: boolean (nullable = true)\n",
      " |    |    |-- can_gild: boolean (nullable = true)\n",
      " |    |    |-- can_mod_post: boolean (nullable = true)\n",
      " |    |    |-- clicked: boolean (nullable = true)\n",
      " |    |    |-- contest_mode: boolean (nullable = true)\n",
      " |    |    |-- created: double (nullable = true)\n",
      " |    |    |-- created_utc: double (nullable = true)\n",
      " |    |    |-- distinguished: string (nullable = true)\n",
      " |    |    |-- domain: string (nullable = true)\n",
      " |    |    |-- downs: long (nullable = true)\n",
      " |    |    |-- edited: boolean (nullable = true)\n",
      " |    |    |-- gilded: long (nullable = true)\n",
      " |    |    |-- hidden: boolean (nullable = true)\n",
      " |    |    |-- hide_score: boolean (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- is_crosspostable: boolean (nullable = true)\n",
      " |    |    |-- is_reddit_media_domain: boolean (nullable = true)\n",
      " |    |    |-- is_self: boolean (nullable = true)\n",
      " |    |    |-- is_video: boolean (nullable = true)\n",
      " |    |    |-- likes: string (nullable = true)\n",
      " |    |    |-- link_flair_css_class: string (nullable = true)\n",
      " |    |    |-- link_flair_text: string (nullable = true)\n",
      " |    |    |-- locked: boolean (nullable = true)\n",
      " |    |    |-- media: string (nullable = true)\n",
      " |    |    |-- mod_reports: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- num_comments: long (nullable = true)\n",
      " |    |    |-- num_crossposts: long (nullable = true)\n",
      " |    |    |-- num_reports: string (nullable = true)\n",
      " |    |    |-- over_18: boolean (nullable = true)\n",
      " |    |    |-- parent_whitelist_status: string (nullable = true)\n",
      " |    |    |-- permalink: string (nullable = true)\n",
      " |    |    |-- pinned: boolean (nullable = true)\n",
      " |    |    |-- quarantine: boolean (nullable = true)\n",
      " |    |    |-- removal_reason: string (nullable = true)\n",
      " |    |    |-- report_reasons: string (nullable = true)\n",
      " |    |    |-- saved: boolean (nullable = true)\n",
      " |    |    |-- score: long (nullable = true)\n",
      " |    |    |-- secure_media: string (nullable = true)\n",
      " |    |    |-- selftext: string (nullable = true)\n",
      " |    |    |-- selftext_html: string (nullable = true)\n",
      " |    |    |-- spoiler: boolean (nullable = true)\n",
      " |    |    |-- stickied: boolean (nullable = true)\n",
      " |    |    |-- subreddit: string (nullable = true)\n",
      " |    |    |-- subreddit_id: string (nullable = true)\n",
      " |    |    |-- subreddit_name_prefixed: string (nullable = true)\n",
      " |    |    |-- subreddit_type: string (nullable = true)\n",
      " |    |    |-- suggested_sort: string (nullable = true)\n",
      " |    |    |-- thumbnail: string (nullable = true)\n",
      " |    |    |-- thumbnail_height: string (nullable = true)\n",
      " |    |    |-- thumbnail_width: string (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- ups: long (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user_reports: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- view_count: string (nullable = true)\n",
      " |    |    |-- visited: boolean (nullable = true)\n",
      " |    |    |-- whitelist_status: string (nullable = true)\n",
      " |-- disable_comments: boolean (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- domain_override: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- embed_type: string (nullable = true)\n",
      " |-- embed_url: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- hidden: boolean (nullable = true)\n",
      " |-- hide_score: boolean (nullable = true)\n",
      " |-- href_url: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- imp_pixel: string (nullable = true)\n",
      " |-- is_crosspostable: boolean (nullable = true)\n",
      " |-- is_reddit_media_domain: boolean (nullable = true)\n",
      " |-- is_self: boolean (nullable = true)\n",
      " |-- is_video: boolean (nullable = true)\n",
      " |-- link_flair_css_class: string (nullable = true)\n",
      " |-- link_flair_text: string (nullable = true)\n",
      " |-- locked: boolean (nullable = true)\n",
      " |-- media: struct (nullable = true)\n",
      " |    |-- event_id: string (nullable = true)\n",
      " |    |-- oembed: struct (nullable = true)\n",
      " |    |    |-- author_name: string (nullable = true)\n",
      " |    |    |-- author_url: string (nullable = true)\n",
      " |    |    |-- cache_age: long (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- html: string (nullable = true)\n",
      " |    |    |-- provider_name: string (nullable = true)\n",
      " |    |    |-- provider_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_height: long (nullable = true)\n",
      " |    |    |-- thumbnail_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_width: long (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- version: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |    |-- reddit_video: struct (nullable = true)\n",
      " |    |    |-- dash_url: string (nullable = true)\n",
      " |    |    |-- duration: long (nullable = true)\n",
      " |    |    |-- fallback_url: string (nullable = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- hls_url: string (nullable = true)\n",
      " |    |    |-- is_gif: boolean (nullable = true)\n",
      " |    |    |-- scrubber_media_url: string (nullable = true)\n",
      " |    |    |-- transcoding_status: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- media_embed: struct (nullable = true)\n",
      " |    |-- content: string (nullable = true)\n",
      " |    |-- height: long (nullable = true)\n",
      " |    |-- scrolling: boolean (nullable = true)\n",
      " |    |-- width: long (nullable = true)\n",
      " |-- mobile_ad_url: string (nullable = true)\n",
      " |-- num_comments: long (nullable = true)\n",
      " |-- num_crossposts: long (nullable = true)\n",
      " |-- original_link: string (nullable = true)\n",
      " |-- over_18: boolean (nullable = true)\n",
      " |-- parent_whitelist_status: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- pinned: boolean (nullable = true)\n",
      " |-- post_hint: string (nullable = true)\n",
      " |-- preview: struct (nullable = true)\n",
      " |    |-- enabled: boolean (nullable = true)\n",
      " |    |-- images: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |-- variants: struct (nullable = true)\n",
      " |    |    |    |    |-- gif: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |-- mp4: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |-- nsfw: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |-- obfuscated: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |-- promoted: boolean (nullable = true)\n",
      " |-- promoted_by: string (nullable = true)\n",
      " |-- promoted_display_name: string (nullable = true)\n",
      " |-- promoted_url: string (nullable = true)\n",
      " |-- retrieved_on: timestamp (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- secure_media: struct (nullable = true)\n",
      " |    |-- event_id: string (nullable = true)\n",
      " |    |-- oembed: struct (nullable = true)\n",
      " |    |    |-- author_name: string (nullable = true)\n",
      " |    |    |-- author_url: string (nullable = true)\n",
      " |    |    |-- cache_age: long (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- html: string (nullable = true)\n",
      " |    |    |-- provider_name: string (nullable = true)\n",
      " |    |    |-- provider_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_height: long (nullable = true)\n",
      " |    |    |-- thumbnail_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_width: long (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- version: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- secure_media_embed: struct (nullable = true)\n",
      " |    |-- content: string (nullable = true)\n",
      " |    |-- height: long (nullable = true)\n",
      " |    |-- media_domain_url: string (nullable = true)\n",
      " |    |-- scrolling: boolean (nullable = true)\n",
      " |    |-- width: long (nullable = true)\n",
      " |-- selftext: string (nullable = true)\n",
      " |-- spoiler: boolean (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- suggested_sort: string (nullable = true)\n",
      " |-- third_party_trackers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- third_party_tracking: string (nullable = true)\n",
      " |-- third_party_tracking_2: string (nullable = true)\n",
      " |-- thumbnail: string (nullable = true)\n",
      " |-- thumbnail_height: long (nullable = true)\n",
      " |-- thumbnail_width: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- whitelist_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submissions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+--------------------+--------------------+-------------------+------------+\n",
      "|   subreddit|             author|               title|            selftext|        created_utc|num_comments|\n",
      "+------------+-------------------+--------------------+--------------------+-------------------+------------+\n",
      "|Conservative|       Foubar_ghost|Liberal lawyer De...|                    |2021-01-06 01:19:57|          44|\n",
      "|changemyview|          [deleted]|CMV: CallMeCarson...|           [removed]|2021-01-06 01:20:24|          31|\n",
      "|Conservative|             f1sh98|Hong Kong Police ...|                    |2021-01-06 01:21:27|          13|\n",
      "|Conservative|          BluePath2|Georgia run off t...|                    |2021-01-06 01:24:40|           0|\n",
      "| Libertarian|          [deleted]|Trump supporters ...|           [deleted]|2021-01-06 01:25:25|         288|\n",
      "|Conservative|          [deleted]|I dont even need ...|           [deleted]|2021-01-06 01:25:31|           0|\n",
      "| Libertarian|    GruntNumber9902|Learn from histor...|Libertarian: an a...|2021-01-06 01:31:23|           0|\n",
      "|Conservative|      ChunkyArsenio|UK: Chief medical...|                    |2021-01-06 01:32:35|           6|\n",
      "|Conservative|      Lionhearted09|Live Updates in G...|                    |2021-01-06 01:33:05|         847|\n",
      "|Conservative|           1221Wood|Just a reminder t...|                    |2021-01-06 01:33:30|           0|\n",
      "|Conservative|     3dprinteddildo|Most Georgia runo...|                    |2021-01-06 01:33:51|         263|\n",
      "|Conservative|          weethomas|How much do you w...|           [removed]|2021-01-06 01:34:41|           0|\n",
      "|Ask_Politics|            nicebol|What does your da...|           [removed]|2021-01-06 01:38:05|           1|\n",
      "|Conservative|  joystickfantastic|Pence told Trump ...|                    |2021-01-06 01:39:35|           0|\n",
      "| Libertarian|            rgshrey|Blatant plug for ...|                    |2021-01-06 01:40:13|           0|\n",
      "|Conservative|          [deleted]|Liberal Law Profe...|           [deleted]|2021-01-06 01:40:42|           0|\n",
      "| Libertarian|anonymous_man842740|Just today I real...|Just today I real...|2021-01-06 01:40:45|          68|\n",
      "|Conservative|          Vimes3000|Where in Reddit i...|           [removed]|2021-01-06 01:40:52|           0|\n",
      "|Conservative|             nimobo|CNN Kicks Off 202...|                    |2021-01-06 01:41:36|           3|\n",
      "|Conservative|          [deleted]|Goodbye /r/Conser...|           [removed]|2021-01-06 01:42:19|           0|\n",
      "+------------+-------------------+--------------------+--------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display a subset of columns\n",
    "submissions.select(\"subreddit\", \"author\", \"title\", \"selftext\", \"created_utc\", \"num_comments\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Eric's data gathering code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import sagemaker\n",
    "# from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# # Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "# role = sagemaker.get_execution_role()\n",
    "# spark_processor = PySparkProcessor(\n",
    "#     base_job_name=\"sm-spark-project\",\n",
    "#     framework_version=\"3.3\",\n",
    "#     role=role,\n",
    "#     instance_count=6,\n",
    "#     instance_type=\"ml.m5.xlarge\",\n",
    "#     max_runtime_in_seconds=7200,\n",
    "# )\n",
    "\n",
    "# # s3 paths\n",
    "# session = sagemaker.Session()\n",
    "# bucket = session.default_bucket()\n",
    "# output_prefix_logs = f\"spark_logs\"\n",
    "# col_name_for_filtering = \"subreddit\"\n",
    "\n",
    "# # modify this comma separated list to choose the subreddits of interest\n",
    "# subreddits = \"changemyview, Ask_Politics\"\n",
    "\n",
    "# configuration = [\n",
    "#     {\n",
    "#         \"Classification\": \"spark-defaults\",\n",
    "#         \"Properties\": {\"spark.executor.memory\": \"12g\", \"spark.executor.cores\": \"4\"},\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "\n",
    "# for year in range(2022, 2024):    \n",
    "#     # comments\n",
    "#     print(f\"Working on Comments for year {year}\")\n",
    "    \n",
    "#     s3_dataset_path_commments = f\"s3://bigdatateaching/reddit-parquet/comments/year={year}/month=*/*.parquet\" \n",
    "#     output_prefix_data_comments = \"project/comments\"\n",
    "\n",
    "#     spark_processor.run(\n",
    "#         submit_app=\"./process_cmv_ap.py\",\n",
    "#         arguments=[\n",
    "#             \"--s3_dataset_path\",\n",
    "#             s3_dataset_path_commments,\n",
    "#             \"--s3_output_bucket\",\n",
    "#             bucket,\n",
    "#             \"--s3_output_prefix\",\n",
    "#             output_prefix_data_comments,\n",
    "#             \"--col_name_for_filtering\",\n",
    "#             col_name_for_filtering,\n",
    "#             \"--values_to_keep\",\n",
    "#             subreddits,\n",
    "#         ],\n",
    "#         spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "#         logs=False,\n",
    "#         configuration=configuration\n",
    "#     )\n",
    "\n",
    "#     # submissions\n",
    "#     print(f\"Working on Submissions for year {year}\")\n",
    "    \n",
    "#     s3_dataset_path_submissions = f\"s3://bigdatateaching/reddit-parquet/submissions/year={year}/month=*/*.parquet\"\n",
    "#     output_prefix_data_submissions = \"project/submissions\"\n",
    "\n",
    "#     spark_processor.run(\n",
    "#         submit_app=\"./process_cmv_ap.py\",\n",
    "#         arguments=[\n",
    "#              \"--s3_dataset_path\",\n",
    "#             s3_dataset_path_submissions,\n",
    "#             \"--s3_output_bucket\",\n",
    "#             bucket,\n",
    "#             \"--s3_output_prefix\",\n",
    "#             output_prefix_data_submissions,\n",
    "#             \"--col_name_for_filtering\",\n",
    "#             col_name_for_filtering,\n",
    "#             \"--values_to_keep\",\n",
    "#             subreddits,\n",
    "#         ],\n",
    "#         spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "#         logs=False,\n",
    "#         configuration=configuration\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
