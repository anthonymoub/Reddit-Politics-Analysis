{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6652aa27-1064-45ad-a21c-c376efdf061a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1217e582-1719-4d36-aac1-b67f8284b9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.10.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyspark==3.4.0 in /opt/conda/lib/python3.10/site-packages (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark==3.4.0) (0.10.9.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spark-nlp==5.1.3 in /opt/conda/lib/python3.10/site-packages (5.1.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install openjdk -y\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.4.0\n",
    "\n",
    "# install spark-nlp\n",
    "%pip install spark-nlp==5.1.3\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445de56-e742-4129-b447-da0b31d30e24",
   "metadata": {},
   "source": [
    "# Download JARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f9d814-9d28-44ef-89e5-4c730dea02b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "2023-11-19 20:28:00  708534094 spark-nlp-assembly-5.1.3.jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f4429907-73aa-435b-896c-eac4ca63bdf0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 247ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f4429907-73aa-435b-896c-eac4ca63bdf0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/9ms)\n",
      "23/11/19 20:28:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "!wget -qO- https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-5.1.3.jar | aws s3 cp - s3://{bucket}/project/spark-nlp-assembly-5.1.3.jar\n",
    "!aws s3 ls s3://{bucket}/project/spark-nlp-assembly-5.1.3.jar\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615374d-0eeb-4f1f-aacd-c605de43d2d7",
   "metadata": {},
   "source": [
    "# Preparing DFs for Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fdd76-7184-4d9d-a80f-ab75fe962e2a",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e55001b-9216-4c82-837b-1c00d73d92e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/19 20:28:13 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------+----------------+-----------+-------------+-------------+------+-------+------------+---------+----------+--------------------+------------+-----+--------+-----------+------------+--------------------+\n",
      "|      author|                body|can_gild|controversiality|created_utc|distinguished|       edited|gilded|     id|is_submitter|  link_id| parent_id|           permalink|retrieved_on|score|stickied|  subreddit|subreddit_id|        cleaned_body|\n",
      "+------------+--------------------+--------+----------------+-----------+-------------+-------------+------+-------+------------+---------+----------+--------------------+------------+-----+--------+-----------+------------+--------------------+\n",
      "|Rude-Way4688|Yee. I like DeSan...|    true|               0| 2021-05-28|         null|1.622224763E9|     0|gzs7gai|       false|t3_nmztty|t1_gzs6nla|/r/Libertarian/co...|        null|  -30|   false|Libertarian|    t5_2qh63|[yee, like, desan...|\n",
      "|   [deleted]|To be fair, this ...|   false|               0| 2021-07-23|         null|        false|     0|h67r8pp|       false|t3_opr5q9|t1_h67ea09|/r/centrist/comme...|  2022-05-14|    2|   false|   centrist|    t5_2qx8j|[fair, south, ali...|\n",
      "|   [deleted]|           [deleted]|    true|               0| 2021-02-10|         null|1.613002061E9|     0|gmrrnpn|       false|t3_lgcjd3|t1_gmrr2c5|/r/Libertarian/co...|        null|   17|   false|Libertarian|    t5_2qh63|            [delete]|\n",
      "|   [deleted]|The point is covi...|   false|               0| 2021-02-07|         null|        false|     0|gmgzk70|       false|t3_leehhu| t3_leehhu|/r/Libertarian/co...|        null|    3|   false|Libertarian|    t5_2qh63|[point, covid, ch...|\n",
      "| Shmodecious|Believing that tr...|    true|               0| 2021-11-17|         null|        false|     0|hkzj1tr|       false|t3_qvgqyd|t1_hky3fvx|/r/Libertarian/co...|  2022-02-27|    0|   false|Libertarian|    t5_2qh63|[believe, transge...|\n",
      "+------------+--------------------+--------+----------------+-----------+-------------+-------------+------+-------+------------+---------+----------+--------------------+------------+-----+--------+-----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------+-----+----------------+-------------+------+--------------------+\n",
      "|      author|  subreddit|created_utc|score|controversiality|distinguished|gilded|     concat_comments|\n",
      "+------------+-----------+-----------+-----+----------------+-------------+------+--------------------+\n",
      "|Rude-Way4688|Libertarian| 2021-05-28|  -30|               0|         null|     0|yee like desantis...|\n",
      "|   [deleted]|   centrist| 2021-07-23|    2|               0|         null|     0|fair south alia m...|\n",
      "|   [deleted]|Libertarian| 2021-02-10|   17|               0|         null|     0|              delete|\n",
      "|   [deleted]|Libertarian| 2021-02-07|    3|               0|         null|     0|point covid choos...|\n",
      "| Shmodecious|Libertarian| 2021-11-17|    0|               0|         null|     0|believe transgend...|\n",
      "+------------+-----------+-----------+-----+----------------+-------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in concat_comments: 14863477\n",
      "Number of empty rows in concat_comments: 63302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, col\n",
    "\n",
    "# Tegveer's S3 -- DO NOT CHANGE\n",
    "s3_directory = f\"s3a://sagemaker-us-east-1-433974840707/project/nlp_cleaned_comments/\"\n",
    "\n",
    "df_comments = spark.read.parquet(s3_directory)\n",
    "df_comments.show(5)\n",
    "\n",
    "# concat list of words into a single string column\n",
    "comms = df_comments.withColumn(\"concat_comments\", concat_ws(\" \", \"cleaned_body\"))\n",
    "\n",
    "# sanity check\n",
    "comms = comms.select(\"author\", \"subreddit\", \"created_utc\", \"score\", \"controversiality\", \"distinguished\", \"gilded\", \"concat_comments\")\n",
    "comms.show(5)\n",
    "\n",
    "# filter out empty rows\n",
    "comms_rows = comms.count()\n",
    "non_empty_comments = comms.filter((col(\"concat_comments\").isNotNull()) & (col(\"concat_comments\") != \"\"))\n",
    "\n",
    "# get number of empty rows\n",
    "empty_comments_count = comms_rows - non_empty_comments.count()\n",
    "print(f\"Number of rows in concat_comments: {comms_rows}\")\n",
    "print(f\"Number of empty rows in concat_comments: {empty_comments_count}\")\n",
    "\n",
    "# filter for specific subreddits -- CHANGE BASED ON SUBREDDITS HERE\n",
    "#subreddits = \"socialism, Economics, Liberal, Conservative, Libertarian, centrist, changemyview, Ask_Politics, finance\"\n",
    "subreddits_list = [\"Conservative\", \"finance\"]\n",
    "filtered_comms = non_empty_comments.filter(col(\"subreddit\").isin(subreddits_list))\n",
    "\n",
    "# write to s3\n",
    "output_path = f\"s3a://sagemaker-us-east-1-224518912016/project/nlp/comments/\"\n",
    "filtered_comms.write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be84bfc-03cf-409a-9fc8-b7c700562369",
   "metadata": {},
   "source": [
    "## Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "917a5769-2b81-478e-9ce4-49c6a0088e84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------+-----+------------+--------------------+-----------+--------------------+------------------+--------+-------+----------------------+-------+-------+--------+--------------------+--------------------+--------------------+------+--------+-------------+--------------------+--------------------+\n",
      "|              author|               title|            selftext|subreddit|score|num_comments|           permalink|created_utc|                 url|            domain|is_video|is_self|is_reddit_media_domain|spoiler|over_18|stickied|           thumbnail|               media|        secure_media|gilded|archived|distinguished|        cleaned_body|       cleaned_title|\n",
      "+--------------------+--------------------+--------------------+---------+-----+------------+--------------------+-----------+--------------------+------------------+--------+-------+----------------------+-------+-------+--------+--------------------+--------------------+--------------------+------+--------+-------------+--------------------+--------------------+\n",
      "|    IsThisReallyNate|Evidence of the W...|Iâ€™ve heard it cla...|socialism|    6|           3|/r/socialism/comm...| 2021-07-29|https://www.reddi...|    self.socialism|   false|   true|                 false|  false|  false|   false|                self|                null|                null|     0|   false|         null|[ive, hear, claim...|[evidence, war, t...|\n",
      "|           [deleted]|Starbucks are blo...|           [deleted]|socialism|   11|           0|/r/socialism/comm...| 2022-04-15|https://twitter.c...|       twitter.com|   false|  false|                 false|  false|  false|   false|             default|{event_id=null, t...|{event_id=null, t...|     0|   false|         null|            [delete]|[starbucks, block...|\n",
      "|MedicineInevitable48|             Shubham|                    |Economics|    1|           1|/r/Economics/comm...| 2021-02-03|https://bsshahedu...|bsshaheducation.in|   false|  false|                 false|  false|  false|   false|             default|                null|                null|     0|   false|         null|                  []|           [shubham]|\n",
      "|             NORDLAN|Federal judge tos...|                    |  Liberal|   17|           2|/r/Liberal/commen...| 2021-06-22|https://www.washi...|washingtonpost.com|   false|  false|                 false|  false|  false|   false|https://b.thumbs....|                null|                null|     0|   false|         null|                  []|[federal, judge, ...|\n",
      "|        haskalah1989|Today marks 214 y...|                    |socialism| 1761|          43|/r/socialism/comm...| 2021-03-25|https://i.redd.it...|         i.redd.it|   false|  false|                  true|  false|  false|   false|https://b.thumbs....|                null|                null|     0|   false|         null|                  []|[today, mark, yea...|\n",
      "+--------------------+--------------------+--------------------+---------+-----+------------+--------------------+-----------+--------------------+------------------+--------+-------+----------------------+-------+-------+--------+--------------------+--------------------+--------------------+------+--------+-------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+---------+-----------+-----+------------+-------------+------+--------------------+\n",
      "|              author|subreddit|created_utc|score|num_comments|distinguished|gilded|  concat_submissions|\n",
      "+--------------------+---------+-----------+-----+------------+-------------+------+--------------------+\n",
      "|    IsThisReallyNate|socialism| 2021-07-29|    6|           3|         null|     0|ive hear claim ma...|\n",
      "|           [deleted]|socialism| 2022-04-15|   11|           0|         null|     0|delete starbucks ...|\n",
      "|MedicineInevitable48|Economics| 2021-02-03|    1|           1|         null|     0|             shubham|\n",
      "|             NORDLAN|  Liberal| 2021-06-22|   17|           2|         null|     0|federal judge tos...|\n",
      "|        haskalah1989|socialism| 2021-03-25| 1761|          43|         null|     0|today mark year s...|\n",
      "+--------------------+---------+-----------+-----+------------+-------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in concat_submissions: 599908\n",
      "Number of empty rows in concat_submissions: 565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Tegveer's S3 -- DO NOT CHANGE\n",
    "s3_directory = f\"s3a://sagemaker-us-east-1-433974840707/project/nlp_cleaned_submissions/\"\n",
    "\n",
    "df_submissions = spark.read.parquet(s3_directory)\n",
    "df_submissions.show(5)\n",
    "\n",
    "# concat \"cleaned_body\" and \"cleaned_title\" new col\n",
    "subs = df_submissions.withColumn(\n",
    "    \"concat_submissions\",\n",
    "    concat_ws(\" \", \"cleaned_body\", \"cleaned_title\")\n",
    ")\n",
    "\n",
    "# sanity check\n",
    "subs = subs.select(\"author\", \"subreddit\", \"created_utc\", \"score\", \"num_comments\", \"distinguished\", \"gilded\", \"concat_submissions\")\n",
    "subs.show(5)\n",
    "\n",
    "# filter out empty rows\n",
    "subs_rows = subs.count()\n",
    "non_empty_subs = subs.filter((col(\"concat_submissions\").isNotNull()) & (col(\"concat_submissions\") != \"\"))\n",
    "\n",
    "# get number of empty rows\n",
    "empty_comments_subs = subs_rows - non_empty_subs.count()\n",
    "print(f\"Number of rows in concat_submissions: {subs_rows}\")\n",
    "print(f\"Number of empty rows in concat_submissions: {empty_comments_subs}\")\n",
    "\n",
    "# filter for specific subreddits\n",
    "filtered_subs = non_empty_subs.filter(col(\"subreddit\").isin(subreddits_list))\n",
    "\n",
    "# write to s3\n",
    "output_path = f\"s3a://sagemaker-us-east-1-224518912016/project/nlp/submissions/\"\n",
    "filtered_subs.write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e801246-1959-4a91-9aac-abc02452a650",
   "metadata": {},
   "source": [
    "# Process S3 data with SageMaker Processing Job `PySparkProcessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecb4bc3-0812-4dbd-9ad7-4c0d0d676d65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./sentiment_conservative_finance.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./sentiment_conservative_finance.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "import json\n",
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--df_target_col\", type=str, help=\"Target Column for Sentiment Analysis\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "    logger.info(f\"args={args}\")\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP\")\\\n",
    "        .config(\"spark.driver.memory\",\"16G\")\\\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\")\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"sparknlp version: {sparknlp.version()}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "    \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True)\n",
    "    \n",
    "    # sentiment analysis\n",
    "    target_col = args.df_target_col\n",
    "    MODEL_NAME = 'sentimentdl_use_twitter'\n",
    "    logger.info(f\"setting up an nlp pipeline with model={MODEL_NAME}\")\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(target_col)\\\n",
    "    .setOutputCol(f\"document_{target_col}\")\n",
    "    \n",
    "    use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    "     .setInputCols([f\"document_{target_col}\"])\\\n",
    "     .setOutputCol(f\"sentence_embeddings_{target_col}\")\n",
    "\n",
    "    sentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")\\\n",
    "    .setInputCols([f\"sentence_embeddings_{target_col}\"])\\\n",
    "    .setOutputCol(f\"sentiment_{target_col}\")\n",
    "\n",
    "    nlpPipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          use,\n",
    "          sentimentdl\n",
    "      ])\n",
    "\n",
    "    logger.info(f\"going to fit pipeline on dataframe\")\n",
    "    nlpModel = nlpPipeline.fit(df)\n",
    "    \n",
    "    logger.info(f\"going to transform pipeline on dataframe\")\n",
    "    result = nlpModel.transform(df)\n",
    "    \n",
    "    # save the cleaned dataframes so that these files can now be used for future analysis\n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_key_prefix}\"\n",
    "    logger.info(f\"going to write data in {s3_path}\")\n",
    "    logger.info(f\"shape of the df_filtered dataframe is {result.count():,}x{len(result.columns)}\")\n",
    "    result.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0880a58-8b72-4a5e-a5b7-9c9021e11781",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project-sentiment\",\n",
    "    image_uri=f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark:latest\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=7200,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "output_prefix = f\"project/sentiment\"\n",
    "output_prefix_logs = f\"spark_logs/sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc53dbd0-0d81-4685-97c0-3ae70320971a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Comments Col concat_comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-sentiment-2023-11-19-20-31-18-590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................................................................................................................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-sentiment-2023-11-19-20-59-16-709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Submissions Col concat_submissions\n",
      "..................................................................................................................................................!"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# comments\n",
    "df_target_col = \"concat_comments\"\n",
    "print(f\"Working on Comments Col {df_target_col}\")\n",
    "spark_processor.run(\n",
    "    submit_app=\"./sentiment_conservative_finance.py\",\n",
    "    submit_jars=[f\"s3://{bucket}/project/spark-nlp-assembly-5.1.3.jar\"],\n",
    "    arguments=[\n",
    "        \"--df_target_col\",\n",
    "        df_target_col,\n",
    "        \"--s3_dataset_path\",\n",
    "        f\"s3://sagemaker-us-east-1-224518912016/project/nlp/comments/\",\n",
    "        \"--s3_output_bucket\",\n",
    "        \"sagemaker-us-east-1-224518912016\",\n",
    "        \"--s3_output_key_prefix\",\n",
    "        f\"{output_prefix}/comments/\",\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")\n",
    "\n",
    "time.sleep(60)\n",
    "\n",
    "# submissions\n",
    "df_target_col = \"concat_submissions\"\n",
    "print(f\"Working on Submissions Col {df_target_col}\")\n",
    "spark_processor.run(\n",
    "    submit_app=\"./sentiment_conservative_finance.py\",\n",
    "    submit_jars=[f\"s3://{bucket}/project/spark-nlp-assembly-5.1.3.jar\"],\n",
    "    arguments=[\n",
    "        \"--df_target_col\",\n",
    "        df_target_col,\n",
    "        \"--s3_dataset_path\",\n",
    "        f\"s3://sagemaker-us-east-1-224518912016/project/nlp/submissions/\",\n",
    "        \"--s3_output_bucket\",\n",
    "        \"sagemaker-us-east-1-224518912016\",\n",
    "        \"--s3_output_key_prefix\",\n",
    "        f\"{output_prefix}/submissions/\",\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d80cd92-294c-4bf1-b4d2-35fd31a19948",
   "metadata": {},
   "source": [
    "# Read in Sentiment Data\n",
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "089d00bb-44e5-4710-966b-96dae18e5e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|   subreddit|  count|\n",
      "+------------+-------+\n",
      "|     finance| 136308|\n",
      "|Conservative|5204573|\n",
      "+------------+-------+\n",
      "\n",
      "['author', 'subreddit', 'created_utc', 'score', 'controversiality', 'distinguished', 'gilded', 'concat_comments']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read all the Parquet files in the directory into a DataFrame\n",
    "df_comments = spark.read.parquet(f\"s3a://sagemaker-us-east-1-224518912016/project/nlp/comments/\")\n",
    "\n",
    "# check counts (ensuring all needed subreddits exist)\n",
    "df_comments.groupBy('subreddit').count().show()\n",
    "\n",
    "# get cols\n",
    "print(df_comments.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef3e90-1050-4382-b864-52359865f827",
   "metadata": {},
   "source": [
    "## Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18cb37f1-5c3f-4c34-8731-038f90431d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|   subreddit| count|\n",
      "+------------+------+\n",
      "|     finance| 28817|\n",
      "|Conservative|343660|\n",
      "+------------+------+\n",
      "\n",
      "['author', 'subreddit', 'created_utc', 'score', 'num_comments', 'distinguished', 'gilded', 'concat_submissions']\n"
     ]
    }
   ],
   "source": [
    "# Read all the Parquet files in the directory into a DataFrame\n",
    "df_submissions = spark.read.parquet(f\"s3a://sagemaker-us-east-1-224518912016/project/nlp/submissions/\")\n",
    "\n",
    "# check counts (ensuring all needed subreddits exist)\n",
    "df_submissions.groupBy('subreddit').count().show()\n",
    "\n",
    "# get cols\n",
    "print(df_submissions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faffbc0-9c54-438e-a5ad-80afdb7aa534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
