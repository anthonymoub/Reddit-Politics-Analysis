# Natural Language Processing

## Business Goals
1. Tabulate the most discussed and the most important words. Visualize text lengths as well as average number of words over time in combined political and economics subreddits.
2. Assess the sentiment in posts and comments for combined political and economics subreddits in relation to U.S GDP.
3. Determine prevalent subjects or themes in combined political and economics subreddits submissions.

## Technical Proposal
1. 
   * *Uncover most discussed words*: We create four tables of select top 100 common words corresponding to politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments. For analyzing submissions, we group politics and economics subreddits separately and count common words on the feature-engineered column of combined post body (`selftext`) and post `title`. We also use regex to replace certain stopwords, such as "https", "www", and "jpg", that the NLP pipeline could not remove. Once the new feature is obtained, it is turned into an array of words, flattened, grouped by word to get the counts, and sorted and filtered to output a `.csv` file of the top 100 words. For analyzing comments, we group politics and economics subreddits separately and count common words in the comment `body`, so no new column was created for comments. We also use the regex mentioned above to replace certain stopwords and utilize the same process to obtain the counts of common words.
   * *Uncover most important words*: We create three tables of the top 10 most important words, measured by TF-IDF score, corresponding to politics subreddits submissions, economics subreddits submissions, and politics subreddits comments (economics subreddits comments are left out due to allocation of compute capacity). Similar to the process for obtaining the most discussed words, we group politics and economics subreddits separately and run the `HashingTF` and `IDF` functions on the feature engineered column of combined post body (`selftext`) and post `title`. For analysing comments, we group politics and economics subreddits separately and run the `HashingTF` and `IDF` functions on comment `body`, so no new column was created for comments.
   * *Visualize text lengths and average number of words over time*: Similar to the process for obtaining the most discussed words, we group politics and economics subreddits separately and find the `length` of the `selftext` column for submissions and `body` column for comments. We then create four `matplotlib` histogram visualizations of the distribution of text lengths corresponding to politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments. Finally, we also create a `matplotlib` lineplot visualization of the average number of words over time for politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments in the same plot. To obtain average number of words over time, we group politics and economics subreddits separately and find the `size` after performing a `split` of the `selftext` column for submissions and `body` column for comments. We then group the data on `year` and `month` and aggregate each month's average number of words.

1. Using SageMaker processing jobs and spark nlp, we generated positive, neutral, or negative sentiment indicators. Considering that `Reddit` is a widely used social media site and forum, we used a pre-trained sentiment detection model typically used for `Twitter`. For submissions, the post's `title` and `body` were concatenated into a `concat_submissions` column, which was used for sentiment generation. Similarly, for comments, the post's `body` was used for sentiment generation. With the generated sentiment data for submissions and comments from each subreddit, we combined the political subreddits' data with the U.S. GDP. We then generated plots showing changes in user activity and overall sentiments from each political subreddit of interest alongside changes in the U.S. GDP.

3. By utilizing both the `pyspark` and `gensim` libraries, we ensure comprehensive topic modeling analysis, and the interactive visualization powered by the `pyLDAvis` package provides a clear representation of the prevalent subjects and themes, enabling informed insights into the content across the targeted subreddits. We specifically chose to perform topic modeling on the submissions instead of comments to reveal which topics users want to initiate a discussion on rather than what they go on to discuss and possibly divert from the initial post. Topic modeling, with `gensim`, for submissions was performed by grouping politics and economics subreddits separately and combining the `selftext` and `title` columns to create a new column called `nlpbodytext`, like we did for Business Goal 8. We then create a `gensim` dictionary and corpus, and run the `LdaModel` function to obtain the topics and their corresponding keywords. Finally, we create an interactive visualization of the topics and their corresponding keywords using the `pyLDAvis` package.