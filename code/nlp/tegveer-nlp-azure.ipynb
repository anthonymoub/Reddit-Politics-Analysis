{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup - Azure"
      ],
      "metadata": {},
      "id": "c6984423-b8f8-4fb5-9b45-2fbc2a0a65b5"
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "37",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:34:45.8369535Z",
              "session_start_time": "2023-11-21T22:34:45.8763111Z",
              "execution_start_time": "2023-11-21T22:37:59.3872244Z",
              "execution_finish_time": "2023-11-21T22:38:02.4104705Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "003176f5-96d3-45be-9209-b497389dd69c"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 37, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7fa467e59480>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-09066444:35475\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1.5.2-108696741</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700606282596
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "82eec931-0914-4e4e-a641-58c32b089b26"
    },
    {
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "37",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:34:45.8378907Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T22:38:02.5295417Z",
              "execution_finish_time": "2023-11-21T22:38:02.8669461Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "ff47a9dc-ffeb-4d72-b1f1-295c205926e2"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 37, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "<SparkContext master=yarn appName=Azure ML Experiment>",
            "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-09066444:35475\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1.5.2-108696741</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700606283004
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b5482867-cda1-4b20-b924-972167890242"
    },
    {
      "cell_type": "code",
      "source": [
        "%%configure -f \\\n",
        "{\"conf\": {\"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.2\"}}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:34:45.8387341Z",
              "session_start_time": "2023-11-21T22:38:02.9871245Z",
              "execution_start_time": "2023-11-21T22:39:52.6150845Z",
              "execution_finish_time": "2023-11-21T22:39:52.6468857Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "7dcbd71a-af02-4403-99ac-8b70e50d2f23"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Unrecognized options: "
        }
      ],
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "2b9da635-56a2-4130-aa72-1a22376e8edc"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark-nlp"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:34:45.8397545Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T22:40:39.5751202Z",
              "execution_finish_time": "2023-11-21T22:40:58.2068753Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "5a2a28bd-c28b-4060-8df1-cce1c4e4016d"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting spark-nlp\n  Downloading spark_nlp-5.1.4-py2.py3-none-any.whl (540 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.7/540.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: spark-nlp\nSuccessfully installed spark-nlp-5.1.4\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "092d5d18-5d92-425f-8700-e51673bb6f6e"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azureml.fsspec"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:34:45.8407559Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T22:40:58.3137632Z",
              "execution_finish_time": "2023-11-21T22:41:10.9930062Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "4aa1f62d-0f4c-452f-9541-0e440f759c70"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting azureml.fsspec\n  Downloading azureml_fsspec-1.2.0-py3-none-any.whl (10 kB)\nCollecting azureml-dataprep<4.14.0a,>=4.12.0a\n  Downloading azureml_dataprep-4.12.7-py3-none-any.whl (38.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: fsspec>=2021.6.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azureml.fsspec) (2022.10.0)\nRequirement already satisfied: cloudpickle<3.0.0,>=1.1.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (2.2.0)\nRequirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (6.0)\nRequirement already satisfied: azure-identity>=1.7.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (1.7.0)\nRequirement already satisfied: dotnetcore2<4.0.0,>=3.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (3.1.23)\nRequirement already satisfied: azureml-dataprep-native<39.0.0,>=38.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (38.0.0)\nRequirement already satisfied: jsonschema in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (4.17.0)\nCollecting azureml-dataprep-rslex~=2.19.6dev0\n  Downloading azureml_dataprep_rslex-2.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting msal-extensions~=0.3.0\n  Downloading msal_extensions-0.3.1-py2.py3-none-any.whl (18 kB)\nRequirement already satisfied: azure-core<2.0.0,>=1.11.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (1.26.1)\nRequirement already satisfied: msal<2.0.0,>=1.12.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (1.18.0)\nRequirement already satisfied: six>=1.12.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (1.16.0)\nRequirement already satisfied: cryptography>=2.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (38.0.3)\nRequirement already satisfied: distro>=1.2.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from dotnetcore2<4.0.0,>=3.0.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (1.8.0)\nRequirement already satisfied: attrs>=17.4.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (22.1.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from jsonschema->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (0.19.2)\nRequirement already satisfied: typing-extensions>=4.0.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.11.0->azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (4.4.0)\nRequirement already satisfied: requests>=2.18.4 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.11.0->azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (2.28.1)\nRequirement already satisfied: cffi>=1.12 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from cryptography>=2.5->azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (1.15.1)\nRequirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from msal<2.0.0,>=1.12.0->azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (2.6.0)\nRequirement already satisfied: portalocker<3,>=1.0 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from msal-extensions~=0.3.0->azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (2.6.0)\nRequirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (2.21)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (1.26.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity>=1.7.0->azureml-dataprep<4.14.0a,>=4.12.0a->azureml.fsspec) (2022.9.24)\nInstalling collected packages: azureml-dataprep-rslex, msal-extensions, azureml-dataprep, azureml.fsspec\n  Attempting uninstall: azureml-dataprep-rslex\n    Found existing installation: azureml-dataprep-rslex 2.11.4\n    Uninstalling azureml-dataprep-rslex-2.11.4:\n      Successfully uninstalled azureml-dataprep-rslex-2.11.4\n  Attempting uninstall: msal-extensions\n    Found existing installation: msal-extensions 1.0.0\n    Uninstalling msal-extensions-1.0.0:\n      Successfully uninstalled msal-extensions-1.0.0\n  Attempting uninstall: azureml-dataprep\n    Found existing installation: azureml-dataprep 4.5.7\n    Uninstalling azureml-dataprep-4.5.7:\n      Successfully uninstalled azureml-dataprep-4.5.7\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nazureml-dataset-runtime 1.47.0 requires azureml-dataprep<4.6.0a,>=4.5.0a, but you have azureml-dataprep 4.12.7 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed azureml-dataprep-4.12.7 azureml-dataprep-rslex-2.19.6 azureml.fsspec-1.2.0 msal-extensions-0.3.1\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "f299fd27-57af-4398-aec1-fc38238dcae5"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import io\n",
        "from azureml.fsspec import AzureMachineLearningFileSystem"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:34:45.8422126Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T22:41:11.1029594Z",
              "execution_finish_time": "2023-11-21T22:41:31.8845434Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "efa3f74f-9fe9-4c23-93fb-2a1765f89bb7"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1700606492071
        },
        "tags": []
      },
      "id": "27188daa-3a2a-4ffd-8cee-219c908e6543"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in Data"
      ],
      "metadata": {},
      "id": "2a549ee9-cb62-4f0b-9f4b-08c230852ebf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comments"
      ],
      "metadata": {},
      "id": "d746ea7d-8af7-4905-9f44-f9fb8f4646d4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure Storage Account details\n",
        "connection_string = \"DefaultEndpointsProtocol=https;AccountName=group01astorage63e260f45;AccountKey=iGcY4Un0hlKMMqSs6BlLhmqNU0D7m8uJyVz2din6CTAp3AvM3QPH8/Tk8k+xN77D5R3KXvJZYBwX+AStLsNR5Q==;EndpointSuffix=core.windows.net\"\n",
        "container_name = \"azureml-blobstore-a1e50e78-9796-4cfe-a8bb-88f7de188a74\"\n",
        "\n",
        "# Initialize the BlobServiceClient\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "\n",
        "# create the filesystem\n",
        "fs = AzureMachineLearningFileSystem(\"azureml://subscriptions/71060237-912c-4e5c-849b-1a23626fc284/resourcegroups/project-rg/workspaces/group-01-aml/datastores/workspaceblobstore/paths/\")\n",
        "\n",
        "# Get the list of files\n",
        "files = fs.glob('comments/*.parquet')\n",
        "\n",
        "# Initialize an empty list to hold DataFrames\n",
        "dfs = []\n",
        "\n",
        "for file in files:  # Skip the first file\n",
        "    # Get the blob client for the file\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file)\n",
        "\n",
        "    # Download the blob to a stream\n",
        "    with io.BytesIO() as input_blob:\n",
        "        blob_client.download_blob().readinto(input_blob)\n",
        "        input_blob.seek(0)  # Go to the start of the stream\n",
        "\n",
        "        # Read the parquet file\n",
        "        df = pd.read_parquet(input_blob, engine='pyarrow')\n",
        "        dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list\n",
        "comments = pd.concat(dfs, ignore_index=True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "32",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-20T22:38:45.8655767Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-20T22:42:03.2626482Z",
              "execution_finish_time": "2023-11-20T22:43:51.8864071Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "4738c24b-35da-4efe-a71d-78607be90036"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 32, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception [Errno 2] No such file or directory: '/home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages/msal_extensions-1.0.0.dist-info/METADATA'.\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1700520232049
        },
        "tags": []
      },
      "id": "bbb0d9b7-76cd-4ed4-ba07-210616d80607"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submissions"
      ],
      "metadata": {},
      "id": "bfd6253d-319a-4c46-b1aa-599a4dd9542c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure Storage Account details\n",
        "connection_string = \"DefaultEndpointsProtocol=https;AccountName=group01astorage63e260f45;AccountKey=iGcY4Un0hlKMMqSs6BlLhmqNU0D7m8uJyVz2din6CTAp3AvM3QPH8/Tk8k+xN77D5R3KXvJZYBwX+AStLsNR5Q==;EndpointSuffix=core.windows.net\"\n",
        "container_name = \"azureml-blobstore-a1e50e78-9796-4cfe-a8bb-88f7de188a74\"\n",
        "\n",
        "# Initialize the BlobServiceClient\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "\n",
        "# create the filesystem\n",
        "fs = AzureMachineLearningFileSystem(\"azureml://subscriptions/71060237-912c-4e5c-849b-1a23626fc284/resourcegroups/project-rg/workspaces/group-01-aml/datastores/workspaceblobstore/paths/\")\n",
        "\n",
        "# Get the list of files\n",
        "files = fs.glob('submissions/*.parquet')\n",
        "\n",
        "# Initialize an empty list to hold DataFrames\n",
        "dfs = []\n",
        "\n",
        "for file in files:  # Skip the first file\n",
        "    # Get the blob client for the file\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file)\n",
        "\n",
        "    # Download the blob to a stream\n",
        "    with io.BytesIO() as input_blob:\n",
        "        blob_client.download_blob().readinto(input_blob)\n",
        "        input_blob.seek(0)  # Go to the start of the stream\n",
        "\n",
        "        # Read the parquet file\n",
        "        df = pd.read_parquet(input_blob, engine='pyarrow')\n",
        "        dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list\n",
        "submissions = pd.concat(dfs, ignore_index=True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:35:29.9030593Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T22:41:31.9928128Z",
              "execution_finish_time": "2023-11-21T22:41:59.5448582Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "006fa9a2-22ae-4a7f-8f6d-753e18ac12d5"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception [Errno 2] No such file or directory: '/home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages/msal_extensions-1.0.0.dist-info/METADATA'.\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700606519735
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "f1744e14-79c3-484d-ab05-ad7ba0a1d0e8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conduct Basic NLP and Text Checks"
      ],
      "metadata": {},
      "id": "f642c4cf-0e18-4b9f-a0fd-66735c8f546b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create pipeline for text cleaning"
      ],
      "metadata": {},
      "id": "42ae1926-3b55-4855-adb1-6df171ad15c6"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline(input_):\n",
        "\n",
        "    # Assemble document\n",
        "    document_assembler = DocumentAssembler() \\\n",
        "        .setInputCol(input_) \\\n",
        "        .setOutputCol(\"document\")\n",
        "\n",
        "    # Tokenize text\n",
        "    tokenizer = Tokenizer() \\\n",
        "        .setInputCols([\"document\"]) \\\n",
        "        .setOutputCol(\"token\")\n",
        "\n",
        "    # Normalize tokens\n",
        "    normalizer = Normalizer() \\\n",
        "        .setInputCols([\"token\"]) \\\n",
        "        .setOutputCol(\"normalized\") \\\n",
        "        .setLowercase(True) \\\n",
        "        .setCleanupPatterns([\"[^A-Za-z]+\"])\n",
        "\n",
        "    # Spell checking\n",
        "    spell_checker = NorvigSweetingModel.pretrained() \\\n",
        "        .setInputCols([\"normalized\"]) \\\n",
        "        .setOutputCol(\"corrected\")\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words_cleaner = StopWordsCleaner() \\\n",
        "        .setInputCols([\"corrected\"]) \\\n",
        "        .setOutputCol(\"cleaned\") \\\n",
        "        .setCaseSensitive(False) \\\n",
        "        .setStopWords(StopWordsCleaner().getStopWords()+['im','youre','k','dont','wont','couldnt'])\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\") \\\n",
        "        .setInputCols([\"cleaned\"]) \\\n",
        "        .setOutputCol(\"lemmatized\")\n",
        "\n",
        "    # Add Finisher\n",
        "    finisher = Finisher() \\\n",
        "        .setInputCols([\"lemmatized\"]) \\\n",
        "        .setOutputCols([\"cleaned_body\"]) \\\n",
        "        .setOutputAsArray(True) \\\n",
        "        .setCleanAnnotations(True)\n",
        "\n",
        "    # Build and run the pipeline\n",
        "    pipeline = Pipeline(stages=[\n",
        "        document_assembler,\n",
        "        tokenizer,\n",
        "        normalizer,\n",
        "        spell_checker,\n",
        "        stop_words_cleaner,\n",
        "        lemmatizer,\n",
        "        finisher\n",
        "    ])\n",
        "\n",
        "    return pipeline"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:35:34.3862222Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T22:41:59.6475201Z",
              "execution_finish_time": "2023-11-21T22:41:59.9484587Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "6255deca-a7cf-4bd4-b803-705978ecbde8"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1700606520095
        }
      },
      "id": "4037e9e2-36cf-4a5d-94d1-e82e6deecc6e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean submission body data through the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "82d10706-f4a4-4f1e-b096-0bbe8653b112"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# remove crosspost_parent and crosspost_parent_list due to NAs\n",
        "submissions = submissions.drop(columns=['crosspost_parent', 'crosspost_parent_list'])\n",
        "\n",
        "# specify schema\n",
        "schema = StructType([\n",
        "    StructField(\"author\", StringType(), True),\n",
        "    StructField(\"title\", StringType(), True),\n",
        "    StructField(\"selftext\", StringType(),True),\n",
        "    StructField(\"subreddit\", StringType(),True),\n",
        "    StructField(\"score\", IntegerType(),True),\n",
        "    StructField(\"num_comments\", StringType(),True),\n",
        "    StructField(\"permalink\", StringType(),True),\n",
        "    StructField(\"created_utc\", DateType(),True),\n",
        "    StructField(\"url\", StringType(),True),\n",
        "    StructField(\"domain\", StringType(),True),\n",
        "    StructField(\"is_video\", BooleanType(),True),\n",
        "    StructField(\"is_self\", BooleanType(),True),\n",
        "    StructField(\"is_reddit_media_domain\", BooleanType(),True),\n",
        "    StructField(\"spoiler\", BooleanType(),True),\n",
        "    StructField(\"over_18\", BooleanType(),True),\n",
        "    StructField(\"stickied\", BooleanType(),True),\n",
        "    StructField(\"thumbnail\", StringType(),True),\n",
        "    StructField(\"media\", StringType(),True),\n",
        "    StructField(\"secure_media\", StringType(),True),\n",
        "    StructField(\"gilded\", IntegerType(),True),\n",
        "    StructField(\"archived\", BooleanType(),True),\n",
        "    StructField(\"distinguished\", StringType(),True)])\n",
        "\n",
        "# convert submissions pandas df to spark df\n",
        "submissions_spark=spark.createDataFrame(submissions, schema=schema)\n",
        "# Run model\n",
        "pipeline = get_pipeline(input_='selftext')\n",
        "model = pipeline.fit(submissions_spark)\n",
        "cleaned_df_subs = model.transform(submissions_spark)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:35:35.4306007Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T22:42:00.054989Z",
              "execution_finish_time": "2023-11-21T22:43:03.1046065Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 4,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "collect at Feature.scala:235",
                    "dataWritten": 0,
                    "dataRead": 3638432,
                    "rowCount": 9108,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "collect at Feature.scala:235",
                    "description": "Job group for statement 11:\nfrom pyspark.sql.types import *\n\n# remove crosspost_parent and crosspost_parent_list due to NAs\nsubmissions = submissions.drop(columns=['crosspost_parent', 'crosspost_parent_list'])\n\n# specify schema\nschema = StructType([\n    StructField(\"author\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"selftext\", StringType(),True),\n    StructField(\"subreddit\", StringType(),True),\n    StructField(\"score\", IntegerType(),True),\n    StructField(\"num_comments\", StringType(),True),\n    StructField(\"permalink\", StringType(),True),\n    StructField(\"created_utc\", DateType(),True),\n    StructField(\"url\", StringType(),True),\n    StructField(\"domain\", StringType(),True),\n    StructField(\"is_video\", BooleanType(),True),\n    StructField(\"is_self\", BooleanType(),True),\n    StructField(\"is_reddit_media_domain\", BooleanType(),True),\n    StructField(\"spoiler\", BooleanType(),True),\n    StructField(\"over_18\", BooleanType(),True),\n    StructField(\"stickied\", BooleanType(),True),\n    StructField(\"thumb...",
                    "submissionTime": "2023-11-21T22:42:59.194GMT",
                    "completionTime": "2023-11-21T22:42:59.646GMT",
                    "stageIds": [
                      3
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 217,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 11:\nfrom pyspark.sql.types import *\n\n# remove crosspost_parent and crosspost_parent_list due to NAs\nsubmissions = submissions.drop(columns=['crosspost_parent', 'crosspost_parent_list'])\n\n# specify schema\nschema = StructType([\n    StructField(\"author\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"selftext\", StringType(),True),\n    StructField(\"subreddit\", StringType(),True),\n    StructField(\"score\", IntegerType(),True),\n    StructField(\"num_comments\", StringType(),True),\n    StructField(\"permalink\", StringType(),True),\n    StructField(\"created_utc\", DateType(),True),\n    StructField(\"url\", StringType(),True),\n    StructField(\"domain\", StringType(),True),\n    StructField(\"is_video\", BooleanType(),True),\n    StructField(\"is_self\", BooleanType(),True),\n    StructField(\"is_reddit_media_domain\", BooleanType(),True),\n    StructField(\"spoiler\", BooleanType(),True),\n    StructField(\"over_18\", BooleanType(),True),\n    StructField(\"stickied\", BooleanType(),True),\n    StructField(\"thumb...",
                    "submissionTime": "2023-11-21T22:42:58.849GMT",
                    "completionTime": "2023-11-21T22:42:59.012GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at Feature.scala:235",
                    "dataWritten": 0,
                    "dataRead": 21590671,
                    "rowCount": 50316,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "collect at Feature.scala:235",
                    "description": "Job group for statement 11:\nfrom pyspark.sql.types import *\n\n# remove crosspost_parent and crosspost_parent_list due to NAs\nsubmissions = submissions.drop(columns=['crosspost_parent', 'crosspost_parent_list'])\n\n# specify schema\nschema = StructType([\n    StructField(\"author\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"selftext\", StringType(),True),\n    StructField(\"subreddit\", StringType(),True),\n    StructField(\"score\", IntegerType(),True),\n    StructField(\"num_comments\", StringType(),True),\n    StructField(\"permalink\", StringType(),True),\n    StructField(\"created_utc\", DateType(),True),\n    StructField(\"url\", StringType(),True),\n    StructField(\"domain\", StringType(),True),\n    StructField(\"is_video\", BooleanType(),True),\n    StructField(\"is_self\", BooleanType(),True),\n    StructField(\"is_reddit_media_domain\", BooleanType(),True),\n    StructField(\"spoiler\", BooleanType(),True),\n    StructField(\"over_18\", BooleanType(),True),\n    StructField(\"stickied\", BooleanType(),True),\n    StructField(\"thumb...",
                    "submissionTime": "2023-11-21T22:42:41.580GMT",
                    "completionTime": "2023-11-21T22:42:47.020GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 12,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 12,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 12,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 607,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 11:\nfrom pyspark.sql.types import *\n\n# remove crosspost_parent and crosspost_parent_list due to NAs\nsubmissions = submissions.drop(columns=['crosspost_parent', 'crosspost_parent_list'])\n\n# specify schema\nschema = StructType([\n    StructField(\"author\", StringType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"selftext\", StringType(),True),\n    StructField(\"subreddit\", StringType(),True),\n    StructField(\"score\", IntegerType(),True),\n    StructField(\"num_comments\", StringType(),True),\n    StructField(\"permalink\", StringType(),True),\n    StructField(\"created_utc\", DateType(),True),\n    StructField(\"url\", StringType(),True),\n    StructField(\"domain\", StringType(),True),\n    StructField(\"is_video\", BooleanType(),True),\n    StructField(\"is_self\", BooleanType(),True),\n    StructField(\"is_reddit_media_domain\", BooleanType(),True),\n    StructField(\"spoiler\", BooleanType(),True),\n    StructField(\"over_18\", BooleanType(),True),\n    StructField(\"stickied\", BooleanType(),True),\n    StructField(\"thumb...",
                    "submissionTime": "2023-11-21T22:42:36.929GMT",
                    "completionTime": "2023-11-21T22:42:41.332GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "77128bf2-f7e4-45a7-8af5-3e1eeba8a812"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:425: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n  Expected a string or bytes dtype, got int64\nAttempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "spellcheck_norvig download started this may take some time.\nApproximate size to download 4.2 MB\n[OK!]\nlemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[OK!]\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1700606583340
        }
      },
      "id": "904b5968-e488-47eb-b446-0221be903bd8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean submission title data through the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "aab5a157-81c7-4b2f-94c3-84bc0a82e6a5"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline(input_):\n",
        "\n",
        "    # Assemble document\n",
        "    document_assembler = DocumentAssembler() \\\n",
        "        .setInputCol(input_) \\\n",
        "        .setOutputCol(\"document\")\n",
        "\n",
        "    # Tokenize text\n",
        "    tokenizer = Tokenizer() \\\n",
        "        .setInputCols([\"document\"]) \\\n",
        "        .setOutputCol(\"token\")\n",
        "\n",
        "    # Normalize tokens\n",
        "    normalizer = Normalizer() \\\n",
        "        .setInputCols([\"token\"]) \\\n",
        "        .setOutputCol(\"normalized\") \\\n",
        "        .setLowercase(True) \\\n",
        "        .setCleanupPatterns([\"[^A-Za-z]+\"])\n",
        "\n",
        "    # Spell checking\n",
        "    spell_checker = NorvigSweetingModel.pretrained() \\\n",
        "        .setInputCols([\"normalized\"]) \\\n",
        "        .setOutputCol(\"corrected\")\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words_cleaner = StopWordsCleaner() \\\n",
        "        .setInputCols([\"corrected\"]) \\\n",
        "        .setOutputCol(\"cleaned\") \\\n",
        "        .setCaseSensitive(False) \\\n",
        "        .setStopWords(StopWordsCleaner().getStopWords()+['im','youre','k','dont','wont','couldnt'])\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\") \\\n",
        "        .setInputCols([\"cleaned\"]) \\\n",
        "        .setOutputCol(\"lemmatized\")\n",
        "\n",
        "    # Add Finisher\n",
        "    finisher = Finisher() \\\n",
        "        .setInputCols([\"lemmatized\"]) \\\n",
        "        .setOutputCols([\"cleaned_title\"]) \\\n",
        "        .setOutputAsArray(True) \\\n",
        "        .setCleanAnnotations(True)\n",
        "\n",
        "    # Build and run the pipeline\n",
        "    pipeline = Pipeline(stages=[\n",
        "        document_assembler,\n",
        "        tokenizer,\n",
        "        normalizer,\n",
        "        spell_checker,\n",
        "        stop_words_cleaner,\n",
        "        lemmatizer,\n",
        "        finisher\n",
        "    ])\n",
        "\n",
        "    return pipeline"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:35:38.2445936Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T22:43:03.2241885Z",
              "execution_finish_time": "2023-11-21T22:43:03.5279221Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "ea8cbc93-ad84-4487-af37-9e64c5670b9d"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 12, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700606583689
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b9034bfc-38a0-4dea-8334-2a47c28e70f2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model\n",
        "pipeline = get_pipeline(input_='title')\n",
        "model = pipeline.fit(cleaned_df_subs)\n",
        "submissions_cleaned = model.transform(cleaned_df_subs)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:35:41.5396433Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T22:43:03.6389735Z",
              "execution_finish_time": "2023-11-21T22:43:13.735774Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "abe6a7d0-008f-438f-806d-9cbe2df74f25"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 13, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "spellcheck_norvig download started this may take some time.\nApproximate size to download 4.2 MB\n[OK!]\nlemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[OK!]\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700606593873
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "529b0592-efd5-4707-8b8a-fe36605b0676"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean comment body data through the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "137b3b30-f4f5-4162-bbf2-d5700c83b28f"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline(input_):\n",
        "\n",
        "    # Assemble document\n",
        "    document_assembler = DocumentAssembler() \\\n",
        "        .setInputCol(input_) \\\n",
        "        .setOutputCol(\"document\")\n",
        "\n",
        "    # Tokenize text\n",
        "    tokenizer = Tokenizer() \\\n",
        "        .setInputCols([\"document\"]) \\\n",
        "        .setOutputCol(\"token\")\n",
        "\n",
        "    # Normalize tokens\n",
        "    normalizer = Normalizer() \\\n",
        "        .setInputCols([\"token\"]) \\\n",
        "        .setOutputCol(\"normalized\") \\\n",
        "        .setLowercase(True) \\\n",
        "        .setCleanupPatterns([\"[^A-Za-z]+\"])\n",
        "\n",
        "    # Spell checking\n",
        "    spell_checker = NorvigSweetingModel.pretrained() \\\n",
        "        .setInputCols([\"normalized\"]) \\\n",
        "        .setOutputCol(\"corrected\")\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words_cleaner = StopWordsCleaner() \\\n",
        "        .setInputCols([\"corrected\"]) \\\n",
        "        .setOutputCol(\"cleaned\") \\\n",
        "        .setCaseSensitive(False) \\\n",
        "        .setStopWords(StopWordsCleaner().getStopWords()+['im','youre','k','dont','wont','couldnt'])\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\") \\\n",
        "        .setInputCols([\"cleaned\"]) \\\n",
        "        .setOutputCol(\"lemmatized\")\n",
        "\n",
        "    # Add Finisher\n",
        "    finisher = Finisher() \\\n",
        "        .setInputCols([\"lemmatized\"]) \\\n",
        "        .setOutputCols([\"cleaned_body\"]) \\\n",
        "        .setOutputAsArray(True) \\\n",
        "        .setCleanAnnotations(True)\n",
        "\n",
        "    # Build and run the pipeline\n",
        "    pipeline = Pipeline(stages=[\n",
        "        document_assembler,\n",
        "        tokenizer,\n",
        "        normalizer,\n",
        "        spell_checker,\n",
        "        stop_words_cleaner,\n",
        "        lemmatizer,\n",
        "        finisher\n",
        "    ])\n",
        "\n",
        "    return pipeline"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "32",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-20T22:39:31.7300719Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-20T22:43:52.0039588Z",
              "execution_finish_time": "2023-11-20T22:43:52.3121133Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "daa50dc7-8ace-48e8-8444-a6006a2c96a5"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 32, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700520232410
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "5f1ce69b-4541-4ac1-ae0d-9e72014f6c94"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# remove author_flair_css_class and author_cakeday due to NAs\n",
        "comments = comments.drop(columns=['author_flair_css_class', 'author_cakeday'])\n",
        "\n",
        "# specify schema\n",
        "schema = StructType([\n",
        "    StructField(\"author\", StringType(), True),\n",
        "    StructField(\"body\", StringType(),True),\n",
        "    StructField(\"can_gild\", StringType(),True),\n",
        "    StructField(\"controversiality\", IntegerType(),True),\n",
        "    StructField(\"created_utc\", DateType(),True),\n",
        "    StructField(\"distinguished\", StringType(),True),\n",
        "    StructField(\"edited\", StringType(),True),\n",
        "    StructField(\"gilded\", IntegerType(),True),\n",
        "    StructField(\"id\", StringType(),True),\n",
        "    StructField(\"is_submitter\", BooleanType(),True),\n",
        "    StructField(\"link_id\", StringType(),True),\n",
        "    StructField(\"parent_id\", StringType(),True),\n",
        "    StructField(\"permalink\", StringType(),True),\n",
        "    StructField(\"retrieved_on\", DateType(),True),\n",
        "    StructField(\"score\", IntegerType(),True),\n",
        "    StructField(\"stickied\", BooleanType(),True),\n",
        "    StructField(\"subreddit\", StringType(),True),\n",
        "    StructField(\"subreddit_id\", StringType(),True)])\n",
        "\n",
        "# convert submissions pandas df to spark df\n",
        "comments_spark=spark.createDataFrame(comments, schema=schema)\n",
        "# Run model\n",
        "pipeline = get_pipeline(input_='body')\n",
        "model = pipeline.fit(comments_spark)\n",
        "comments_cleaned = model.transform(comments_spark)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "32",
              "statement_id": 11,
              "state": "submitted",
              "livy_statement_state": "running",
              "queued_time": "2023-11-20T22:39:32.9772183Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-20T22:43:52.4208819Z",
              "execution_finish_time": null,
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "FAILED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "faff6c63-fff7-468e-bf04-107a5e68d341"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 32, 11, Submitted, Running)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:425: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n  Expected a string or bytes dtype, got bool\nAttempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n"
        },
        {
          "output_type": "error",
          "ename": "InvalidHttpRequestToLivy",
          "evalue": "Submission failed due to error content =[\"requirement failed: Session isn't active.\"] HTTP status code: 400. Trace ID: 693095dc-4b95-4b78-8b23-0e122d26f05e.",
          "traceback": [
            "InvalidHttpRequestToLivy: Submission failed due to error content =[\"requirement failed: Session isn't active.\"] HTTP status code: 400. Trace ID: 693095dc-4b95-4b78-8b23-0e122d26f05e."
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700520711124
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "eda7d962-cc7b-4094-9da7-6974b6b72eed"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export cleaned submissions and comments as parquet"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "199fcd01-1065-47a7-b395-d4d807858c7b"
    },
    {
      "cell_type": "code",
      "source": [
        "workspace_default_storage_account = \"group01astorage63e260f45\"\n",
        "workspace_default_container = \"azureml-blobstore-a1e50e78-9796-4cfe-a8bb-88f7de188a74\"\n",
        "\n",
        "workspace_wasbs_base_url = (\n",
        "    f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\")\n",
        "\n",
        "# save submissions\n",
        "submissions_cleaned.repartition(64).write.mode(\"overwrite\").parquet(f\"{workspace_wasbs_base_url}nlp_cleaned_teg/submissions\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-11-18T22:48:23.6750055Z",
              "execution_start_time": "2023-11-18T22:46:58.9242219Z",
              "livy_statement_state": "available",
              "parent_msg_id": "9c01dcab-3f78-4cd3-b2d9-e3c8815681ed",
              "queued_time": "2023-11-18T22:35:21.9018336Z",
              "session_id": "8",
              "session_start_time": null,
              "spark_jobs": {
                "jobs": [
                  {
                    "completionTime": "2023-11-18T22:48:22.050GMT",
                    "dataRead": 283233607,
                    "dataWritten": 182514458,
                    "description": "Job group for statement 17:\nworkspace_default_storage_account = \"group01astorage63e260f45\"\nworkspace_default_container = \"azureml-blobstore-a1e50e78-9796-4cfe-a8bb-88f7de188a74\"\n\nworkspace_wasbs_base_url = (\n    f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\")\n\n# save submissions\nsubmissions_cleaned.repartition(64).write.mode(\"overwrite\").parquet(f\"{workspace_wasbs_base_url}nlp_cleaned_teg/submissions\")",
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "jobGroup": "17",
                    "jobId": 5,
                    "killedTasksSummary": {},
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 64,
                    "numCompletedStages": 1,
                    "numCompletedTasks": 64,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 1,
                    "numSkippedTasks": 32,
                    "numTasks": 96,
                    "rowCount": 1199816,
                    "stageIds": [
                      5,
                      6
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T22:48:17.527GMT",
                    "usageDescription": ""
                  },
                  {
                    "completionTime": "2023-11-18T22:48:17.442GMT",
                    "dataRead": 0,
                    "dataWritten": 283233607,
                    "description": "Job group for statement 17:\nworkspace_default_storage_account = \"group01astorage63e260f45\"\nworkspace_default_container = \"azureml-blobstore-a1e50e78-9796-4cfe-a8bb-88f7de188a74\"\n\nworkspace_wasbs_base_url = (\n    f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\")\n\n# save submissions\nsubmissions_cleaned.repartition(64).write.mode(\"overwrite\").parquet(f\"{workspace_wasbs_base_url}nlp_cleaned_teg/submissions\")",
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "jobGroup": "17",
                    "jobId": 4,
                    "killedTasksSummary": {},
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 32,
                    "numCompletedStages": 1,
                    "numCompletedTasks": 32,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 0,
                    "numSkippedTasks": 0,
                    "numTasks": 32,
                    "rowCount": 599908,
                    "stageIds": [
                      4
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T22:47:01.088GMT",
                    "usageDescription": ""
                  }
                ],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 2,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "7bcc356e-2304-45b0-a972-3b83d052d5ce",
              "state": "finished",
              "statement_id": 17
            },
            "text/plain": "StatementMeta(7bcc356e-2304-45b0-a972-3b83d052d5ce, 8, 17, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700347701731
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "f686ed13-237e-4a13-97d5-4bc77ba75097"
    },
    {
      "cell_type": "code",
      "source": [
        "# save comments\n",
        "\n",
        "workspace_default_storage_account = \"group01astorage63e260f45\"\n",
        "workspace_default_container = \"azureml-blobstore-a1e50e78-9796-4cfe-a8bb-88f7de188a74\"\n",
        "\n",
        "workspace_wasbs_base_url = (\n",
        "    f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\")\n",
        "\n",
        "comments_cleaned.repartition(64).write.mode(\"overwrite\").parquet(f\"{workspace_wasbs_base_url}nlp_cleaned_teg/comments\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": "2023-11-18T22:48:23.7785564Z",
              "livy_statement_state": "running",
              "parent_msg_id": "6bbaf03d-cad7-4d4b-ad7c-559307885e1e",
              "queued_time": "2023-11-18T22:35:29.7599492Z",
              "session_id": "8",
              "session_start_time": null,
              "spark_jobs": {
                "jobs": [
                  {
                    "dataRead": 0,
                    "dataWritten": 6283484975,
                    "description": "Job group for statement 18:\n# save comments\n\nworkspace_default_storage_account = \"group01astorage63e260f45\"\nworkspace_default_container = \"azureml-blobstore-a1e50e78-9796-4cfe-a8bb-88f7de188a74\"\n\nworkspace_wasbs_base_url = (\n    f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\")\n\ncomments_cleaned.repartition(64).write.mode(\"overwrite\").parquet(f\"{workspace_wasbs_base_url}nlp_cleaned_teg/comments\")",
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "jobGroup": "18",
                    "jobId": 6,
                    "killedTasksSummary": {},
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "numActiveStages": 1,
                    "numActiveTasks": 6,
                    "numCompletedIndices": 26,
                    "numCompletedStages": 0,
                    "numCompletedTasks": 26,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 0,
                    "numSkippedTasks": 0,
                    "numTasks": 32,
                    "rowCount": 12077173,
                    "stageIds": [
                      7
                    ],
                    "status": "RUNNING",
                    "submissionTime": "2023-11-18T22:48:24.580GMT",
                    "usageDescription": ""
                  }
                ],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 1,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "7bcc356e-2304-45b0-a972-3b83d052d5ce",
              "state": "submitted",
              "statement_id": 18
            },
            "text/plain": "StatementMeta(7bcc356e-2304-45b0-a972-3b83d052d5ce, 8, 18, Submitted, Running)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700324882164
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "99db6bcc-49ca-44d2-99cc-2461691d7b66"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF, Common Words, and Text Length"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "027a4ff4-4ed0-4bd7-af5b-5d46c99e0892"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine the most important words in politics subreddits submissions body and title using TF-IDF"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "d16047bf-465d-4db1-898c-8193fa3f5850"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF, HashingTF, StopWordsRemover\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, MapType, StringType\n",
        "import string\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import SQLTransformer\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer, Normalizer, StopWordsCleaner, LemmatizerModel"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:35:58.9601749Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T22:43:13.8537743Z",
              "execution_finish_time": "2023-11-21T22:43:14.1545401Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "ca0d31bf-768a-4c92-ad66-3e8dd20a7d19"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 14, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700606594271
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "1c9389ef-eef0-4170-9249-c2e4dc51365e"
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_subreddits = [\"Economics\", \"finance\"]\n",
        "\n",
        "# Concat body, title and filter for political subreddits\n",
        "pol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits)) \\\n",
        "                                    .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\\\n",
        "\n",
        "# Split the nlpbodytext into an array of words\n",
        "pol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n",
        "\n",
        "# Define HashingTF\n",
        "hashingTF = HashingTF(inputCol=\"array_words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n",
        "\n",
        "# Define IDF\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "words = pol_subs_body_title_words.select('array_words')\n",
        "\n",
        "# Hashing frequency\n",
        "tf = hashingTF.transform(words)\n",
        "\n",
        "# IDF\n",
        "idf_model = idf.fit(tf)\n",
        "\n",
        "# TFIDF\n",
        "tfidf = idf_model.transform(tf)\n",
        "\n",
        "ndf = tfidf.select(F.explode('array_words').name('expwords')).withColumn('words',F.array('expwords'))\n",
        "\n",
        "# Define HashingTF\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n",
        "\n",
        "hashudf = F.udf(lambda vector : vector.indices.tolist()[0],F.StringType())\n",
        "wordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(F.col('rawFeatures')))\n",
        "wordtf = wordtf.dropDuplicates([\"expwords\"])\n",
        "\n",
        "udf1 = F.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))\n",
        "valuedf = tfidf.select(F.explode(udf1(F.col('features'))).name('wordhash','value'))\n",
        "valuedf = valuedf.dropDuplicates([\"wordhash\"])\n",
        "join_importance = wordtf.join(valuedf, wordtf.wordhash == valuedf.wordhash, 'inner').select(wordtf.expwords, valuedf.value)\n",
        "top100_pol_subs = join_importance.sort(F.col(\"value\").desc()).limit(100).toPandas()\n",
        "\n",
        "top100_pol_subs = top100_pol_subs.reset_index(drop=True)\n",
        "top100_pol_subs = top100_pol_subs.rename(columns={\"expwords\":\"word\", \"value\":\"tfidf_value\"})"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-11-18T17:18:26.2957898Z",
              "execution_start_time": "2023-11-18T17:14:13.0965607Z",
              "livy_statement_state": "available",
              "parent_msg_id": "eeaa1293-7952-4907-8f4e-a60a8493b4a5",
              "queued_time": "2023-11-18T17:14:13.0015627Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": {
                "jobs": [
                  {
                    "completionTime": "2023-11-18T17:18:24.831GMT",
                    "dataRead": 15122927,
                    "dataWritten": 28902,
                    "description": "Job group for statement 28:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"array_words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = pol_subs_body_title_words.select('array_words')\n\n# Hashing frequency\ntf = hashingTF.transform(words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('array_words').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define Has...",
                    "displayName": "toPandas at /tmp/ipykernel_25516/939292906.py:39",
                    "jobGroup": "28",
                    "jobId": 12,
                    "killedTasksSummary": {},
                    "name": "toPandas at /tmp/ipykernel_25516/939292906.py:39",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 15,
                    "numCompletedStages": 2,
                    "numCompletedTasks": 15,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 1,
                    "numSkippedTasks": 32,
                    "numTasks": 47,
                    "rowCount": 541826,
                    "stageIds": [
                      16,
                      17,
                      18
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T17:18:19.469GMT",
                    "usageDescription": ""
                  },
                  {
                    "completionTime": "2023-11-18T17:17:38.470GMT",
                    "dataRead": 0,
                    "dataWritten": 15094025,
                    "description": "Job group for statement 28:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"array_words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = pol_subs_body_title_words.select('array_words')\n\n# Hashing frequency\ntf = hashingTF.transform(words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('array_words').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define Has...",
                    "displayName": "toPandas at /tmp/ipykernel_25516/939292906.py:39",
                    "jobGroup": "28",
                    "jobId": 10,
                    "killedTasksSummary": {},
                    "name": "toPandas at /tmp/ipykernel_25516/939292906.py:39",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 32,
                    "numCompletedStages": 1,
                    "numCompletedTasks": 32,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 0,
                    "numSkippedTasks": 0,
                    "numTasks": 32,
                    "rowCount": 539026,
                    "stageIds": [
                      13
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T17:15:30.218GMT",
                    "usageDescription": ""
                  },
                  {
                    "completionTime": "2023-11-18T17:18:16.510GMT",
                    "dataRead": 0,
                    "dataWritten": 15785129,
                    "description": "Job group for statement 28:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"array_words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = pol_subs_body_title_words.select('array_words')\n\n# Hashing frequency\ntf = hashingTF.transform(words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('array_words').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define Has...",
                    "displayName": "toPandas at /tmp/ipykernel_25516/939292906.py:39",
                    "jobGroup": "28",
                    "jobId": 9,
                    "killedTasksSummary": {},
                    "name": "toPandas at /tmp/ipykernel_25516/939292906.py:39",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 32,
                    "numCompletedStages": 1,
                    "numCompletedTasks": 32,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 0,
                    "numSkippedTasks": 0,
                    "numTasks": 32,
                    "rowCount": 539180,
                    "stageIds": [
                      12
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T17:15:27.256GMT",
                    "usageDescription": ""
                  },
                  {
                    "completionTime": "2023-11-18T17:15:24.825GMT",
                    "dataRead": 8297434,
                    "dataWritten": 8297434,
                    "description": "Job group for statement 28:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"array_words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = pol_subs_body_title_words.select('array_words')\n\n# Hashing frequency\ntf = hashingTF.transform(words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('array_words').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define Has...",
                    "displayName": "treeAggregate at IDF.scala:55",
                    "jobGroup": "28",
                    "jobId": 8,
                    "killedTasksSummary": {},
                    "name": "treeAggregate at IDF.scala:55",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 37,
                    "numCompletedStages": 2,
                    "numCompletedTasks": 37,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 0,
                    "numSkippedTasks": 0,
                    "numTasks": 37,
                    "rowCount": 64,
                    "stageIds": [
                      10,
                      11
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T17:14:13.255GMT",
                    "usageDescription": ""
                  }
                ],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 4,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "7bcc356e-2304-45b0-a972-3b83d052d5ce",
              "state": "finished",
              "statement_id": 28
            },
            "text/plain": "StatementMeta(7bcc356e-2304-45b0-a972-3b83d052d5ce, 5, 28, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700327904359
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "4048c675-92e1-4b42-a1c8-08f41347c834"
    },
    {
      "cell_type": "code",
      "source": [
        "top100_pol_subs.to_csv(\"Users/tg693/fall-2023-reddit-project-team-01/data/csv/tfidf_pol_subs.csv\",index=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-11-18T17:40:33.3310424Z",
              "execution_start_time": "2023-11-18T17:40:31.8308094Z",
              "livy_statement_state": "available",
              "parent_msg_id": "57273044-6d7a-4f89-b3e3-affe1803f881",
              "queued_time": "2023-11-18T17:40:31.7399185Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": {
                "jobs": [],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "7bcc356e-2304-45b0-a972-3b83d052d5ce",
              "state": "finished",
              "statement_id": 38
            },
            "text/plain": "StatementMeta(7bcc356e-2304-45b0-a972-3b83d052d5ce, 5, 38, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700329233423
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "663efd29-8623-40aa-a07e-6e25fb474ad8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine the most important words in economics subreddits submissions body and title using TF-IDF"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "fca54564-a1f2-4d80-8390-c958082521b4"
    },
    {
      "cell_type": "code",
      "source": [
        "econ_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits)) \\\n",
        "                                       .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\\\n",
        "                                       .alias(\"nlpbodytext\")\n",
        "\n",
        "# Split the nlpbodytext into an array of words\n",
        "econ_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n",
        "\n",
        "# Define HashingTF\n",
        "hashingTF = HashingTF(inputCol=\"array_words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n",
        "\n",
        "# Define IDF\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "words = econ_subs_body_title_words.select('array_words')\n",
        "\n",
        "# Hashing frequency\n",
        "tf = hashingTF.transform(econ_subs_body_title_words)\n",
        "\n",
        "# IDF\n",
        "idf_model = idf.fit(tf)\n",
        "\n",
        "# TFIDF\n",
        "tfidf = idf_model.transform(tf)\n",
        "\n",
        "ndf = tfidf.select(F.explode('array_words').name('expwords')).withColumn('words',F.array('expwords'))\n",
        "\n",
        "# Define HashingTF\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n",
        "\n",
        "hashudf = F.udf(lambda vector : vector.indices.tolist()[0],F.StringType())\n",
        "wordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(F.col('rawFeatures')))\n",
        "wordtf = wordtf.dropDuplicates([\"expwords\"])\n",
        "\n",
        "udf1 = F.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))\n",
        "valuedf = tfidf.select(F.explode(udf1(F.col('features'))).name('wordhash','value'))\n",
        "valuedf = valuedf.dropDuplicates([\"wordhash\"])\n",
        "join_importance = wordtf.join(valuedf, wordtf.wordhash == valuedf.wordhash, 'inner').select(wordtf.expwords, valuedf.value)\n",
        "top100_econ_subs = join_importance.sort(F.col(\"value\").desc()).limit(100).toPandas()\n",
        "\n",
        "top100_econ_subs = top100_econ_subs.reset_index(drop=True)\n",
        "top100_econ_subs = top100_econ_subs.rename(columns={\"expwords\":\"word\", \"value\":\"tfidf_value\"})"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-11-18T17:52:08.9726581Z",
              "execution_start_time": "2023-11-18T17:51:21.1724971Z",
              "livy_statement_state": "available",
              "parent_msg_id": "1f3b0360-bb8d-4aab-8ec5-7446b4281818",
              "queued_time": "2023-11-18T17:51:21.0637862Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": {
                "jobs": [
                  {
                    "completionTime": "2023-11-18T17:52:07.114GMT",
                    "dataRead": 3769167,
                    "dataWritten": 5012,
                    "description": "Job group for statement 39:\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                        .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))                                       .alias(\"nlpbodytext\")\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"array_words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = econ_subs_body_title_words.select('array_words')\n\n# Hashing frequency\ntf = hashingTF.transform(econ_subs_body_title_words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('array_words').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define HashingTF\nhashingT...",
                    "displayName": "toPandas at /tmp/ipykernel_25516/2496054437.py:38",
                    "jobGroup": "39",
                    "jobId": 17,
                    "killedTasksSummary": {},
                    "name": "toPandas at /tmp/ipykernel_25516/2496054437.py:38",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 4,
                    "numCompletedStages": 2,
                    "numCompletedTasks": 4,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 1,
                    "numSkippedTasks": 32,
                    "numTasks": 36,
                    "rowCount": 127303,
                    "stageIds": [
                      27,
                      25,
                      26
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T17:52:02.360GMT",
                    "usageDescription": ""
                  },
                  {
                    "completionTime": "2023-11-18T17:51:56.246GMT",
                    "dataRead": 0,
                    "dataWritten": 4037202,
                    "description": "Job group for statement 39:\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                        .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))                                       .alias(\"nlpbodytext\")\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"array_words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = econ_subs_body_title_words.select('array_words')\n\n# Hashing frequency\ntf = hashingTF.transform(econ_subs_body_title_words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('array_words').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define HashingTF\nhashingT...",
                    "displayName": "toPandas at /tmp/ipykernel_25516/2496054437.py:38",
                    "jobGroup": "39",
                    "jobId": 15,
                    "killedTasksSummary": {},
                    "name": "toPandas at /tmp/ipykernel_25516/2496054437.py:38",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 32,
                    "numCompletedStages": 1,
                    "numCompletedTasks": 32,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 0,
                    "numSkippedTasks": 0,
                    "numTasks": 32,
                    "rowCount": 126734,
                    "stageIds": [
                      22
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T17:51:41.058GMT",
                    "usageDescription": ""
                  },
                  {
                    "completionTime": "2023-11-18T17:52:00.512GMT",
                    "dataRead": 0,
                    "dataWritten": 3764155,
                    "description": "Job group for statement 39:\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                        .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))                                       .alias(\"nlpbodytext\")\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"array_words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = econ_subs_body_title_words.select('array_words')\n\n# Hashing frequency\ntf = hashingTF.transform(econ_subs_body_title_words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('array_words').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define HashingTF\nhashingT...",
                    "displayName": "toPandas at /tmp/ipykernel_25516/2496054437.py:38",
                    "jobGroup": "39",
                    "jobId": 14,
                    "killedTasksSummary": {},
                    "name": "toPandas at /tmp/ipykernel_25516/2496054437.py:38",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 32,
                    "numCompletedStages": 1,
                    "numCompletedTasks": 32,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 0,
                    "numSkippedTasks": 0,
                    "numTasks": 32,
                    "rowCount": 126703,
                    "stageIds": [
                      21
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T17:51:38.327GMT",
                    "usageDescription": ""
                  },
                  {
                    "completionTime": "2023-11-18T17:51:34.690GMT",
                    "dataRead": 4866078,
                    "dataWritten": 4866078,
                    "description": "Job group for statement 39:\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                        .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))                                       .alias(\"nlpbodytext\")\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"array_words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = econ_subs_body_title_words.select('array_words')\n\n# Hashing frequency\ntf = hashingTF.transform(econ_subs_body_title_words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('array_words').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define HashingTF\nhashingT...",
                    "displayName": "treeAggregate at IDF.scala:55",
                    "jobGroup": "39",
                    "jobId": 13,
                    "killedTasksSummary": {},
                    "name": "treeAggregate at IDF.scala:55",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 37,
                    "numCompletedStages": 2,
                    "numCompletedTasks": 37,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 0,
                    "numSkippedTasks": 0,
                    "numTasks": 37,
                    "rowCount": 64,
                    "stageIds": [
                      19,
                      20
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T17:51:21.357GMT",
                    "usageDescription": ""
                  }
                ],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 4,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "7bcc356e-2304-45b0-a972-3b83d052d5ce",
              "state": "finished",
              "statement_id": 39
            },
            "text/plain": "StatementMeta(7bcc356e-2304-45b0-a972-3b83d052d5ce, 5, 39, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 37,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700329929235
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "1e217d8f-235f-48d9-8d15-6a7ae8437d71"
    },
    {
      "cell_type": "code",
      "source": [
        "top100_econ_subs.to_csv(\"Users/tg693/fall-2023-reddit-project-team-01/data/csv/tfidf_econ_subs.csv\",index=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-11-18T17:52:26.0809184Z",
              "execution_start_time": "2023-11-18T17:52:25.2867965Z",
              "livy_statement_state": "available",
              "parent_msg_id": "83fb1a33-8867-4736-b411-869aebdfa104",
              "queued_time": "2023-11-18T17:52:25.1954468Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": {
                "jobs": [],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "7bcc356e-2304-45b0-a972-3b83d052d5ce",
              "state": "finished",
              "statement_id": 41
            },
            "text/plain": "StatementMeta(7bcc356e-2304-45b0-a972-3b83d052d5ce, 5, 41, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 39,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700329946187
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "36618b46-5d19-47f9-a6ce-e76ce48973e0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine the most important words in politics subreddits comments body using TF-IDF"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "7f23d366-e07f-4fc1-b6b2-17312039b26f"
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_subreddits = [\"Economics\", \"finance\"]\n",
        "\n",
        "# Concat body, title and filter for political subreddits\n",
        "pol_comms_body_words = comments_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits)) \\\n",
        "                                               .select(F.col(\"cleaned_body\"))\\\n",
        "\n",
        "# Define HashingTF\n",
        "hashingTF = HashingTF(inputCol=\"cleaned_body\", outputCol=\"rawFeatures\",numFeatures=40000000)\n",
        "\n",
        "# Define IDF\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "words = pol_comms_body_words.select('cleaned_body')\n",
        "\n",
        "# Hashing frequency\n",
        "tf = hashingTF.transform(words)\n",
        "\n",
        "# IDF\n",
        "idf_model = idf.fit(tf)\n",
        "\n",
        "# TFIDF\n",
        "tfidf = idf_model.transform(tf)\n",
        "\n",
        "ndf = tfidf.select(F.explode('cleaned_body').name('expwords')).withColumn('words',F.array('expwords'))\n",
        "\n",
        "# Define HashingTF\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n",
        "\n",
        "hashudf = F.udf(lambda vector : vector.indices.tolist()[0],F.StringType())\n",
        "wordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(F.col('rawFeatures')))\n",
        "wordtf = wordtf.dropDuplicates([\"expwords\"])\n",
        "\n",
        "udf1 = F.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))\n",
        "valuedf = tfidf.select(F.explode(udf1(F.col('features'))).name('wordhash','value'))\n",
        "valuedf = valuedf.dropDuplicates([\"wordhash\"])\n",
        "join_importance = wordtf.join(valuedf, wordtf.wordhash == valuedf.wordhash, 'inner').select(wordtf.expwords, valuedf.value)\n",
        "top100_pol_comms = join_importance.sort(F.col(\"value\").desc()).limit(100).toPandas()\n",
        "\n",
        "top100_pol_comms = top100_pol_comms.reset_index(drop=True)\n",
        "top100_pol_comms = top100_pol_comms.rename(columns={\"expwords\":\"word\", \"value\":\"tfidf_value\"})"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-11-18T20:04:38.0682279Z",
              "execution_start_time": "2023-11-18T18:04:51.8704607Z",
              "livy_statement_state": "available",
              "parent_msg_id": "b42c9ff9-2c4b-4d71-98f3-7ca37477af5a",
              "queued_time": "2023-11-18T18:04:51.7708831Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": {
                "jobs": [
                  {
                    "completionTime": "2023-11-18T20:04:35.864GMT",
                    "dataRead": 130453535,
                    "dataWritten": 117340,
                    "description": "Job group for statement 42:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_comms_body_words = comments_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                                .select(F.col(\"cleaned_body\"))\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"cleaned_body\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = pol_comms_body_words.select('cleaned_body')\n\n# Hashing frequency\ntf = hashingTF.transform(words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('cleaned_body').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\nhashudf = F.udf(lambda vector : vector.indices.tolist()[0],F.StringType())\nwordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(F.col('rawFeatures')))\nwordtf = w...",
                    "displayName": "toPandas at /tmp/ipykernel_25516/3184604692.py:37",
                    "jobGroup": "42",
                    "jobId": 22,
                    "killedTasksSummary": {},
                    "name": "toPandas at /tmp/ipykernel_25516/3184604692.py:37",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 35,
                    "numCompletedStages": 2,
                    "numCompletedTasks": 35,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 3,
                    "numSkippedTasks": 98,
                    "numTasks": 133,
                    "rowCount": 4878114,
                    "stageIds": [
                      37,
                      34,
                      38,
                      35,
                      36
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T20:04:29.046GMT",
                    "usageDescription": ""
                  },
                  {
                    "completionTime": "2023-11-18T20:04:26.231GMT",
                    "dataRead": 113956669,
                    "dataWritten": 63799124,
                    "description": "Job group for statement 42:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_comms_body_words = comments_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                                .select(F.col(\"cleaned_body\"))\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"cleaned_body\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = pol_comms_body_words.select('cleaned_body')\n\n# Hashing frequency\ntf = hashingTF.transform(words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('cleaned_body').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\nhashudf = F.udf(lambda vector : vector.indices.tolist()[0],F.StringType())\nwordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(F.col('rawFeatures')))\nwordtf = w...",
                    "displayName": "toPandas at /tmp/ipykernel_25516/3184604692.py:37",
                    "jobGroup": "42",
                    "jobId": 21,
                    "killedTasksSummary": {},
                    "name": "toPandas at /tmp/ipykernel_25516/3184604692.py:37",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 34,
                    "numCompletedStages": 1,
                    "numCompletedTasks": 34,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 1,
                    "numSkippedTasks": 32,
                    "numTasks": 66,
                    "rowCount": 4876654,
                    "stageIds": [
                      33,
                      32
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T20:04:25.627GMT",
                    "usageDescription": ""
                  },
                  {
                    "completionTime": "2023-11-18T19:40:05.080GMT",
                    "dataRead": 0,
                    "dataWritten": 66537071,
                    "description": "Job group for statement 42:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_comms_body_words = comments_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                                .select(F.col(\"cleaned_body\"))\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"cleaned_body\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = pol_comms_body_words.select('cleaned_body')\n\n# Hashing frequency\ntf = hashingTF.transform(words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('cleaned_body').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\nhashudf = F.udf(lambda vector : vector.indices.tolist()[0],F.StringType())\nwordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(F.col('rawFeatures')))\nwordtf = w...",
                    "displayName": "toPandas at /tmp/ipykernel_25516/3184604692.py:37",
                    "jobGroup": "42",
                    "jobId": 20,
                    "killedTasksSummary": {},
                    "name": "toPandas at /tmp/ipykernel_25516/3184604692.py:37",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 32,
                    "numCompletedStages": 1,
                    "numCompletedTasks": 32,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 0,
                    "numSkippedTasks": 0,
                    "numTasks": 32,
                    "rowCount": 3457574,
                    "stageIds": [
                      31
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T18:31:47.040GMT",
                    "usageDescription": ""
                  },
                  {
                    "completionTime": "2023-11-18T20:04:25.573GMT",
                    "dataRead": 0,
                    "dataWritten": 113956669,
                    "description": "Job group for statement 42:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_comms_body_words = comments_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                                .select(F.col(\"cleaned_body\"))\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"cleaned_body\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = pol_comms_body_words.select('cleaned_body')\n\n# Hashing frequency\ntf = hashingTF.transform(words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('cleaned_body').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\nhashudf = F.udf(lambda vector : vector.indices.tolist()[0],F.StringType())\nwordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(F.col('rawFeatures')))\nwordtf = w...",
                    "displayName": "toPandas at /tmp/ipykernel_25516/3184604692.py:37",
                    "jobGroup": "42",
                    "jobId": 19,
                    "killedTasksSummary": {},
                    "name": "toPandas at /tmp/ipykernel_25516/3184604692.py:37",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 32,
                    "numCompletedStages": 1,
                    "numCompletedTasks": 32,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 0,
                    "numSkippedTasks": 0,
                    "numTasks": 32,
                    "rowCount": 3462914,
                    "stageIds": [
                      30
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T18:31:44.210GMT",
                    "usageDescription": ""
                  },
                  {
                    "completionTime": "2023-11-18T18:31:42.245GMT",
                    "dataRead": 23812169,
                    "dataWritten": 23812169,
                    "description": "Job group for statement 42:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_comms_body_words = comments_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                                .select(F.col(\"cleaned_body\"))\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"cleaned_body\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\n# Define IDF\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\nwords = pol_comms_body_words.select('cleaned_body')\n\n# Hashing frequency\ntf = hashingTF.transform(words)\n\n# IDF\nidf_model = idf.fit(tf)\n\n# TFIDF\ntfidf = idf_model.transform(tf)\n\nndf = tfidf.select(F.explode('cleaned_body').name('expwords')).withColumn('words',F.array('expwords'))\n\n# Define HashingTF\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n\nhashudf = F.udf(lambda vector : vector.indices.tolist()[0],F.StringType())\nwordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(F.col('rawFeatures')))\nwordtf = w...",
                    "displayName": "treeAggregate at IDF.scala:55",
                    "jobGroup": "42",
                    "jobId": 18,
                    "killedTasksSummary": {},
                    "name": "treeAggregate at IDF.scala:55",
                    "numActiveStages": 0,
                    "numActiveTasks": 0,
                    "numCompletedIndices": 37,
                    "numCompletedStages": 2,
                    "numCompletedTasks": 37,
                    "numFailedStages": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numSkippedStages": 0,
                    "numSkippedTasks": 0,
                    "numTasks": 37,
                    "rowCount": 64,
                    "stageIds": [
                      28,
                      29
                    ],
                    "status": "SUCCEEDED",
                    "submissionTime": "2023-11-18T18:04:51.991GMT",
                    "usageDescription": ""
                  }
                ],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 5,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "7bcc356e-2304-45b0-a972-3b83d052d5ce",
              "state": "finished",
              "statement_id": 42
            },
            "text/plain": "StatementMeta(7bcc356e-2304-45b0-a972-3b83d052d5ce, 5, 42, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 40,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700337878249
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "18bfabc2-99d9-49f1-b677-5b2d149f840e"
    },
    {
      "cell_type": "code",
      "source": [
        "top100_pol_comms.to_csv(\"Users/tg693/fall-2023-reddit-project-team-01/data/csv/tfidf_pol_comms.csv\",index=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-11-18T20:04:38.9976192Z",
              "execution_start_time": "2023-11-18T20:04:38.171028Z",
              "livy_statement_state": "available",
              "parent_msg_id": "56ec6888-424f-49ba-9272-a4946d43069c",
              "queued_time": "2023-11-18T18:04:54.7682529Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": {
                "jobs": [],
                "limit": 20,
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "rule": "ALL_DESC"
              },
              "spark_pool": "7bcc356e-2304-45b0-a972-3b83d052d5ce",
              "state": "finished",
              "statement_id": 43
            },
            "text/plain": "StatementMeta(7bcc356e-2304-45b0-a972-3b83d052d5ce, 5, 43, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1700337879128
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "33beee8f-f945-41b3-beb5-894138c1297c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine the most important words in economics subreddits comments body using TF-IDF"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "784fb967-27a4-4a21-a5fb-4af625720a5a"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# Azure Storage Account details\n",
        "connection_string = \"DefaultEndpointsProtocol=https;AccountName=group01astorage63e260f45;AccountKey=iGcY4Un0hlKMMqSs6BlLhmqNU0D7m8uJyVz2din6CTAp3AvM3QPH8/Tk8k+xN77D5R3KXvJZYBwX+AStLsNR5Q==;EndpointSuffix=core.windows.net\"\n",
        "container_name = \"azureml-blobstore-a1e50e78-9796-4cfe-a8bb-88f7de188a74\"\n",
        "\n",
        "# Initialize the BlobServiceClient\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "\n",
        "# create the filesystem\n",
        "fs = AzureMachineLearningFileSystem(\"azureml://subscriptions/71060237-912c-4e5c-849b-1a23626fc284/resourcegroups/project-rg/workspaces/group-01-aml/datastores/workspaceblobstore/paths/\")\n",
        "\n",
        "# Get the list of files\n",
        "files = fs.glob('nlp_cleaned_teg/comments/*.parquet')\n",
        "\n",
        "# Initialize an empty list to hold DataFrames\n",
        "dfs = []\n",
        "\n",
        "for file in files:  # Skip the first file\n",
        "    # Get the blob client for the file\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file)\n",
        "\n",
        "    # Download the blob to a stream\n",
        "    with io.BytesIO() as input_blob:\n",
        "        blob_client.download_blob().readinto(input_blob)\n",
        "        input_blob.seek(0)  # Go to the start of the stream\n",
        "\n",
        "        # Read the parquet file\n",
        "        df = pd.read_parquet(input_blob, engine='pyarrow')\n",
        "        dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list\n",
        "comments_cleaned = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# specify schema\n",
        "schema = StructType([\n",
        "    StructField(\"author\", StringType(), True),\n",
        "    StructField(\"body\", StringType(),True),\n",
        "    StructField(\"can_gild\", StringType(),True),\n",
        "    StructField(\"controversiality\", IntegerType(),True),\n",
        "    StructField(\"created_utc\", DateType(),True),\n",
        "    StructField(\"distinguished\", StringType(),True),\n",
        "    StructField(\"edited\", StringType(),True),\n",
        "    StructField(\"gilded\", IntegerType(),True),\n",
        "    StructField(\"id\", StringType(),True),\n",
        "    StructField(\"is_submitter\", BooleanType(),True),\n",
        "    StructField(\"link_id\", StringType(),True),\n",
        "    StructField(\"parent_id\", StringType(),True),\n",
        "    StructField(\"permalink\", StringType(),True),\n",
        "    StructField(\"retrieved_on\", DateType(),True),\n",
        "    StructField(\"score\", IntegerType(),True),\n",
        "    StructField(\"stickied\", BooleanType(),True),\n",
        "    StructField(\"subreddit\", StringType(),True),\n",
        "    StructField(\"subreddit_id\", StringType(),True)])\n",
        "\n",
        "# convert submissions pandas df to spark df\n",
        "comments_cleaned=spark.createDataFrame(comments_cleaned, schema=schema)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "waiting",
              "livy_statement_state": null,
              "queued_time": "2023-11-21T01:38:48.8243638Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null,
              "spark_jobs": null,
              "parent_msg_id": "6d7c9f46-3172-416d-96ff-d4aa078a1972"
            },
            "text/plain": "StatementMeta(, , , Waiting, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700521777690
        }
      },
      "id": "c3c5cdaf-c774-4b9e-b1f1-a3863b0c0380"
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_subreddits = [\"Economics\", \"finance\"]\n",
        "\n",
        "# Concat body, title and filter for political subreddits\n",
        "econ_comms_body_words = comments_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))\\\n",
        "                                        .select(F.col(\"cleaned_body\"))\n",
        "print(\"econ_comms_body_words created\")\n",
        "\n",
        "# Define HashingTF\n",
        "hashingTF = HashingTF(inputCol=\"cleaned_body\", outputCol=\"rawFeatures\",numFeatures=40000000)\n",
        "\n",
        "# Define IDF\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# Hashing frequency\n",
        "tf = hashingTF.transform(econ_comms_body_words)\n",
        "print(\"hashingTF done\")\n",
        "\n",
        "# IDF\n",
        "idf_model = idf.fit(tf)\n",
        "\n",
        "# TFIDF\n",
        "tfidf = idf_model.transform(tf)\n",
        "print(\"idf_model done\")\n",
        "\n",
        "ndf = tfidf.select(F.explode('cleaned_body').name('expwords')).withColumn('words',F.array('expwords'))\n",
        "\n",
        "# Define HashingTF\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=40000000)\n",
        "\n",
        "hashudf = F.udf(lambda vector : vector.indices.tolist()[0],F.StringType())\n",
        "wordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(F.col('rawFeatures')))\n",
        "wordtf = wordtf.dropDuplicates([\"expwords\"])\n",
        "print(\"Duplicates dropped\")\n",
        "\n",
        "udf1 = F.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))\n",
        "valuedf = tfidf.select(F.explode(udf1(F.col('features'))).name('wordhash','value'))\n",
        "valuedf = valuedf.dropDuplicates([\"wordhash\"])\n",
        "join_importance = wordtf.join(valuedf, wordtf.wordhash == valuedf.wordhash, 'inner').select(wordtf.expwords, valuedf.value)\n",
        "top100_econ_comms = join_importance.sort(F.col(\"value\").desc()).limit(100).toPandas()\n",
        "print(\"top100_econ_comms created\")\n",
        "\n",
        "top100_econ_comms = top100_econ_comms.reset_index(drop=True)\n",
        "top100_econ_comms = top100_econ_comms.rename(columns={\"expwords\":\"word\", \"value\":\"tfidf_value\"})"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "waiting",
              "livy_statement_state": null,
              "queued_time": "2023-11-21T01:38:49.115367Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null,
              "spark_jobs": null,
              "parent_msg_id": "4f2acf2d-7f88-48b7-86dd-a1ff614ef5c9"
            },
            "text/plain": "StatementMeta(, , , Waiting, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700521777772
        }
      },
      "id": "d256ba59-273a-4700-9e67-20c1a81c275f"
    },
    {
      "cell_type": "code",
      "source": [
        "top100_econ_comms.to_csv(\"Users/tg693/fall-2023-reddit-project-team-01/data/csv/tfidf_econ_comms.csv\",index=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-11-20T22:17:03.5473651Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-11-20T22:31:12.2583915Z",
              "spark_jobs": null,
              "parent_msg_id": "cf591a08-71cc-4b6d-9467-06752bf88477"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700519472434
        }
      },
      "id": "4debcb6c-aad5-481a-9390-1bbb3294a7eb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA Topic Modeling for Politics and Economics Subreddits Submissions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "da12c582-c70f-431a-b91e-29d8be6d0abb"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.clustering import LDA\n",
        "from pyspark.ml.feature import Tokenizer"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T22:48:03.1420464Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T22:48:03.2704723Z",
              "execution_finish_time": "2023-11-21T22:48:03.5904401Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "d3305805-2346-4ba2-96f7-fe9a5d6c45f6"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 17, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700606883690
        }
      },
      "id": "668ad7eb-7bd3-4b81-9265-fb8f9a9aa655"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Politics Subreddits"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "9d44d16d-8e75-4e06-a185-ba19966dc1a9"
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_subreddits = [\"Economics\", \"finance\"]\n",
        "\n",
        "# Concat body, title and filter for political subreddits\n",
        "pol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits)) \\\n",
        "                                    .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\\\n",
        "\n",
        "substr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\n",
        "regex = \"|\".join(substr_to_remove)\n",
        "pol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n",
        "\n",
        "# Split the nlpbodytext into an array of words\n",
        "pol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n",
        "\n",
        "# Create the CountVectorizer model\n",
        "cv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
        "cv_model = cv.fit(pol_subs_body_title_words)\n",
        "vectorized = cv_model.transform(pol_subs_body_title_words)\n",
        "\n",
        "# Set the number of topics and max iterations\n",
        "num_topics = 5\n",
        "max_iterations = 50\n",
        "\n",
        "# Create and fit the LDA model\n",
        "lda = LDA(k=num_topics, maxIter=max_iterations, featuresCol=\"features\")\n",
        "lda_model = lda.fit(vectorized)\n",
        "\n",
        "# Get the topics and their top words\n",
        "topics = lda_model.describeTopics(maxTermsPerTopic=10)\n",
        "vocab = cv_model.vocabulary\n",
        "\n",
        "# Define a function to map topic words indices to actual words\n",
        "def indices_to_words(indices):\n",
        "    return [vocab[int(index)] for index in indices]\n",
        "\n",
        "# Map topic words indices to actual words\n",
        "indices_to_words_udf = F.udf(indices_to_words, ArrayType(StringType()))\n",
        "pol_topics = topics.withColumn(\"topic_words\", indices_to_words_udf(topics[\"termIndices\"]))\n",
        "\n",
        "# Show the topics and their top words\n",
        "pol_topics.select(\"topic\", \"topic_words\").show(truncate=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 23,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T23:10:17.3300179Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T23:10:17.4619271Z",
              "execution_finish_time": "2023-11-21T23:17:58.3117741Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 105,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 425,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:56.265GMT",
                    "completionTime": "2023-11-21T23:17:56.760GMT",
                    "stageIds": [
                      635
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 5,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 5,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 5,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 424,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:54.091GMT",
                    "completionTime": "2023-11-21T23:17:55.979GMT",
                    "stageIds": [
                      633,
                      634
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 423,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:53.866GMT",
                    "completionTime": "2023-11-21T23:17:54.042GMT",
                    "stageIds": [
                      632
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 422,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:51.898GMT",
                    "completionTime": "2023-11-21T23:17:53.841GMT",
                    "stageIds": [
                      630,
                      631
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 421,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:51.679GMT",
                    "completionTime": "2023-11-21T23:17:51.849GMT",
                    "stageIds": [
                      629
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 420,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:49.639GMT",
                    "completionTime": "2023-11-21T23:17:51.652GMT",
                    "stageIds": [
                      628,
                      627
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 419,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:49.384GMT",
                    "completionTime": "2023-11-21T23:17:49.560GMT",
                    "stageIds": [
                      626
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 418,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:47.399GMT",
                    "completionTime": "2023-11-21T23:17:49.358GMT",
                    "stageIds": [
                      625,
                      624
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 417,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:47.169GMT",
                    "completionTime": "2023-11-21T23:17:47.343GMT",
                    "stageIds": [
                      623
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 416,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:45.240GMT",
                    "completionTime": "2023-11-21T23:17:47.143GMT",
                    "stageIds": [
                      622,
                      621
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 415,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:45.012GMT",
                    "completionTime": "2023-11-21T23:17:45.182GMT",
                    "stageIds": [
                      620
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 414,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:43.053GMT",
                    "completionTime": "2023-11-21T23:17:44.982GMT",
                    "stageIds": [
                      619,
                      618
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 413,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:42.827GMT",
                    "completionTime": "2023-11-21T23:17:42.994GMT",
                    "stageIds": [
                      617
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 412,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:40.866GMT",
                    "completionTime": "2023-11-21T23:17:42.796GMT",
                    "stageIds": [
                      615,
                      616
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 411,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:40.640GMT",
                    "completionTime": "2023-11-21T23:17:40.810GMT",
                    "stageIds": [
                      614
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 410,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:38.708GMT",
                    "completionTime": "2023-11-21T23:17:40.615GMT",
                    "stageIds": [
                      612,
                      613
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 409,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:38.476GMT",
                    "completionTime": "2023-11-21T23:17:38.646GMT",
                    "stageIds": [
                      611
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 408,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:36.561GMT",
                    "completionTime": "2023-11-21T23:17:38.448GMT",
                    "stageIds": [
                      610,
                      609
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 407,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:36.317GMT",
                    "completionTime": "2023-11-21T23:17:36.501GMT",
                    "stageIds": [
                      608
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 406,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 23:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\npol_subs_body_title_words = submissions_cleaned.filter(~F.col(\"subreddit\").isin(excluded_subreddits))                                     .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body).alias(\"nlpbodytext\"))\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\npol_subs_body_title_words = pol_subs_body_title_words.withColumn(\"array_words\", F.split(pol_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(pol_subs_body_title_words)\nvectorized = cv_model.transform(pol_subs_bod...",
                    "submissionTime": "2023-11-21T23:17:34.326GMT",
                    "completionTime": "2023-11-21T23:17:36.282GMT",
                    "stageIds": [
                      607,
                      606
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "40292223-fdfa-4bac-8dc2-960214394b29"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 23, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+-----+-----------------------------------------------------------------------------+\n|topic|topic_words                                                                  |\n+-----+-----------------------------------------------------------------------------+\n|0    |[widen, trump, democrat, republican, vote, say, election, bill, house, state]|\n|1    |[, cmv, user, conservative, people, trump, woman, ban, e, s]                 |\n|2    |[, covid, school, vaccine, news, police, new, get, say, shoot]               |\n|3    |[people, , like, think, make, get, one, say, e, see]                         |\n|4    |[widen, we, ukraine, war, china, say, russia, border, year, desantis]        |\n+-----+-----------------------------------------------------------------------------+\n\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700608680717
        }
      },
      "id": "4a88b7df-ef23-4eef-baaf-ce6a7695e8f0"
    },
    {
      "cell_type": "code",
      "source": [
        "pol_topics = pol_topics.toPandas()\n",
        "pol_topics.to_csv(\"Users/tg693/fall-2023-reddit-project-team-01/data/csv/lda_pol_subs.csv\",index=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 24,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T23:10:17.9613892Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T23:17:58.4333019Z",
              "execution_finish_time": "2023-11-21T23:18:02.0547846Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 1,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_11235/1445182049.py:1",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 426,
                    "name": "toPandas at /tmp/ipykernel_11235/1445182049.py:1",
                    "description": "Job group for statement 24:\npol_topics = pol_topics.toPandas()\npol_topics.to_csv(\"Users/tg693/fall-2023-reddit-project-team-01/data/csv/lda_pol_subs.csv\",index=False)",
                    "submissionTime": "2023-11-21T23:17:58.756GMT",
                    "completionTime": "2023-11-21T23:17:59.581GMT",
                    "stageIds": [
                      636
                    ],
                    "jobGroup": "24",
                    "status": "SUCCEEDED",
                    "numTasks": 5,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 5,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 5,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "ece768cd-642b-493e-85c1-911fa092881c"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 24, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700608684387
        }
      },
      "id": "c81efede-299b-4dbf-bede-822b05446626"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Economics Subreddits"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "94660a88-7293-48ab-8d37-e55cc9bb8228"
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_subreddits = [\"Economics\", \"finance\"]\n",
        "\n",
        "# Concat body, title and filter for political subreddits\n",
        "econ_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))\\\n",
        "                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)\\\n",
        "                                               .alias(\"nlpbodytext\"))\n",
        "\n",
        "substr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\n",
        "regex = \"|\".join(substr_to_remove)\n",
        "econ_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n",
        "\n",
        "# Split the nlpbodytext into an array of words\n",
        "econ_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n",
        "\n",
        "# Create the CountVectorizer model\n",
        "cv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
        "cv_model = cv.fit(econ_subs_body_title_words)\n",
        "vectorized = cv_model.transform(econ_subs_body_title_words)\n",
        "\n",
        "# Set the number of topics and max iterations\n",
        "num_topics = 5\n",
        "max_iterations = 50\n",
        "\n",
        "# Create and fit the LDA model\n",
        "lda = LDA(k=num_topics, maxIter=max_iterations, featuresCol=\"features\")\n",
        "lda_model = lda.fit(vectorized)\n",
        "\n",
        "# Get the topics and their top words\n",
        "topics = lda_model.describeTopics(maxTermsPerTopic=10)\n",
        "vocab = cv_model.vocabulary\n",
        "\n",
        "# Define a function to map topic words indices to actual words\n",
        "def indices_to_words(indices):\n",
        "    return [vocab[int(index)] for index in indices]\n",
        "\n",
        "# Map topic words indices to actual words\n",
        "indices_to_words_udf = F.udf(indices_to_words, ArrayType(StringType()))\n",
        "econ_topics = topics.withColumn(\"topic_words\", indices_to_words_udf(topics[\"termIndices\"]))\n",
        "\n",
        "# Show the topics and their top words\n",
        "econ_topics.select(\"topic\", \"topic_words\").show(truncate=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 25,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T23:10:19.1588428Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T23:18:02.1577358Z",
              "execution_finish_time": "2023-11-21T23:20:23.407769Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 105,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 531,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:20.902GMT",
                    "completionTime": "2023-11-21T23:20:21.439GMT",
                    "stageIds": [
                      793
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 5,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 5,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 5,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 530,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:18.858GMT",
                    "completionTime": "2023-11-21T23:20:20.544GMT",
                    "stageIds": [
                      791,
                      792
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 529,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:18.613GMT",
                    "completionTime": "2023-11-21T23:20:18.792GMT",
                    "stageIds": [
                      790
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 528,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:16.848GMT",
                    "completionTime": "2023-11-21T23:20:18.587GMT",
                    "stageIds": [
                      789,
                      788
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 527,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:16.570GMT",
                    "completionTime": "2023-11-21T23:20:16.747GMT",
                    "stageIds": [
                      787
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 526,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:14.823GMT",
                    "completionTime": "2023-11-21T23:20:16.532GMT",
                    "stageIds": [
                      785,
                      786
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 525,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:14.604GMT",
                    "completionTime": "2023-11-21T23:20:14.762GMT",
                    "stageIds": [
                      784
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 524,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:12.896GMT",
                    "completionTime": "2023-11-21T23:20:14.576GMT",
                    "stageIds": [
                      782,
                      783
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 523,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:12.672GMT",
                    "completionTime": "2023-11-21T23:20:12.836GMT",
                    "stageIds": [
                      781
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 522,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:10.955GMT",
                    "completionTime": "2023-11-21T23:20:12.630GMT",
                    "stageIds": [
                      779,
                      780
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 521,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:10.743GMT",
                    "completionTime": "2023-11-21T23:20:10.895GMT",
                    "stageIds": [
                      778
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 520,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:08.984GMT",
                    "completionTime": "2023-11-21T23:20:10.717GMT",
                    "stageIds": [
                      776,
                      777
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 519,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:08.759GMT",
                    "completionTime": "2023-11-21T23:20:08.922GMT",
                    "stageIds": [
                      775
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 518,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:07.075GMT",
                    "completionTime": "2023-11-21T23:20:08.730GMT",
                    "stageIds": [
                      773,
                      774
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 517,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:06.868GMT",
                    "completionTime": "2023-11-21T23:20:07.025GMT",
                    "stageIds": [
                      772
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 516,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:05.173GMT",
                    "completionTime": "2023-11-21T23:20:06.842GMT",
                    "stageIds": [
                      771,
                      770
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 515,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:04.930GMT",
                    "completionTime": "2023-11-21T23:20:05.091GMT",
                    "stageIds": [
                      769
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 514,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:03.236GMT",
                    "completionTime": "2023-11-21T23:20:04.891GMT",
                    "stageIds": [
                      767,
                      768
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "isEmpty at LDAOptimizer.scala:448",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 513,
                    "name": "isEmpty at LDAOptimizer.scala:448",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:03.023GMT",
                    "completionTime": "2023-11-21T23:20:03.182GMT",
                    "stageIds": [
                      766
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at LDAOptimizer.scala:510",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 512,
                    "name": "treeAggregate at LDAOptimizer.scala:510",
                    "description": "Job group for statement 25:\nexcluded_subreddits = [\"Economics\", \"finance\"]\n\n# Concat body, title and filter for political subreddits\necon_subs_body_title_words = submissions_cleaned.filter(F.col(\"subreddit\").isin(excluded_subreddits))                                               .select(F.concat_ws(\" \", submissions_cleaned.cleaned_title,submissions_cleaned.cleaned_body)                                               .alias(\"nlpbodytext\"))\n\nsubstr_to_remove = [\"http\", \"https\", \"tinyurl\", \"com\", \"www\", \"jpg\", \"uploads\", \"delete\", \"remove\"]\nregex = \"|\".join(substr_to_remove)\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"nlpbodytext\", F.regexp_replace(\"nlpbodytext\", regex, \" \"))\n\n# Split the nlpbodytext into an array of words\necon_subs_body_title_words = econ_subs_body_title_words.withColumn(\"array_words\", F.split(econ_subs_body_title_words[\"nlpbodytext\"], \" \"))\n\n# Create the CountVectorizer model\ncv = CountVectorizer(inputCol=\"array_words\", outputCol=\"features\", vocabSize=10000, minDF=5)\ncv_model = cv.fit(econ_sub...",
                    "submissionTime": "2023-11-21T23:20:01.289GMT",
                    "completionTime": "2023-11-21T23:20:02.989GMT",
                    "stageIds": [
                      764,
                      765
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 20,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "9a9b0ff7-7a00-4911-b3a1-fc860c9885bd"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 25, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+-----+----------------------------------------------------------------------------+\n|topic|topic_words                                                                 |\n+-----+----------------------------------------------------------------------------+\n|0    |[we, inflation, rate, price, year, economy, market, high, bank, say]        |\n|1    |[stock, money, make, news, get, take, say, change, life, good]              |\n|2    |[know, view, one, question, team, finance, editorial, r, loan, un]          |\n|3    |[, s, fintechinshorts, content, fintechnews, wp, bank, user, launch, new]   |\n|4    |[currency, review, strength, e, weakness, th, war, russian, ukraine, dollar]|\n+-----+----------------------------------------------------------------------------+\n\n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700608823740
        }
      },
      "id": "3a0dbf86-c2f7-43d9-b5c9-6bfa25e53b77"
    },
    {
      "cell_type": "code",
      "source": [
        "econ_topics = econ_topics.toPandas()\n",
        "econ_topics.to_csv(\"Users/tg693/fall-2023-reddit-project-team-01/data/csv/lda_econ_subs.csv\",index=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "17b2472e-89f2-4244-b483-22f4ee0cf696",
              "session_id": "38",
              "statement_id": 26,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-21T23:10:19.6201376Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-21T23:20:23.5207737Z",
              "execution_finish_time": "2023-11-21T23:20:25.5263849Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 1,
                  "UNKNOWN": 0,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_11235/1326558090.py:1",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 532,
                    "name": "toPandas at /tmp/ipykernel_11235/1326558090.py:1",
                    "description": "Job group for statement 26:\necon_topics = econ_topics.toPandas()\necon_topics.to_csv(\"Users/tg693/fall-2023-reddit-project-team-01/data/csv/lda_econ_subs.csv\",index=False)",
                    "submissionTime": "2023-11-21T23:20:23.555GMT",
                    "completionTime": "2023-11-21T23:20:24.017GMT",
                    "stageIds": [
                      794
                    ],
                    "jobGroup": "26",
                    "status": "SUCCEEDED",
                    "numTasks": 5,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 5,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 5,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "332644c0-cfc8-47a4-a90d-031ef3e59a8d"
            },
            "text/plain": "StatementMeta(17b2472e-89f2-4244-b483-22f4ee0cf696, 38, 26, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700608825584
        }
      },
      "id": "13e16a4f-6806-4191-aabe-06c995625d03"
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "name": "ml.geospatial.interactive",
        "_defaultOrder": 20,
        "hideHardwareSpecs": true,
        "vcpuNum": 0,
        "_isFastLaunch": false,
        "gpuNum": 0,
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "category": "General purpose",
        "memoryGiB": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "instance_type": "ml.c5.xlarge",
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}