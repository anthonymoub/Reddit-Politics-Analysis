[
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "EDA",
    "section": "",
    "text": "Our exploratory data analysis (EDA) on Reddit’s major political and economic subreddits has provided a nuanced view of online political discourse, offering valuable context for understanding user behavior. The analysis delved into various aspects of these online subreddit communities, shedding light on the dynamics of discussions and the interplay of user interests.\nPreliminary analysis unveiled that r/Conservative is the most active subreddit, with the highest number of posts and comments, while r/AskPolitics has the most distinct posts and comments, indicating a more diverse user base. The r/Conservative subreddit also has the highest average post score, suggesting that its content resonates strongly with its user base. The analysis also revealed that the r/Conservative subreddit has the most significant number of users in common with other political subreddits, indicating that these users engage in various political communities. Subreddits, such as r/socialism and r/Liberal, showcase consistent activity with fewer fluctuations across number of comments and submissions, suggesting a more stable user base.\nAdditionally, weekly patterns of posting activity revealed that Thursdays are the most active days for posting across r/Liberterian, r/socialism, and r/centrist subreddits, with the exception of r/Conservative, which sees the highest activity on Wednesdays. The analysis also showed that posting activity generally decreases as the year progresses, with the lowest numbers appearing in December. The data also revealed that Saturdays and Sundays consistently have fewer posts than weekdays, in line with typical online engagement trends.\nFinally, we incorporated U.S GDP data from the FRED python package from January 2021 to March 2023 to compare the economic data with the r/Economics subreddit posting activity. The analysis revealed that there is a relationship between the state of the economy and the level of engagement and sentiment on the r/Economics subreddit. During times of economic uncertainty, there is an increase in the number of submissions and a decrease in the average submission score. This suggests that people are more likely to turn to online communities to discuss and seek information about the economy during difficult times. Additionally, the types of posts that are upvoted during these times suggest that people are looking for content that resonates with their concerns and anxieties.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Line Chart of number of posts (per subreddit)\n\n\n\nFigure 1 presents a line chart that tracks the posting activity across various political and economic subreddits from January 2021 to March 2023. The subreddit r/Conservative shows the most pronounced variability and has the highest peaks, suggesting periods of intense activity which might be attributed to specific political events or discussions that resonated strongly with its user base. It’s noteworthy that the peaks for r/Conservative sharply spike above all other subreddits, indicating that certain topics or times drove engagement significantly more than usual.\nIn contrast, other subreddits such as r/Economics, r/Finance, and r/ChangeMyView maintain a relatively stable, low-level activity over time, with Economics occasionally showing slight increases that could correspond to economic events or policy discussions. Subreddits like r/Liberal, r/socialism, r/Libertarian, and r/centrist exhibit moderate activity with fewer fluctuations, reflecting a consistent level of engagement without the sharp spikes observed in the r/Conservative subreddit. The data suggests that while political subreddits can experience bursts of heightened activity, forums centered around economics and finance tend to have more steady, predictable participation rates.\n\n\n\n\n\nFigure 2: Line Chart of number of comments (per subreddit)\n\n\n\nFigure 2 tracks the volume of comments across various subreddits, highlighting how user interaction ebbs and flows over time. The “Conservative” subreddit shows the most pronounced fluctuations, with sharp spikes indicative of intense conversational bursts. The r/Libertarian subreddit also experiences notable, albeit less extreme, surges in activity, potentially aligning with key events or discussions. Other subreddits, including r/Liberal, r/socialism, and r/centrist present more consistent comment patterns, with occasional upticks that may correspond to specific events. The steady increase in the r/Economics subreddit comments towards the end of the timeline suggests growing engagement. This visualization reflects the dynamic nature of online discourse, with political forums often at the mercy of the news cycle, while more thematic forums like r/Finance or r/Economics maintain a steadier level of dialogue.\nNote: Similar plots for distinct posts, comment counts, and distinct comments can be found in the appendix.\n\n\n\nThe subreddits were ranked based on the counts of meaures that correspond with controversial posts and comments. As Figures 1 and 2 have shown already, there is greater number of posts and comments from the r/Conservative subreddit, so the counts shown in Table 1 have been normalized. Unsurprisingly, the r/ChangeMyView subreddit had the greatest number of distinguished posts and comments (being nearly 2 standard deviations from the mean), which may hint at heavy moderation, potentially as a result of touchy subjects being discussed. Ironically, the subreddits typically associated with the authoritarian left and right on the political compass (r/socialism and r/Conservative, respectively) have less distinguished submissions and posts that the libertarian left and right subreddits (r/Liberal and r/Libertarian, respectively), pointing to an ironic inversion from the expectations of subreddit moderation. Additionally, the r/Conservative subreddit had the greatest number of gilded posts and comments. This, at a glance, may seem like users incentivizing each other financially with the prospects of Reddit Gold, which would inevitably promote an echo chamber of ideas in an online political space; however, this doesn’t seem to be the case when considering that the subreddit also has the highest count of controversial comments (having a count of over 2 standard deviations from the mean). Instead, this may suggest that users on this subreddit are simply more willing to monetarily support posts and comments that they find appealing.\n\n\n\n\n\n\nTable 1: Subreddits with the Most Number of Controversial Posts and Comments (Normalized)\n\n\n\nNext, we looked at individual users and ranked them based on the counts of measures that corerspond with controversial posts and comments. Based on the distinguished submissions and comments, we can see that among the top 20 controversial users, only one of them (user: “ultimis”) is a moderator. Another finding is that the top 20 users all come from the subreddits r/centrist, r/Libertarian, or r/Conservative. Much like the findings from Table 1, Table 2 also seems to suggest that these three political subreddits have a large amount of intra-subreddit discourse among their posts and comments.\n\n\n\n\n\n\nTable 2: Top 20 Users with the Most Number of Controversial Posts and Comments\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Bubble Chart showing the 50 highest scored posts per subreddit.\n\n\n\nFigure 3 is a bubble chart illustrating the average scores of the top 50 posts from various political subreddits, where the score is calculated by subtracting the number of downvotes from upvotes. The size of each subreddit’s bubble reflects the average score of its posts. The chart shows that the subreddit represented by the largest bubble has the highest average post score, while the smallest bubble corresponds to the subreddit with the lowest average score.\nThe subreddit with the largest bubble (r/Conservative) has significantly outpaced the others in terms of average score, suggesting that posts in this community resonate strongly with its members and receive more upvotes. Conversely, the subreddit with the smallest bubble (r/centrist) has the lowest average scores, indicating either a smaller community, less engagement, or a tendency towards more divisive content that doesn’t amass as many upvotes.\nWithin each of these larger subreddit bubbles are smaller bubbles, each representing an individual post; the title of the post is indicated within these smaller bubbles. Additionally, interacting with these smaller bubbles by clicking on them will redirect the viewer to the respective post’s link on Reddit for further exploration.\n\n\n\n\n\n\n\n\nFigure 4: Calendar Heatmap of r/Conservative Reddit Posts\n\n\n\nFigure 4 depicts the daily and monthly number of posts on the r/Conservative subreddit. There’s a clear trend of higher activity on weekdays, with Wednesdays being the peak, and lower activity on weekends, particularly on Sundays. The data indicates a seasonal trend, with post volumes generally higher in March and April and reducing towards the end of the year, in November and December. The color gradient emphasizes these variations, with darker hues representing more posts, allowing for a visual representation of user engagement on Reddit throughout the different months and days of the week.\nNote: A calendar heatmap for different subreddits can be found in the appendix section.\n\n\n\n\n\n\n\n\nFigure 5: Dependency Wheel of Users in different Subreddits\n\n\n\nThe dependency wheel depicted in Figure 5 above provides a graphical representation of the shared user base between various political subreddits. The thickness of the connecting bands is directly proportional to the number of users that participate in both subreddits at each end of the band. A notable observation from the wheel is the significant interconnectivity between certain subreddits, which could suggest a shared ideological proximity or an interest overlap among their user bases. For instance, if there is a thick band between the subreddits labeled r/Liberal and r/socialism, this would indicate a large common audience, possibly due to similar political leanings or discussions that appeal to both groups.\nAdditionally, the visualization reveals subreddits that serve as common grounds for diverse political discourse, such as r/ChangeMyView or r/AskPolitics, where the number of interlinking bands suggests a wide range of users from different political backgrounds converging for debate or inquiry. This could imply that these platforms are more neutral or open-ended, attracting a varied audience seeking to engage with multiple perspectives. The overall pattern highlights not only the segmentation within the political discourse on Reddit but also the points of intersection where cross-ideological conversations are occurring.\n\n\n\nWe present the Cosine and Jaccard similarity scores for posts’ bodies and titles across grouped political and economics subreddits. The political subreddits utilized for the two tables below are r/Conservative, r/Socialism, r/centrist, and r/Libertarian and the economic subreddits are r/Finance and r/Economics. Initially, we obtained the counts of three words, [recession, inflation, and unemployment], for the grouped subreddits from regex patterns on the posts’ bodies and ran Cosine as well as Jaccard Similarity scores on the counts. The Cosine Similarity score between the grouped subreddits was 0.9 and Jaccard Similarity was 0.04, which meant that the two subreddits have similar distributions of words, even though the counts of individual words were significantly different. This finding culminated in two hypotheses, the first being that the grouped economic subreddits must be containing a large number of “shitposts”, probably related to cryptocurrencies and NFT’s, and the second one that economics subreddits posts’ bodies contain mainly links to online news articles with the key words present in the post titles. Consequently, we updated the word list to account for [crypto, blockchain, nft] words contained in these flippant posts and also ran the similarity algorithms on the posts’ titles.\nIn addition to the economics word list, we also made a political word list to obtain grouped subreddit counts for the words [trump, biden, election, fed, powell] and ran the similarity algorithms on the posts’ bodies and titles. Its results and discussion are presented below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3: Cosine and Jaccard Similarity of Post Body for combined Political and Economics subreddits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4: Cosine and Jaccard Similarity of Post Title for combined Political and Economics subreddits\n\n\n\nTable 4 supports our hyptheses, as we can see that the Jaccard Similarity score is highest across the post titles of the grouped subreddits when we filter for the economics word list. This means that the posts’ titles not only have a similar distribution of counts across the grouped subreddits but they also have similar raw counts for each individual word. This is because the posts’ titles contain the key words that we are looking for, while the posts’ bodies contain links to online news articles. On the other, political words in either the posts’ bodies or titles do not have a similar distribution of counts across the grouped subreddits, as shown in both Table 3 and Table 4.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Number of r/Economics Submissions vs U.S Real GDP\n\n\n\nFigure 6 presents a comparative analysis of the United States’ Real Gross Domestic Product (GDP) against the number of submissions in the r/Economics subreddit over a period extending from January 2021 to March 2023. Evident in the plot, the submission count hovers around 1,000 posts per month, reflecting a steady engagement level within the subreddit community.\nA notable deviation occurs in July 2022, marked by an annotation that points out the release of news indicating the U.S economy’s contraction for two consecutive quarters, often considered a technical indicator of a recession. Following this announcement, there is a marked spike in subreddit activity in August and September 2022, with submissions surging to well over four times the usual number. This spike in activity suggests a heightened collective concern and a rush to discuss and understand the implications of the economic news\n\n\n\n\n\n\n\n\nFigure 7: Average Score of r/Economics Submissions vs U.S Real GDP\n\n\n\nFigure 7 again plots the United States’ Real Gross Domestic Product (GDP), but this time against the average submission score in the r/Economics subreddit over a period extending from January 2021 to March 2023. Conversely, this visualization reveals a stark drop in the average score of submissions during the same period of economic concern. This significant downturn in average score might also be interpreted as a barometer of public sentiment towards the economy, rather than simply a measure of post quality. In times of economic uncertainty, it’s common for sentiment to sour, and this shift is often reflected in the collective mood of discussions and the types of content that get upvoted. As the real GDP contracted for two consecutive quarters, the mood likely shifted from cautiously optimistic to more pessimistic and critical, leading to a preference for upvoting posts that resonate with the community’s concerns and anxieties. However, it is also worth noting that since the number of posts in that period also significantly increased, it is likely that lower post activity (comments/upvotes) could also have impacted the average score during that period.\n\n\n\n\n\n\n\n\n\n\nAppendix 1: Line Chart of number of posts by distinct number of users (per subreddit)\n\n\n\n\n\n\n\n\n\nAppendix 2: Line Chart of number of comments by distinct number of users (per subreddit)\n\n\n\n\n\n\n\n\n\n\nAppendix 3: Calendar Heatmap of r/Liberterian Reddit Posts\n\n\n\n\n\n\n\n\n\nAppendix 4: Calendar Heatmap of r/centrist Reddit Posts\n\n\n\n\n\n\n\n\n\nAppendix 5: Calendar Heatmap of r/socialism Reddit Posts"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "ML",
    "section": "",
    "text": "The ML (Machine Learning) section delves into applying various machine learning models to predict certain aspects of Reddit posts. First, for submissions, we wanted to see if there were certain characteristics that indicated the score that a post would get. In order to do so, we employed regression models, such as Linear Regression and Random Forest Regressor, for determining the score of submissions. Second, we wanted to determine what aspects of reddit users’ comments resulted in it being flagged as controversial. We used classification models, such as Logistic Regression, Gradient Boosting Trees, and Random Forest, to predict the controversiality indicator of comments. Lastly, as an exploratory excercise, we implemented KMeans clustering on the word embeddings of the submissions to gauge whether subreddits can be segmented based on content. These models were chosen for their suitability in addressing three distinct types of problems:\n\nRegression for quantitatively predicting a post’s score, and\nClassification for determining the binary outcome of a post’s controversial nature.\nClustering for understanding whether subreddits submissions can be segmented based on content and other features.\n\nThroughout the analysis, we faced challenges like class imbalances in our features and response. We created multiple models for the regression task, by including and excluding predictors, and found that the Linear Regression model with all predictors performed best. Yet, the best regression model still could explain only 43% of the variability in the response, score. Classification, on the other hand, yielded better results with the highest accuracy, 84%, outputted by the Random Forest Classifier. For Clustering, we had to reduce the dimensionality of the feature set using Principal Component Analysis (PCA), given that the Word2Vec model returned thousands of columns. These results provided valuable insights into the models’ performances and the complex nature of predictive modeling with massive social media data.\n\n\n\n\n\nOne of the questions we tried to answer is how accurately we’d be able to predict a submission’s score based on several features. This task dealt with approximately 600,000 rows of submissions. In Reddit nomenclature, a submission’s score refers to the number of upvotes minus the number of downvotes. To answer this question, we test two models: Linear Regression and Random Forest.\nThe main features being used for this predictive model are the following:\n\nis_video: Whether a submission has a video.\nspoiler: Whether a submission has spoilers.\nover_18: Whether a submission has content over 18.\ntitle_length: Length of title, which was feature engineered from the title column.\nsubreddit: Subreddit to which a post belongs.\n\nOur initial hypothesis was that regression will lead to very poor results, because the response, score is highly skewed (see table and plot below).\n\n\n\n\n\nColumn\n1%\n25%\n50%\n75%\n99%\n\n\n\n\nscore\n0.0\n1.0\n1.0\n29.0\n110685.0\n\n\n\n\nTable 1: Percentile Distribution of score\n\n\n\n \n\n\n\n\nFigure 1: Distribution of score below 75th Percentile\n\n\n\nThe primary factor contributing to the poor performance is inherent to the response variable, score. Firstly, a vast majority of scores are centered around 0 and 1 and significantly lower frequencies relative to the maximum score of more than 110,685. Figure 1 above illustrates the distribution of this variable, highlighting its right-skewed distribution. Despite attempts to address this skewness through log transforms, no substantial improvement was observed, as even the transformed data remained skewed.\n\n\nThe two main models used were Linear Regression and Random Forest Regressor. The resulting \\(R^2\\) values were 0.005 and 0.007 respectively, which reinforced our hypothesis before we ran the models.\nAnother pivotal reason why the models performed poorly is due to the high class imbalance in three categorical features, is_video, is_spoiler, and over_18. Below is an example of the value counts fot is_spoiler (the other two can be found in the Appendix section):\n\n\n\n\n\nis_spoiler\ncount\n\n\n\n\nfalse\n598,852\n\n\ntrue\n1,056\n\n\n\n\nTable 2: Value Counts of is_spoiler\n\n\n\n \n\n\n\nInitially, we didn’t include time-related features (day, month, year) as well as the features num_comments and gilded, but later incorporated them to explore any temporal effects and user as well as moderator induced behavior on the score. Adding these features significantly increased the model’s performance on the test set! The \\(R^2\\) values for the Linear Regression and Random Forest models both improved, indicating that time did play a significant role in determining the score of a submission. The test set \\(R^2\\) values for the Linear Regression and Random Forest models were 0.429 and 0.386 and the test set RMSE values were 402.07 and 417.101 respectively.\nThe Random Forest model performed slightly worse than the Linear Regression model, which was surprising given that Random Forests are generally more robust to outliers and noise. These metrics are shown in the tables below:\n\n\n\n\n\nEvaluation Metric\nValue\n\n\n\n\n\\(R^2\\)\n0.429\n\n\nRMSE\n402.07\n\n\n\n\nTable 3: Linear Regression Test Set Model Metrics\n\n\n\n \n\n\n\n\n\nEvaluation Metric\nValue\n\n\n\n\n\\(R^2\\)\n0.386\n\n\nRMSE\n417.101\n\n\n\n\nTable 4: Random Forest Regressor Test Set Model Metrics\n\n\n\nIn fact, through the use of the featureImportances attribute of the Random Forest model, we were able to determine that the most important features were gilded, day, title_length, subreddit_vec, and year. This is shown in the plot below:\n\n\n\n\nFigure 2: Random Forest Feature Importance\n\n\n\nWe also plotted the residuals of both regression models to see if there were any patterns in the errors. The plots below shows that a decent amount of the errors are centered around 0, which is a good sign. However, there are some outliers that are worth investigating further. Specifically, from Linear Regression’s residual plot, we see that its predicted values are much higher than the predicted values of the Random Forest model. This is likely due to the fact that Linear Regression is more sensitive to outliers than Random Forests. However, this could also be the reason why Linear Regression performed better than Random Forests because it was able to capture the extreme values of the response variable.\n\n\n\n\nFigure 3: Linear Regression Test Set Residuals\n\n\n\n\n\n\n\nFigure 4: Random Forest Test Set Residuals\n\n\n\nTo improve the performance of Random Forest Regressor, we could conduct grid search hyper-parameter tuning. We did try increasing numTrees from 30 to 100 and found that the \\(R^2\\) value increased to 0.39, by 1%. However, this is not a significant improvement and we believe that the Random Forest Regressor is overfitting the data.\n\n\n\n\nAlongside submissions analysis, we wanted to look into the truly interactive element of Reddit – the comments section. Much like the scores on submissions, comments also have a score that refers to the number of upvotes minus the number of downvotes. However, the controverisality marker is a binary indicator regarding these upvotes and downvotes. According to this Reddit post, controversiality is determined based on both interactions and the balance of these interactions. In other words, a comment needs to have sufficiently large number of upvotes and downvotes, as well as having a balance of upvotes and downvotes (which would result in a score close to 0).\nTo prepare the comments for analysis, further pre-processing was conducted, such as excluding comments that have been removed or deleted and separating our the created_utc column into individual years, months, and days. Most importantly, since controversiality is a severly imbalanced target variable (approximately 93% non-controversial and 7% controversial), we added a weight column for controversiality to balance class weights and ensure our models pay more attention to controversial comments by up-weighting its importance during training.\nWe created a Pipeline with the following steps:\n\nStringIndexer(): Converts the columns controversiality, distinguished, and subreddit into numerical representations.\nVectorAssembler(): Combines the feature columns distinguished_ix, subreddit_ix, year, month, day, score, and gilded into a single features column.\npipeline_model: Represents the ML model being used in the pipeline. Changes based on the argument passed into the Spark Processing Job. Possible options are Random Forests, Logistic Regression, and Gradient Boosted Trees.\nIndexToString(): Converts the numeric predictions from the model back to the original binary indicators of controversiality.\n\n \n\n\nThe three models fit were Random Forest, Logistic Regression, and Gradient Boosting. The confusion matrices for each of the models are in the appendix section below, but here is a summary of the findings:\n\nRandom Forest: The Random Forest model performed the best out of the three models. It had the highest accuracy, precision, recall, and F1-score. However, it has the lowest number of True Positives, which is not ideal given that we want to be able to predict controversial comments, and comprises on performance by classifying the highest number of True Negatives, cases where the actual label is non-controversial and the model predicts that, and classifying the fewest False Negatives, cases where the actual label is controversial and the model fails to predicts that, than the other algorithms.\nLogistic Regression: The Logistic Regression model performed the worst out of the three models. It had the lowest accuracy, precision, recall, and F1-score. However, opposite to Random Forest, it has the highest number of True Positives, which is ideal given that we want to be able to predict controversial comments. Yet, it classifies the highest number of False Negatives, cases where the actual label is controversial and the model fails to predicts that, than the other algorithms.\nGradient Boosted Trees: The Gradient Boosted Trees model performed in the middle out of the three models. It had the second highest accuracy, precision, recall, and F1-score. It has the second highest number of True Positives, which is ideal given that we want to be able to predict controversial comments. Yet, it classifies the second highest number of False Negatives, cases where the actual label is controversial and the model fails to predicts that, than the other algorithms.\n\nThe AUC-ROC curve of each of the models can be found below:\n\n\n\n\n\n\nFigure 5: Test Set AUC-ROC Curves\n\n\n\nGiven that the dataset is heavily imbalanced, we believe the AUC-ROC plot is a better measure of a model’s ability to distinguish between controversial and non-controversial comments. When looking solely at the figure above, Gradient Boosted Trees performs best (albeit marginally), followed by Random Forests, and lastly Logistic Regression.\nWhen accounting for the evaluation metrics, confusion matrices, and the AUC-ROC plot, Random Forests seems to perform the best overall, followed by Gradient Boosted Trees, and lastly Logistic Regression.\n\n\n\n\nThe final task we wanted to tackle was to determine what subreddit a submission belongs to based on the submission body and other features. This task also dealt with approximately 600,000 rows of submissions. In order to do so, we employed clustering models, such as K-Means, for determining the subreddit of a submission. The features used for this unsupervised model are the following: day, month, year, distinguished, score, gilded, num_comments, score, over_18, is_spoiler, and is_video. Apart from these features, we also used the word embeddings of the submission body, which were generated using the Word2Vec algorithm. Once all the features were combined, we used VectorAssembler to combine them into a single features column, scaled the features using StandardScaler, and then reduced the dimensionality of the features using PCA to obtain two principal components.\nThe figure below showcases the elbow method, which was one of the ways to determine the optimal number of clusters.\n\n\n\n\nFigure 6: Elbow method\n\n\n\nThe optimal number of clusters was determined to be four based on distortion and six based on inertia. Therefore, we decided to go with five clusters as a compromise between the two methods:\n\n\n\n\nFigure 7: K-Means Clustering with 5 Clusters\n\n\n\nNot satisfied with the elbow method, we also decided to use the silhouette score to determine the optimal number of clusters. The silhouette score is a measure of how similar an object is to its own cluster compared to other clusters. The score ranges from -1 to 1, where a score of 1 indicates that the object is far away from its neighboring cluster and very close to the cluster it is assigned to. The higher the score for a cluster, the more distinct and well-separated the clusters are. In contrast, a score close to -1 suggests overlapping clusters, indicating that the object could be assigned to a neighboring cluster. The figure below shows the silhouette score for different number of clusters:\n\n\n\n\nFigure 8: Silhouette Score vs Number of Clusters\n\n\n\nWe see that the silhouette score is highest for four clusters, which is in line with the elbow method. In fact, the highest silhouette score is 0.91, corresponding to two clusters. This reconciles well with the types of subreddits we are analyzing, which are either political or economics related. The silhouette plot below coupled with the scatter plot on the right shows the clusters for nine clusters:\n\n\n\n\nFigure 9: Silhouette Analysis (9 clusters)\n\n\n\nSilhouette plots look to have an edge over the elbow method as one can evaluate clusters on multiple criteria, including scores below average Silhouette score (red vertical line), wide fluctuations in the size of the plot, and non-uniform thickness. Therefore, it is highly likely that one can end up determining the most optimal number of clusters in K-means using the above plots.\nMost clusters have scores above the average Silhouette score (red vertical line) and the thickness of the silhouette plots suggests that the cluster sizes are highly different or non-uniform. Our aim is to choose those n_clusters that correspond to uniform thickness of the clusters’ Silhouette plot. In the appendix below, for n_clusters=4, the silhouette score is lower than that of n_clusters=9, but the thickness of the silhouette plots is still non-uniform. Unfortunately, we have no n_clusters value that showcases that all the clusters are more or less of similar thickness and, hence, are of similar sizes as can also be verified from the labelled scatter plot on the right. Therefore, we decided to go with two clusters, given that it has the highest silhouette score and the clusters are of similar thickness, as can be seen from the silhouette plot below:\n\n\n\n\nFigure 10: Silhouette Analysis (2 clusters)\n\n\n\nIn conclusion, given the complexity of the data, we believe that the K-Means clustering algorithm did not perform well. The sheer number of overlap in principal components between the political and economics subreddits made it difficult for the algorithm to cluster the data. In addition, the fact that the data is highly imbalanced also made it difficult for the algorithm to cluster the data. In the future, we would like to explore other clustering algorithms, such as DBSCAN, to see if we can get better results.\n\n\n\n\n\n\n\n\n\nover_18\ncount\n\n\n\n\nfalse\n595,936\n\n\ntrue\n3,972\n\n\n\n\nAppendix 1: Value Counts of over_18\n\n\n\n \n\n\n\n\n\nis_video\ncount\n\n\n\n\nfalse\n596876\n\n\ntrue\n3032\n\n\n\n\nAppendix 2: Value Counts of is_video\n\n\n\n \n\n\n\n\nAppendix 3: Test Set Confusion Matrix - Random Forest\n\n\n\n \n\n\n\n\n\nEvaluation Metric\nValue\n\n\n\n\nTest Accuracy\n0.841\n\n\nTest Error\n0.159\n\n\nPrecision\n0.958\n\n\nRecall\n0.867\n\n\nF1-score\n0.868\n\n\n\n\nAppendix 4: Random Forest Model Test Set Metrics\n\n\n\n \n\n\n\n\nAppendix 5: Test Set Confusion Matrix - Gradient Boosted Trees\n\n\n\n \n\n\n\n\n\nEvaluation Metric\nValue\n\n\n\n\nTest Accuracy\n0.667\n\n\nTest Error\n0.333\n\n\nPrecision\n0.971\n\n\nRecall\n0.662\n\n\nF1-score\n0.749\n\n\n\n\nAppendix 6: Gradient Boosted Trees Model Test Set Metrics\n\n\n\n \n\n\n\n\nAppendix 7: Test Set Confusion Matrix - Logistic Regression\n\n\n\n \n\n\n\n\n\nEvaluation Metric\nValue\n\n\n\n\nTest Accuracy\n0.451\n\n\nTest Error\n0.549\n\n\nPrecision\n0.960\n\n\nRecall\n0.427\n\n\nF1-score\n0.561\n\n\n\n\nAppendix 8: Logistic Regression Model Test Set Metrics\n\n\n\n\n\n\n\nAppendix 9: Silhouette Plot for 4 clusters\n\n\n\n\n\n\n\nAppendix 10: Silhouette Plot for 6 clusters\n\n\n\n\n\n\n\nAppendix 11: Silhouette Plot for 8 clusters"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "NLP",
    "section": "",
    "text": "The NLP section focuses on analyzing the trends in submission posts and comments of the nine subreddits gathered, seven related to politics and two related to economics. In these nine subreddits, we uncovered key insights about the overall text distributions and sentiments expressed by its users in both submissions and comments. The main focus, however, is on sentiment analysis, which was conducted using a pre-trained sentiment model from the johnsnowlabs sparkNLP library. With its help, we assigned an overall sentiment of positive, negative, or neutral to each submission and comment.\nThe results of our analysis revealed that there is a decent amount of overlap in the top 100 words present in the politics and economics subreddits comments. We also found major dips in activity in economics subreddits comments, which could be attributed to U.S or global major events in December 2021. In terms of sentiment, we found that r/Conservative, r/Libertarian, and r/centrist contain higher positive sentiment in comments, while r/Economics and r/Liberal contain higher negative sentiment. In submissions, r/AskPolitics and r/Conservative contain higher negative sentiment, while r/Economics and r/Liberal contain higher positive sentiment. We also found that the majority of comments and submissions are positive, with a significant amount of negative sentiment. This suggests that while there is a prevailing positive discourse, a substantial amount of the conversation is polarized, with negative comments forming a notable part of the dialogue. The low percentage of neutral comments could indicate that participants in these discussions tend to take a clear stance rather than a neutral one.\nWe also conducted sentiment analysis on the r/Economics subreddit using external data from the fred python package The results revealed a significant spike in the frequency of “recession” mentions in the r/Economics subreddit, which aligns closely with key economic events. This suggests that the subreddit’s users are highly responsive to real-world economic indicators and news, and underscores the role of major economic announcements in shaping public discourse.\nFinally, LDA Topic Modeling was conducted separately on the submissions of the seven political subreddits and two economics subreddits and it illuminated distinct themes within political and economics subreddits. In politics, discussions ranged from exploring beliefs and perspectives, dissecting political parties and national contexts, to examining government policies and societal initiatives. These encompassed global affairs, societal issues, and ideological perspectives on political systems. In economics, topics spanned global economic and political interconnectedness, discussions on oil protests, housing markets, cryptocurrencies, Biden administration’s efforts in student loan accessibility, trade dynamics, and the fusion of technology and finance through fintech applications.\n\n\n\n\n\nWe created an NLP Pipeline built with johnsnowlabs and the spark-nlp package. The following functions from the package were utilized on both submissions, including the selftext and title columns, and comments, including the body column, datasets:\n\nDocumentAssembler(): Transforms input data into Spark NLP annotated documents.\nTokenizer(): Splits the text into individual words, also known as tokens.\nNormalizer(): Performs various text normalization techniques, such as converting text to lowercase, removing special characters, and replacing abbreviations with their full forms.\nNorvigSweetingModel.pretrained(): Applies a spell checker based on the Norvig and Sweeting algorithm to correct spelling mistakes.\nStopWordsCleaner(): Removes common stop words from the text, such as “the”, “and”, “a”, etc., which are generally not helpful in determining the sentiment of a text.\nLemmatizerModel.pretrained(): Reduces each word to its base form (lemma), which can improve the consistency of analysis by grouping together related words.\nFinisher(): Extracts the processed text from the Spark NLP document and formats it as a regular string for further analysis.\n\n\n\n\n\n\n\n\nRank\nWord\nWord Count\n\n\n\n\n1\npeople\n71,173\n\n\n2\ncmv (Change My View)\n60,735\n\n\n3\nBiden\n50,499\n\n\n…\n…\n…\n\n\n9\nTrump\n33,879\n\n\n…\n…\n…\n\n\n13\nright\n27,886\n\n\n14\nstate\n27,344\n\n\n…\n…\n…\n\n\n29\ngovernment\n19,999\n\n\n…\n…\n…\n\n\n42\ndemocrat\n16,373\n\n\n43\ncountry\n15,758\n\n\n44\nman\n15,494\n\n\n45\nrepublic\n15,333\n\n\n…\n…\n…\n\n\n53\nconservative\n13,856\n\n\n54\nfind\n13,637\n\n\n55\nlaw\n13,603\n\n\n56\nparty\n13,541\n\n\n…\n…\n…\n\n\n59\npolice\n13,263\n\n\n60\nmean\n13,216\n\n\n61\nelection\n13,151\n\n\n…\n…\n…\n\n\n73\nAmerica\n12,469\n\n\n74\nlibertarian\n12,465\n\n\n…\n…\n…\n\n\n99\ngun\n10,698\n\n\n100\ntax\n10,677\n\n\n\n\nTable 1: Select Top 100 Frequent Words in Politics Subreddits Submissions\n\n\n\nTable 1 above shows the top 100 most frequent words in the submissions of the seven political subreddits. We can see that users predominantly discuss a variety of political topics. The top ten most frequently used words suggest a focus on individuals (“people”), the Change My View (CMV) subreddit, and prominent political figures such as Joe Biden and Donald Trump. Other notable themes include discussions about political ideologies (“right,” “conservative,” “libertarian”), governmental entities (“government,” “state”), and political processes (“election”). Additionally, terms like “democrat,” “republic,” and “country” indicate discussions related to political parties and the broader national context. Finally, the presence of words like “police,” “gun,” and “tax” suggests discussions on law enforcement, firearms, and taxation policies. Overall, the word frequency analysis indicates a diverse range of political topics and perspectives within these subreddits, reflecting the complexity and breadth of political discourse on political subreddits.\nNote: Tables for other subreddits are in the appendix.\n\n\n\n\n\n\n\n\nRank\nWord\nTF-IDF Value\n\n\n\n\n1\nbaathism\n99.91752355387538\n\n\n2\nsunscreen\n99.91752355387538\n\n\n3\nstormcloaks\n99.9059313812928\n\n\n4\nsubjectivism\n99.9059313812928\n\n\n5\nbreakdancing\n99.9059313812928\n\n\n6\nthalmor\n99.9059313812928\n\n\n7\nmdici\n99.9059313812928\n\n\n8\nneopaganism\n99.9059313812928\n\n\n9\nhaganah\n99.9059313812928\n\n\n10\nakilensai\n99.9059313812928\n\n\n\n\nTable 2: Top 10 Important Words in Politics Subreddits Submissions\n\n\n\nNote: Tables for other subreddits are in the appendix.\n\n\n\n\n\n\n\nFigure 1: Histogram of Text Length for Politics Subreddits Submissions\n\n\n\n\n\n\n\nFigure 2: Line Chart of Number of Words Over Time in Economics and Politics Subreddits Submissions and Comments\n\n\n\nFigure 1 (and all histograms similar to it in the appendix) denote large outliers for submissions and comments in the subreddits (with Figure 1 focusing on Politics subreddits), and a majority of submissions and posts have text lengths between 0 to 500 words. In figure 2, however, we see a massive drop in average word count from 40 to almost 25 in the economics subreddits’ comments from December 2021 to March 2022. A number of major events took place in the U.S around those dates and could have been the cause of the drop in average word count. The events include the Supreme Court hearing arguments on the Texas abortion law and COVID-19 updates on the Omicron virus. Activity for political subreddits’ submissions and comments seem to be relatively stable, with a slight peak in submissions in May 2022.\n\n\n\n\n\n\n\n\n\n\nTable 3: Count of Sentiment Type by Politics and Economics Subreddits (Comments)\n\n\n\nTable 3 presents sentiment analysis data for various political subreddits. Notably, ‘Conservative’ and ‘Libertarian’ subreddits exhibit a higher proportion of positive comments, with 57.44% and 58.44%, respectively. In contrast, the ‘Economics’ subreddit shows a balance between positive and negative sentiments, with negative comments slightly edging out at 57.22%. The ‘Liberal’ subreddit has a relatively even distribution of positive and negative sentiments, but negative comments are more prevalent at 52.59%. ‘Centrist’ stands out with the highest percentage of positive comments at 60.16%. Each subreddit demonstrates a unique sentiment profile, which could reflect the general attitudes and discussions prevalent in these online communities.\n\n\n\n\n\n\nTable 4: Total Count by Sentiment Type (Comments)\n\n\n\nTable 4 summarizes the overall sentiment of comments, showing that the majority (57.85%) are positive, while a significant portion (38.14%) are negative. Neutral comments constitute the smallest group at 4.02%. This distribution suggests that while there is a prevailing positive discourse, a substantial amount of the conversation is polarized, with negative comments forming a notable part of the dialogue. The low percentage of neutral comments could indicate that participants in these discussions tend to take a clear stance rather than a neutral one.\n\n\n\n\n\n\nTable 5: Count of Sentiment Type by Politics and Economics Subreddits (Submissions)\n\n\n\nTable 5 provides data on the sentiment of submissions across various political subreddits. In r/AskPolitics, the distribution is almost evenly split between positive and negative submissions, with negligible neutral content. r/Conservative submissions lean more towards the positive at 51.23%, but also have a substantial proportion of negative sentiment at 44.87%. The r/Economics subreddit is predominantly positive at 56.97%, with negative submissions also forming a significant part at 40.05%. ‘Liberal’ submissions are mostly positive at 54.37%. Overall, while positive submissions prevail across the subreddits, there is a notable presence of negative sentiment, reflecting the contentious nature of political discourse on these platforms.\n\n\n\n\n\n\nTable 6: Total Count by Sentiment Type (Submissions)\n\n\n\nTable 6 shows the sentiment distribution of total submissions, where positive sentiments constitute the majority with 54.28%. Negative sentiments are also prominent, comprising 42.02% of submissions, which points to a significant level of divisiveness or critical discourse. Neutral sentiments are relatively rare, making up only 3.71% of the total, suggesting that most contributors have definitive opinions that are either positive or negative, rather than neutral. This distribution highlights a polarized environment where neutral or ambivalent voices are far less common compared to those expressing clear-cut positive or negative stances.\n\n\n\n\n\nFigure 3: Number of Posts By Sentiment Over Time in r/centrist Submissions\n\n\n\nFigure 3 above (and the similar ones in the appendix) showcases the relationship between U.S. Real GDP and the number of posts categorized by sentiment (positive, neutral, and negative) from various political subreddits over time. The line graph represents the trend in Real GDP, which seems to rise steadily in most charts before plateauing or contracting, indicating a period of economic downturn. Bar graphs display the sentiment of posts in the respective subreddits, with the volume of posts often showing fluctuations.\nFrom a cross-comparison among the subreddits, it seems that the trends in sentiment and GDP do not correspond uniformly across different political spectra. The sentiment analysis from these subreddits could potentially reflect the general mood or economic outlook of their members, with varying degrees of positive, neutral, and negative posts. Notably, there are points where an increase in negative sentiment coincides with a contraction in GDP, which could suggest a correlation between economic performance and public sentiment as expressed in these online communities. However, without a deeper statistical analysis, it cannot be conclusively stated that there is a causal relationship.\n\n\n\n\n\nFigure 4: Number of Submissions in the r/Economics subreddit with mentions of “recession” vs U.S Real GDP\n\n\n\nAs discussed in the EDA section, we gathered external data from the fred python package. The bar chart above depicts the frequency of “recession” mentions in the r/Economics subreddit reveals a significant pattern that aligns closely with key economic events. Initially, the mentions of “recession” were extremely low, indicating a period of less concern or focus on economic downturns among the subreddit’s discussions. This phase reflects a period of either economic stability or a lack of immediate economic threats that would trigger widespread discussions about a recession.\nHowever, a notable change occurs around the third quarter of 2022. During these months, the mentions of “recession” spiked dramatically. This increase in discussions around recession correlates strongly with a significant real-world economic event: the United States announcing that its Gross Domestic Product (GDP) contracted for two consecutive quarters. This is a classic technical indicator of a recession, which understandably would spark increased interest and concern in an economics-focused community.\nThe timing of this spike in conversation suggests that the subreddit’s users are highly responsive to real-world economic indicators and news. It also underscores the role of major economic announcements in shaping public discourse. The heightened focus on the term “recession” during these months likely reflects a mix of concern, analysis, and speculation about the future of the economy, both in the U.S. and globally.\n\n\n\nThe Latent Dirichlet Allocation (LDA) Model is a statistical method for uncovering hidden (latent) topics within a set of documents. It assumes that each document consists of a blend of a limited number of topics, and each topic is essentially a probability distribution across words. The model functions by portraying each document as a probability distribution over topics and each topic as a probability distribution over words. Through the examination of these distributions, LDA can pinpoint the most significant topics related to the documents in a collection, even if these topics are not explicitly expressed in the documents.\nIn this section, we aim to determine the prevalent subjects or themes of the combined seven political subreddits and two economics subreddits submissions. We first employed the LDA model from the pyspark.ml.clustering class in Azure ML and then the LdaModel from gensim.models.ldamodel to conduct topic modeling on the submissions and comments of the nine subreddits. The gensim library was used to generate the interactive visualization, created with the pyLDAvis package, below for our topics as pyspark does not have a visualization tool for LDA models. However, both pyspark and gensim outputted similar topics and the topics generated with gensim are presented below.\nAlso note that when creating the LDA model, we set the number of topics to 5.\n\n\n\n\n\n\n\nFigure 5: Interactive Visualization of Topics in Politics Subreddits Submissions\n\n\n\nThe table below summarizes Figure 5 shown above:\n\n\n\n\n\nTopic\nTopic Words\nSummary\n\n\n\n\n1\npeople, think, want, good, right, cmv (change my view), life, use, change, believe\nExploring perspectives and beliefs in political discourse to understand and potentially alter their own opinions (similar to Table 3 above)\n\n\n2\nwiden (Biden), Trump, cmv (change my view), democrat, republican, conservative, vote, gun, party, bill\nDiscussions related to political parties and the broader national context (similar to Table 1 above)\n\n\n3\nstate, government, work, school, free, child, money, social, public, give, allow\nDiscussion on government policies and societal initiatives with a focus on financial support and accessibility\n\n\n4\ncovid, white, house, police, election, vaccine, abortion, death, law, crime, russia, military, justice\nDiscussions encompassing mournful local and global affairs as well as societal issues\n\n\n5\nlibertarian, socialist, leftist, war, power, politics, history, story, ukraine, afghanistan, russian\nIdeological perspectives on political systems with with a focus on historical narratives and geopolitical events in war-torn regions\n\n\n\n\nTable 7: Summary of Topics in Politics Subreddits Submissions\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Interactive Visualization of Topics in Economics Subreddits Submissions\n\n\n\nThe table below summarizes Figure 6 shown above:\n\n\n\n\n\n\n\n\n\n\nTopic\nTopic Words\nSummary\n\n\n\n\n1\nstock, inflation, market, rate, change, economy, high, world, china, covid, ukraine, russia\nDiscussions highlighting the interconnectedness of global economic and political events with financial markets\n\n\n2\nprice, crypto, rise, tax, house, work, million, bitcoin, investor, job, uk, oil, gas\nDiscussions related to oil protests in the UK, housing markets, and cryptocurrencies\n\n\n3\nnew, year, buy, people, loan, credit, help, widen (Biden), claim, report, student\nDiscussion on Biden administration’s efforts to expand student loan access and financial assistance for borrowers\n\n\n4\nmake, money, trade, business, currency, weakness, strength, payment\nDiscussions on trade and currency dynamics influencing global economic strength and business opportunities\n\n\n5\nfintechinshorts, fintechnews, finance, fund, debt, investment, bill, partner, risk, asset, app\nDiscussing the convergence of technology and finance, while underscoring the collaborative and user-centric approach of fintech as well as risk management\n\n\n\n\nTable 8: Summary of Topics in Economics Subreddits Submissions\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\nWord\nWord Count\n\n\n\n\n1\nfintechinshorts\n7,003\n\n\n…\n…\n…\n\n\n5\nbank\n3,810\n\n\n…\n…\n…\n\n\n10\nmarket\n2,885\n\n\n11\ninflation\n2,784\n\n\n12\nstock\n2,537\n\n\n…\n…\n…\n\n\n17\nrate\n16,373\n\n\n18\nmoney\n2,121\n\n\n19\nman\n2,116\n\n\n20\nget\n2,074\n\n\n21\nprice\n1,958\n\n\n22\ngood\n1,943\n\n\n23\neconomy\n1,937\n\n\n24\nreview\n1,850\n\n\n25\nfinance\n1,790\n\n\n…\n…\n…\n\n\n30\neconomic\n1,448\n\n\n31\ncrypto\n1,429\n\n\n32\ntrade\n1,429\n\n\n33\nworld\n1,429\n\n\n…\n…\n…\n\n\n38\ncovid\n1,310\n\n\n39\nbuy\n1,301\n\n\n40\nglobal\n1,298\n\n\n41\nweakness\n1,296\n\n\n…\n…\n…\n\n\n51\nrise\n1,234\n\n\n52\nhelp\n1,225\n\n\n53\npay\n1,225\n\n\n54\nwork\n1,203\n\n\n55\ntime\n1,200\n\n\n56\nfund\n1,124\n\n\n…\n…\n…\n\n\n96\naccount\n854\n\n\n97\ngrowth\n854\n\n\n98\nfall\n851\n\n\n99\ncrisis\n846\n\n\n100\nrecession\n833\n\n\n\n\nAppendix 1: Select Top 100 Frequent Words in Economics Subreddits Submissions\n\n\n\n\n\n\n\n\n\n\nRank\nWord\nWord Count\n\n\n\n\n1\npeople\n3,916,872\n\n\n2\nlike\n2,429,385\n\n\n3\nsay\n2,388,813\n\n\n4\nget\n2,383,236\n\n\n5\nthink\n2,212,302\n\n\n…\n…\n…\n\n\n28\nstate\n938,986\n\n\n29\nwe\n927,029\n\n\n30\nyear\n828,483\n\n\n31\ngovernment\n828,131\n\n\n…\n…\n…\n\n\n48\nTrump\n655,714\n\n\n49\nactually\n652,720\n\n\n50\nlook\n650,575\n\n\n51\nvote\n644,104\n\n\n52\ncountry\n627,094\n\n\n…\n…\n…\n\n\n59\npay\n583,674\n\n\n60\npost\n573,795\n\n\n61\nlaw\n572,163\n\n\n62\nreason\n565,496\n\n\n…\n…\n…\n\n\n77\nchild\n506,417\n\n\n78\nmoney\n504,222\n\n\n79\nargument\n504,091\n\n\n80\nlive\n500,994\n\n\n…\n…\n…\n\n\n98\nyes\n433,362\n\n\n99\nwrong\n428,341\n\n\n100\nkeep\n427,201\n\n\n\n\nAppendix 2: Select Top 100 Frequent Words in Politics Subreddits Comments\n\n\n\n\n\n\n\n\n\n\nRank\nWord\nWord Count\n\n\n\n\n1\npeople\n274,244\n\n\n2\nmake\n215,993\n\n\n3\nget\n209,560\n\n\n4\ngo\n194,391\n\n\n…\n…\n…\n\n\n7\npay\n143,362\n\n\n8\nmoney\n141,992\n\n\n9\nwork\n141,738\n\n\n10\nthink\n138,345\n\n\n11\nsay\n134,141\n\n\n12\nprice\n128,054\n\n\n13\ntax\n127,177\n\n\n…\n…\n…\n\n\n19\ninflation\n110,184\n\n\n20\nrate\n109,396\n\n\n21\ntime\n106,155\n\n\n22\nmarket\n103,920\n\n\n23\neven\n103,606\n\n\n24\nhouse\n103,563\n\n\n25\nhigh\n99,101\n\n\n…\n…\n…\n\n\n37\njob\n81,568\n\n\n38\nbuy\n80,449\n\n\n39\ncompany\n79,214\n\n\n40\nway\n78,705\n\n\n41\nincrease\n78,237\n\n\n42\ngovernment\n78,082\n\n\n…\n…\n…\n\n\n52\ncountry\n64,732\n\n\n53\ninterest\n64,663\n\n\n54\nlot\n64,473\n\n\n55\nhome\n64,098\n\n\n56\neconomy\n64,068\n\n\n57\nright\n63,889\n\n\n58\nmany\n62,990\n\n\n59\nwage\n62,931\n\n\n60\nbank\n62,018\n\n\n61\nrule\n61,256\n\n\n…\n…\n…\n\n\n98\nreal\n44,515\n\n\n99\nchina\n44,042\n\n\n100\nevery\n44,012\n\n\n\n\nAppendix 3: Select Top 100 Frequent Words in Economics Subreddits Comments\n\n\n\n\n\n\n\n\n\n\nRank\nWord\nTF-IDF Value\n\n\n\n\n1\nblackfriday\n9.762917158451936\n\n\n2\ncraic\n9.762917158451936\n\n\n3\nboo\n9.762917158451936\n\n\n4\nbookings\n9.762917158451936\n\n\n5\nacn\n9.762917158451936\n\n\n6\nbookkeeper\n9.762917158451936\n\n\n7\nadm\n9.762917158451936\n\n\n8\nbrave\n9.762917158451936\n\n\n9\naerospace\n9.762917158451936\n\n\n10\nbroadcom\n9.762917158451936\n\n\n\n\nAppendix 4: Top 10 Important Words in Economics Subreddits Submissions\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\nWord\nWord Count\n\n\n\n\n1\nemancipation\n99.52226201944997\n\n\n2\nsabine\n99.44126423729803\n\n\n3\nspikeproteins\n98.70374062769325\n\n\n4\nwilleford\n98.03656936906297\n\n\n5\ntetrapod\n97.42748973013556\n\n\n6\nmikayla\n97.42748973013556\n\n\n7\nsubsource\n96.34843497134476\n\n\n8\narbitration\n95.59369519372572\n\n\n9\ndelgado\n95.06230029904857\n\n\n10\nadvocateforrightsandknowledgeofamericansarkaghostio\n94.2598337269131\n\n\n\n\nAppendix 5: Top 10 Important Words in Politics Subreddits Comments\n\n\n\n\n\n\n\nAppendix 6: Histogram of Text Length for Economics Subreddits Submissions\n\n\n\n\n\n\n\nAppendix 7: Histogram of Text Length for Politics Subreddits Comments\n\n\n\n\n\n\n\nAppendix 8: Histogram of Text Length for Economics Subreddits Comments\n\n\n\n\n\n\n\nAppendix 9: Line Chart of Number of Words Over Time in Economics and Politics Subreddits Submissions and Comments\n\n\n\n\n\n\n\n\nAppendix 10: Line Chart of Number of Words Over Time in Economics and Politics Subreddits Submissions and Comments\n\n\n\n\n\n\n\n\nAppendix 11: Line Chart of Number of Words Over Time in Economics and Politics Subreddits Submissions and Comments\n\n\n\n\n\n\n\n\nAppendix 12: Line Chart of Number of Words Over Time in Economics and Politics Subreddits Submissions and Comments"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary"
  },
  {
    "objectID": "eda.html#executive-summary",
    "href": "eda.html#executive-summary",
    "title": "EDA",
    "section": "",
    "text": "Our exploratory data analysis (EDA) on Reddit’s major political and economic subreddits has provided a nuanced view of online political discourse, offering valuable context for understanding user behavior. The analysis delved into various aspects of these online subreddit communities, shedding light on the dynamics of discussions and the interplay of user interests.\nPreliminary analysis unveiled that r/Conservative is the most active subreddit, with the highest number of posts and comments, while r/AskPolitics has the most distinct posts and comments, indicating a more diverse user base. The r/Conservative subreddit also has the highest average post score, suggesting that its content resonates strongly with its user base. The analysis also revealed that the r/Conservative subreddit has the most significant number of users in common with other political subreddits, indicating that these users engage in various political communities. Subreddits, such as r/socialism and r/Liberal, showcase consistent activity with fewer fluctuations across number of comments and submissions, suggesting a more stable user base.\nAdditionally, weekly patterns of posting activity revealed that Thursdays are the most active days for posting across r/Liberterian, r/socialism, and r/centrist subreddits, with the exception of r/Conservative, which sees the highest activity on Wednesdays. The analysis also showed that posting activity generally decreases as the year progresses, with the lowest numbers appearing in December. The data also revealed that Saturdays and Sundays consistently have fewer posts than weekdays, in line with typical online engagement trends.\nFinally, we incorporated U.S GDP data from the FRED python package from January 2021 to March 2023 to compare the economic data with the r/Economics subreddit posting activity. The analysis revealed that there is a relationship between the state of the economy and the level of engagement and sentiment on the r/Economics subreddit. During times of economic uncertainty, there is an increase in the number of submissions and a decrease in the average submission score. This suggests that people are more likely to turn to online communities to discuss and seek information about the economy during difficult times. Additionally, the types of posts that are upvoted during these times suggest that people are looking for content that resonates with their concerns and anxieties."
  },
  {
    "objectID": "eda.html#analysis-report",
    "href": "eda.html#analysis-report",
    "title": "EDA",
    "section": "",
    "text": "Figure 1: Line Chart of number of posts (per subreddit)\n\n\n\nFigure 1 presents a line chart that tracks the posting activity across various political and economic subreddits from January 2021 to March 2023. The subreddit r/Conservative shows the most pronounced variability and has the highest peaks, suggesting periods of intense activity which might be attributed to specific political events or discussions that resonated strongly with its user base. It’s noteworthy that the peaks for r/Conservative sharply spike above all other subreddits, indicating that certain topics or times drove engagement significantly more than usual.\nIn contrast, other subreddits such as r/Economics, r/Finance, and r/ChangeMyView maintain a relatively stable, low-level activity over time, with Economics occasionally showing slight increases that could correspond to economic events or policy discussions. Subreddits like r/Liberal, r/socialism, r/Libertarian, and r/centrist exhibit moderate activity with fewer fluctuations, reflecting a consistent level of engagement without the sharp spikes observed in the r/Conservative subreddit. The data suggests that while political subreddits can experience bursts of heightened activity, forums centered around economics and finance tend to have more steady, predictable participation rates.\n\n\n\n\n\nFigure 2: Line Chart of number of comments (per subreddit)\n\n\n\nFigure 2 tracks the volume of comments across various subreddits, highlighting how user interaction ebbs and flows over time. The “Conservative” subreddit shows the most pronounced fluctuations, with sharp spikes indicative of intense conversational bursts. The r/Libertarian subreddit also experiences notable, albeit less extreme, surges in activity, potentially aligning with key events or discussions. Other subreddits, including r/Liberal, r/socialism, and r/centrist present more consistent comment patterns, with occasional upticks that may correspond to specific events. The steady increase in the r/Economics subreddit comments towards the end of the timeline suggests growing engagement. This visualization reflects the dynamic nature of online discourse, with political forums often at the mercy of the news cycle, while more thematic forums like r/Finance or r/Economics maintain a steadier level of dialogue.\nNote: Similar plots for distinct posts, comment counts, and distinct comments can be found in the appendix.\n\n\n\nThe subreddits were ranked based on the counts of meaures that correspond with controversial posts and comments. As Figures 1 and 2 have shown already, there is greater number of posts and comments from the r/Conservative subreddit, so the counts shown in Table 1 have been normalized. Unsurprisingly, the r/ChangeMyView subreddit had the greatest number of distinguished posts and comments (being nearly 2 standard deviations from the mean), which may hint at heavy moderation, potentially as a result of touchy subjects being discussed. Ironically, the subreddits typically associated with the authoritarian left and right on the political compass (r/socialism and r/Conservative, respectively) have less distinguished submissions and posts that the libertarian left and right subreddits (r/Liberal and r/Libertarian, respectively), pointing to an ironic inversion from the expectations of subreddit moderation. Additionally, the r/Conservative subreddit had the greatest number of gilded posts and comments. This, at a glance, may seem like users incentivizing each other financially with the prospects of Reddit Gold, which would inevitably promote an echo chamber of ideas in an online political space; however, this doesn’t seem to be the case when considering that the subreddit also has the highest count of controversial comments (having a count of over 2 standard deviations from the mean). Instead, this may suggest that users on this subreddit are simply more willing to monetarily support posts and comments that they find appealing.\n\n\n\n\n\n\nTable 1: Subreddits with the Most Number of Controversial Posts and Comments (Normalized)\n\n\n\nNext, we looked at individual users and ranked them based on the counts of measures that corerspond with controversial posts and comments. Based on the distinguished submissions and comments, we can see that among the top 20 controversial users, only one of them (user: “ultimis”) is a moderator. Another finding is that the top 20 users all come from the subreddits r/centrist, r/Libertarian, or r/Conservative. Much like the findings from Table 1, Table 2 also seems to suggest that these three political subreddits have a large amount of intra-subreddit discourse among their posts and comments.\n\n\n\n\n\n\nTable 2: Top 20 Users with the Most Number of Controversial Posts and Comments\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Bubble Chart showing the 50 highest scored posts per subreddit.\n\n\n\nFigure 3 is a bubble chart illustrating the average scores of the top 50 posts from various political subreddits, where the score is calculated by subtracting the number of downvotes from upvotes. The size of each subreddit’s bubble reflects the average score of its posts. The chart shows that the subreddit represented by the largest bubble has the highest average post score, while the smallest bubble corresponds to the subreddit with the lowest average score.\nThe subreddit with the largest bubble (r/Conservative) has significantly outpaced the others in terms of average score, suggesting that posts in this community resonate strongly with its members and receive more upvotes. Conversely, the subreddit with the smallest bubble (r/centrist) has the lowest average scores, indicating either a smaller community, less engagement, or a tendency towards more divisive content that doesn’t amass as many upvotes.\nWithin each of these larger subreddit bubbles are smaller bubbles, each representing an individual post; the title of the post is indicated within these smaller bubbles. Additionally, interacting with these smaller bubbles by clicking on them will redirect the viewer to the respective post’s link on Reddit for further exploration.\n\n\n\n\n\n\n\n\nFigure 4: Calendar Heatmap of r/Conservative Reddit Posts\n\n\n\nFigure 4 depicts the daily and monthly number of posts on the r/Conservative subreddit. There’s a clear trend of higher activity on weekdays, with Wednesdays being the peak, and lower activity on weekends, particularly on Sundays. The data indicates a seasonal trend, with post volumes generally higher in March and April and reducing towards the end of the year, in November and December. The color gradient emphasizes these variations, with darker hues representing more posts, allowing for a visual representation of user engagement on Reddit throughout the different months and days of the week.\nNote: A calendar heatmap for different subreddits can be found in the appendix section.\n\n\n\n\n\n\n\n\nFigure 5: Dependency Wheel of Users in different Subreddits\n\n\n\nThe dependency wheel depicted in Figure 5 above provides a graphical representation of the shared user base between various political subreddits. The thickness of the connecting bands is directly proportional to the number of users that participate in both subreddits at each end of the band. A notable observation from the wheel is the significant interconnectivity between certain subreddits, which could suggest a shared ideological proximity or an interest overlap among their user bases. For instance, if there is a thick band between the subreddits labeled r/Liberal and r/socialism, this would indicate a large common audience, possibly due to similar political leanings or discussions that appeal to both groups.\nAdditionally, the visualization reveals subreddits that serve as common grounds for diverse political discourse, such as r/ChangeMyView or r/AskPolitics, where the number of interlinking bands suggests a wide range of users from different political backgrounds converging for debate or inquiry. This could imply that these platforms are more neutral or open-ended, attracting a varied audience seeking to engage with multiple perspectives. The overall pattern highlights not only the segmentation within the political discourse on Reddit but also the points of intersection where cross-ideological conversations are occurring.\n\n\n\nWe present the Cosine and Jaccard similarity scores for posts’ bodies and titles across grouped political and economics subreddits. The political subreddits utilized for the two tables below are r/Conservative, r/Socialism, r/centrist, and r/Libertarian and the economic subreddits are r/Finance and r/Economics. Initially, we obtained the counts of three words, [recession, inflation, and unemployment], for the grouped subreddits from regex patterns on the posts’ bodies and ran Cosine as well as Jaccard Similarity scores on the counts. The Cosine Similarity score between the grouped subreddits was 0.9 and Jaccard Similarity was 0.04, which meant that the two subreddits have similar distributions of words, even though the counts of individual words were significantly different. This finding culminated in two hypotheses, the first being that the grouped economic subreddits must be containing a large number of “shitposts”, probably related to cryptocurrencies and NFT’s, and the second one that economics subreddits posts’ bodies contain mainly links to online news articles with the key words present in the post titles. Consequently, we updated the word list to account for [crypto, blockchain, nft] words contained in these flippant posts and also ran the similarity algorithms on the posts’ titles.\nIn addition to the economics word list, we also made a political word list to obtain grouped subreddit counts for the words [trump, biden, election, fed, powell] and ran the similarity algorithms on the posts’ bodies and titles. Its results and discussion are presented below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3: Cosine and Jaccard Similarity of Post Body for combined Political and Economics subreddits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4: Cosine and Jaccard Similarity of Post Title for combined Political and Economics subreddits\n\n\n\nTable 4 supports our hyptheses, as we can see that the Jaccard Similarity score is highest across the post titles of the grouped subreddits when we filter for the economics word list. This means that the posts’ titles not only have a similar distribution of counts across the grouped subreddits but they also have similar raw counts for each individual word. This is because the posts’ titles contain the key words that we are looking for, while the posts’ bodies contain links to online news articles. On the other, political words in either the posts’ bodies or titles do not have a similar distribution of counts across the grouped subreddits, as shown in both Table 3 and Table 4.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Number of r/Economics Submissions vs U.S Real GDP\n\n\n\nFigure 6 presents a comparative analysis of the United States’ Real Gross Domestic Product (GDP) against the number of submissions in the r/Economics subreddit over a period extending from January 2021 to March 2023. Evident in the plot, the submission count hovers around 1,000 posts per month, reflecting a steady engagement level within the subreddit community.\nA notable deviation occurs in July 2022, marked by an annotation that points out the release of news indicating the U.S economy’s contraction for two consecutive quarters, often considered a technical indicator of a recession. Following this announcement, there is a marked spike in subreddit activity in August and September 2022, with submissions surging to well over four times the usual number. This spike in activity suggests a heightened collective concern and a rush to discuss and understand the implications of the economic news\n\n\n\n\n\n\n\n\nFigure 7: Average Score of r/Economics Submissions vs U.S Real GDP\n\n\n\nFigure 7 again plots the United States’ Real Gross Domestic Product (GDP), but this time against the average submission score in the r/Economics subreddit over a period extending from January 2021 to March 2023. Conversely, this visualization reveals a stark drop in the average score of submissions during the same period of economic concern. This significant downturn in average score might also be interpreted as a barometer of public sentiment towards the economy, rather than simply a measure of post quality. In times of economic uncertainty, it’s common for sentiment to sour, and this shift is often reflected in the collective mood of discussions and the types of content that get upvoted. As the real GDP contracted for two consecutive quarters, the mood likely shifted from cautiously optimistic to more pessimistic and critical, leading to a preference for upvoting posts that resonate with the community’s concerns and anxieties. However, it is also worth noting that since the number of posts in that period also significantly increased, it is likely that lower post activity (comments/upvotes) could also have impacted the average score during that period."
  },
  {
    "objectID": "nlp.html#analysis-2",
    "href": "nlp.html#analysis-2",
    "title": "NLP",
    "section": "",
    "text": "The bar chart above depicts the frequency of “recession” mentions in the r/Economics subreddit reveals a significant pattern that aligns closely with key economic events. Initially, the mentions of “recession” were extremely low, indicating a period of less concern or focus on economic downturns among the subreddit’s discussions. This phase reflects a period of either economic stability or a lack of immediate economic threats that would trigger widespread discussions about a recession. However, a notable change occurs around the third quarter of 2022. During these months, the mentions of “recession” spiked dramatically. This increase in discussions around recession correlates strongly with a significant real-world economic event: the United States announcing that its Gross Domestic Product (GDP) contracted for two consecutive quarters. This is a classic technical indicator of a recession, which understandably would spark increased interest and concern in an economics-focused community. The timing of this spike in conversation suggests that the subreddit’s users are highly responsive to real-world economic indicators and news. It also underscores the role of major economic announcements in shaping public discourse. The heightened focus on the term “recession” during these months likely reflects a mix of concern, analysis, and speculation about the future of the economy, both in the U.S. and globally."
  },
  {
    "objectID": "nlp.html#tables",
    "href": "nlp.html#tables",
    "title": "NLP",
    "section": "",
    "text": "Table 1 presents sentiment analysis data for various political subreddits. Notably, ‘Conservative’ and ‘Libertarian’ subreddits exhibit a higher proportion of positive comments, with 57.44% and 58.44%, respectively. In contrast, the ‘Economics’ subreddit shows a balance between positive and negative sentiments, with negative comments slightly edging out at 57.22%. The ‘Liberal’ subreddit has a relatively even distribution of positive and negative sentiments, but negative comments are more prevalent at 52.59%. ‘Centrist’ stands out with the highest percentage of positive comments at 60.16%. Each subreddit demonstrates a unique sentiment profile, which could reflect the general attitudes and discussions prevalent in these online communities.\n\n\n\nTable 2 summarizes the overall sentiment of comments, showing that the majority (57.85%) are positive, while a significant portion (38.14%) are negative. Neutral comments constitute the smallest group at 4.02%. This distribution suggests that while there is a prevailing positive discourse, a substantial amount of the conversation is polarized, with negative comments forming a notable part of the dialogue. The low percentage of neutral comments could indicate that participants in these discussions tend to take a clear stance rather than a neutral one.  Table 3 provides data on the sentiment of submissions across various political subreddits. In ‘AskPolitics’, the distribution is almost evenly split between positive and negative submissions, with negligible neutral content. ‘Conservative’ submissions lean more towards the positive at 51.23%, but also have a substantial proportion of negative sentiment at 44.87%. The ‘Economics’ subreddit is predominantly positive at 56.97%, with negative submissions also forming a significant part at 40.05%. ‘Liberal’ submissions are mostly positive at 54.37%. Overall, while positive submissions prevail across the subreddits, there is a notable presence of negative sentiment, reflecting the contentious nature of political discourse on these platforms. \nTable 4 shows the sentiment distribution of total submissions, where positive sentiments constitute the majority with 54.28%. Negative sentiments are also prominent, comprising 42.02% of submissions, which points to a significant level of divisiveness or critical discourse. Neutral sentiments are relatively rare, making up only 3.71% of the total, suggesting that most contributors have definitive opinions that are either positive or negative, rather than neutral. This distribution highlights a polarized environment where neutral or ambivalent voices are far less common compared to those expressing clear-cut positive or negative stances."
  },
  {
    "objectID": "nlp.html#analysis",
    "href": "nlp.html#analysis",
    "title": "NLP",
    "section": "",
    "text": "The series of plots above showcase the relationship between U.S. Real GDP and the number of posts categorized by sentiment (positive, neutral, and negative) from various political subreddits over time. The line graph represents the trend in Real GDP, which seems to rise steadily in most charts before plateauing or contracting, indicating a period of economic downturn. Bar graphs display the sentiment of posts in the respective subreddits, with the volume of posts often showing fluctuations.\nFrom a cross-comparison among the subreddits, it seems that the trends in sentiment and GDP do not correspond uniformly across different political spectra. The sentiment analysis from these subreddits could potentially reflect the general mood or economic outlook of their members, with varying degrees of positive, neutral, and negative posts. Notably, there are points where an increase in negative sentiment coincides with a contraction in GDP, which could suggest a correlation between economic performance and public sentiment as expressed in these online communities. However, without a deeper statistical analysis, it cannot be conclusively stated that there is a causal relationship."
  },
  {
    "objectID": "nlp.html#executive-summary",
    "href": "nlp.html#executive-summary",
    "title": "NLP",
    "section": "",
    "text": "The NLP section focuses on analyzing the trends in submission posts and comments of the nine subreddits gathered, seven related to politics and two related to economics. In these nine subreddits, we uncovered key insights about the overall text distributions and sentiments expressed by its users in both submissions and comments. The main focus, however, is on sentiment analysis, which was conducted using a pre-trained sentiment model from the johnsnowlabs sparkNLP library. With its help, we assigned an overall sentiment of positive, negative, or neutral to each submission and comment.\nThe results of our analysis revealed that there is a decent amount of overlap in the top 100 words present in the politics and economics subreddits comments. We also found major dips in activity in economics subreddits comments, which could be attributed to U.S or global major events in December 2021. In terms of sentiment, we found that r/Conservative, r/Libertarian, and r/centrist contain higher positive sentiment in comments, while r/Economics and r/Liberal contain higher negative sentiment. In submissions, r/AskPolitics and r/Conservative contain higher negative sentiment, while r/Economics and r/Liberal contain higher positive sentiment. We also found that the majority of comments and submissions are positive, with a significant amount of negative sentiment. This suggests that while there is a prevailing positive discourse, a substantial amount of the conversation is polarized, with negative comments forming a notable part of the dialogue. The low percentage of neutral comments could indicate that participants in these discussions tend to take a clear stance rather than a neutral one.\nWe also conducted sentiment analysis on the r/Economics subreddit using external data from the fred python package The results revealed a significant spike in the frequency of “recession” mentions in the r/Economics subreddit, which aligns closely with key economic events. This suggests that the subreddit’s users are highly responsive to real-world economic indicators and news, and underscores the role of major economic announcements in shaping public discourse.\nFinally, LDA Topic Modeling was conducted separately on the submissions of the seven political subreddits and two economics subreddits and it illuminated distinct themes within political and economics subreddits. In politics, discussions ranged from exploring beliefs and perspectives, dissecting political parties and national contexts, to examining government policies and societal initiatives. These encompassed global affairs, societal issues, and ideological perspectives on political systems. In economics, topics spanned global economic and political interconnectedness, discussions on oil protests, housing markets, cryptocurrencies, Biden administration’s efforts in student loan accessibility, trade dynamics, and the fusion of technology and finance through fintech applications."
  },
  {
    "objectID": "nlp.html#analysis-report",
    "href": "nlp.html#analysis-report",
    "title": "NLP",
    "section": "",
    "text": "We created an NLP Pipeline built with johnsnowlabs and the spark-nlp package. The following functions from the package were utilized on both submissions, including the selftext and title columns, and comments, including the body column, datasets:\n\nDocumentAssembler(): Transforms input data into Spark NLP annotated documents.\nTokenizer(): Splits the text into individual words, also known as tokens.\nNormalizer(): Performs various text normalization techniques, such as converting text to lowercase, removing special characters, and replacing abbreviations with their full forms.\nNorvigSweetingModel.pretrained(): Applies a spell checker based on the Norvig and Sweeting algorithm to correct spelling mistakes.\nStopWordsCleaner(): Removes common stop words from the text, such as “the”, “and”, “a”, etc., which are generally not helpful in determining the sentiment of a text.\nLemmatizerModel.pretrained(): Reduces each word to its base form (lemma), which can improve the consistency of analysis by grouping together related words.\nFinisher(): Extracts the processed text from the Spark NLP document and formats it as a regular string for further analysis.\n\n\n\n\n\n\n\n\nRank\nWord\nWord Count\n\n\n\n\n1\npeople\n71,173\n\n\n2\ncmv (Change My View)\n60,735\n\n\n3\nBiden\n50,499\n\n\n…\n…\n…\n\n\n9\nTrump\n33,879\n\n\n…\n…\n…\n\n\n13\nright\n27,886\n\n\n14\nstate\n27,344\n\n\n…\n…\n…\n\n\n29\ngovernment\n19,999\n\n\n…\n…\n…\n\n\n42\ndemocrat\n16,373\n\n\n43\ncountry\n15,758\n\n\n44\nman\n15,494\n\n\n45\nrepublic\n15,333\n\n\n…\n…\n…\n\n\n53\nconservative\n13,856\n\n\n54\nfind\n13,637\n\n\n55\nlaw\n13,603\n\n\n56\nparty\n13,541\n\n\n…\n…\n…\n\n\n59\npolice\n13,263\n\n\n60\nmean\n13,216\n\n\n61\nelection\n13,151\n\n\n…\n…\n…\n\n\n73\nAmerica\n12,469\n\n\n74\nlibertarian\n12,465\n\n\n…\n…\n…\n\n\n99\ngun\n10,698\n\n\n100\ntax\n10,677\n\n\n\n\nTable 1: Select Top 100 Frequent Words in Politics Subreddits Submissions\n\n\n\nTable 1 above shows the top 100 most frequent words in the submissions of the seven political subreddits. We can see that users predominantly discuss a variety of political topics. The top ten most frequently used words suggest a focus on individuals (“people”), the Change My View (CMV) subreddit, and prominent political figures such as Joe Biden and Donald Trump. Other notable themes include discussions about political ideologies (“right,” “conservative,” “libertarian”), governmental entities (“government,” “state”), and political processes (“election”). Additionally, terms like “democrat,” “republic,” and “country” indicate discussions related to political parties and the broader national context. Finally, the presence of words like “police,” “gun,” and “tax” suggests discussions on law enforcement, firearms, and taxation policies. Overall, the word frequency analysis indicates a diverse range of political topics and perspectives within these subreddits, reflecting the complexity and breadth of political discourse on political subreddits.\nNote: Tables for other subreddits are in the appendix.\n\n\n\n\n\n\n\n\nRank\nWord\nTF-IDF Value\n\n\n\n\n1\nbaathism\n99.91752355387538\n\n\n2\nsunscreen\n99.91752355387538\n\n\n3\nstormcloaks\n99.9059313812928\n\n\n4\nsubjectivism\n99.9059313812928\n\n\n5\nbreakdancing\n99.9059313812928\n\n\n6\nthalmor\n99.9059313812928\n\n\n7\nmdici\n99.9059313812928\n\n\n8\nneopaganism\n99.9059313812928\n\n\n9\nhaganah\n99.9059313812928\n\n\n10\nakilensai\n99.9059313812928\n\n\n\n\nTable 2: Top 10 Important Words in Politics Subreddits Submissions\n\n\n\nNote: Tables for other subreddits are in the appendix.\n\n\n\n\n\n\n\nFigure 1: Histogram of Text Length for Politics Subreddits Submissions\n\n\n\n\n\n\n\nFigure 2: Line Chart of Number of Words Over Time in Economics and Politics Subreddits Submissions and Comments\n\n\n\nFigure 1 (and all histograms similar to it in the appendix) denote large outliers for submissions and comments in the subreddits (with Figure 1 focusing on Politics subreddits), and a majority of submissions and posts have text lengths between 0 to 500 words. In figure 2, however, we see a massive drop in average word count from 40 to almost 25 in the economics subreddits’ comments from December 2021 to March 2022. A number of major events took place in the U.S around those dates and could have been the cause of the drop in average word count. The events include the Supreme Court hearing arguments on the Texas abortion law and COVID-19 updates on the Omicron virus. Activity for political subreddits’ submissions and comments seem to be relatively stable, with a slight peak in submissions in May 2022.\n\n\n\n\n\n\n\n\n\n\nTable 3: Count of Sentiment Type by Politics and Economics Subreddits (Comments)\n\n\n\nTable 3 presents sentiment analysis data for various political subreddits. Notably, ‘Conservative’ and ‘Libertarian’ subreddits exhibit a higher proportion of positive comments, with 57.44% and 58.44%, respectively. In contrast, the ‘Economics’ subreddit shows a balance between positive and negative sentiments, with negative comments slightly edging out at 57.22%. The ‘Liberal’ subreddit has a relatively even distribution of positive and negative sentiments, but negative comments are more prevalent at 52.59%. ‘Centrist’ stands out with the highest percentage of positive comments at 60.16%. Each subreddit demonstrates a unique sentiment profile, which could reflect the general attitudes and discussions prevalent in these online communities.\n\n\n\n\n\n\nTable 4: Total Count by Sentiment Type (Comments)\n\n\n\nTable 4 summarizes the overall sentiment of comments, showing that the majority (57.85%) are positive, while a significant portion (38.14%) are negative. Neutral comments constitute the smallest group at 4.02%. This distribution suggests that while there is a prevailing positive discourse, a substantial amount of the conversation is polarized, with negative comments forming a notable part of the dialogue. The low percentage of neutral comments could indicate that participants in these discussions tend to take a clear stance rather than a neutral one.\n\n\n\n\n\n\nTable 5: Count of Sentiment Type by Politics and Economics Subreddits (Submissions)\n\n\n\nTable 5 provides data on the sentiment of submissions across various political subreddits. In r/AskPolitics, the distribution is almost evenly split between positive and negative submissions, with negligible neutral content. r/Conservative submissions lean more towards the positive at 51.23%, but also have a substantial proportion of negative sentiment at 44.87%. The r/Economics subreddit is predominantly positive at 56.97%, with negative submissions also forming a significant part at 40.05%. ‘Liberal’ submissions are mostly positive at 54.37%. Overall, while positive submissions prevail across the subreddits, there is a notable presence of negative sentiment, reflecting the contentious nature of political discourse on these platforms.\n\n\n\n\n\n\nTable 6: Total Count by Sentiment Type (Submissions)\n\n\n\nTable 6 shows the sentiment distribution of total submissions, where positive sentiments constitute the majority with 54.28%. Negative sentiments are also prominent, comprising 42.02% of submissions, which points to a significant level of divisiveness or critical discourse. Neutral sentiments are relatively rare, making up only 3.71% of the total, suggesting that most contributors have definitive opinions that are either positive or negative, rather than neutral. This distribution highlights a polarized environment where neutral or ambivalent voices are far less common compared to those expressing clear-cut positive or negative stances.\n\n\n\n\n\nFigure 3: Number of Posts By Sentiment Over Time in r/centrist Submissions\n\n\n\nFigure 3 above (and the similar ones in the appendix) showcases the relationship between U.S. Real GDP and the number of posts categorized by sentiment (positive, neutral, and negative) from various political subreddits over time. The line graph represents the trend in Real GDP, which seems to rise steadily in most charts before plateauing or contracting, indicating a period of economic downturn. Bar graphs display the sentiment of posts in the respective subreddits, with the volume of posts often showing fluctuations.\nFrom a cross-comparison among the subreddits, it seems that the trends in sentiment and GDP do not correspond uniformly across different political spectra. The sentiment analysis from these subreddits could potentially reflect the general mood or economic outlook of their members, with varying degrees of positive, neutral, and negative posts. Notably, there are points where an increase in negative sentiment coincides with a contraction in GDP, which could suggest a correlation between economic performance and public sentiment as expressed in these online communities. However, without a deeper statistical analysis, it cannot be conclusively stated that there is a causal relationship.\n\n\n\n\n\nFigure 4: Number of Submissions in the r/Economics subreddit with mentions of “recession” vs U.S Real GDP\n\n\n\nAs discussed in the EDA section, we gathered external data from the fred python package. The bar chart above depicts the frequency of “recession” mentions in the r/Economics subreddit reveals a significant pattern that aligns closely with key economic events. Initially, the mentions of “recession” were extremely low, indicating a period of less concern or focus on economic downturns among the subreddit’s discussions. This phase reflects a period of either economic stability or a lack of immediate economic threats that would trigger widespread discussions about a recession.\nHowever, a notable change occurs around the third quarter of 2022. During these months, the mentions of “recession” spiked dramatically. This increase in discussions around recession correlates strongly with a significant real-world economic event: the United States announcing that its Gross Domestic Product (GDP) contracted for two consecutive quarters. This is a classic technical indicator of a recession, which understandably would spark increased interest and concern in an economics-focused community.\nThe timing of this spike in conversation suggests that the subreddit’s users are highly responsive to real-world economic indicators and news. It also underscores the role of major economic announcements in shaping public discourse. The heightened focus on the term “recession” during these months likely reflects a mix of concern, analysis, and speculation about the future of the economy, both in the U.S. and globally.\n\n\n\nThe Latent Dirichlet Allocation (LDA) Model is a statistical method for uncovering hidden (latent) topics within a set of documents. It assumes that each document consists of a blend of a limited number of topics, and each topic is essentially a probability distribution across words. The model functions by portraying each document as a probability distribution over topics and each topic as a probability distribution over words. Through the examination of these distributions, LDA can pinpoint the most significant topics related to the documents in a collection, even if these topics are not explicitly expressed in the documents.\nIn this section, we aim to determine the prevalent subjects or themes of the combined seven political subreddits and two economics subreddits submissions. We first employed the LDA model from the pyspark.ml.clustering class in Azure ML and then the LdaModel from gensim.models.ldamodel to conduct topic modeling on the submissions and comments of the nine subreddits. The gensim library was used to generate the interactive visualization, created with the pyLDAvis package, below for our topics as pyspark does not have a visualization tool for LDA models. However, both pyspark and gensim outputted similar topics and the topics generated with gensim are presented below.\nAlso note that when creating the LDA model, we set the number of topics to 5.\n\n\n\n\n\n\n\nFigure 5: Interactive Visualization of Topics in Politics Subreddits Submissions\n\n\n\nThe table below summarizes Figure 5 shown above:\n\n\n\n\n\nTopic\nTopic Words\nSummary\n\n\n\n\n1\npeople, think, want, good, right, cmv (change my view), life, use, change, believe\nExploring perspectives and beliefs in political discourse to understand and potentially alter their own opinions (similar to Table 3 above)\n\n\n2\nwiden (Biden), Trump, cmv (change my view), democrat, republican, conservative, vote, gun, party, bill\nDiscussions related to political parties and the broader national context (similar to Table 1 above)\n\n\n3\nstate, government, work, school, free, child, money, social, public, give, allow\nDiscussion on government policies and societal initiatives with a focus on financial support and accessibility\n\n\n4\ncovid, white, house, police, election, vaccine, abortion, death, law, crime, russia, military, justice\nDiscussions encompassing mournful local and global affairs as well as societal issues\n\n\n5\nlibertarian, socialist, leftist, war, power, politics, history, story, ukraine, afghanistan, russian\nIdeological perspectives on political systems with with a focus on historical narratives and geopolitical events in war-torn regions\n\n\n\n\nTable 7: Summary of Topics in Politics Subreddits Submissions\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Interactive Visualization of Topics in Economics Subreddits Submissions\n\n\n\nThe table below summarizes Figure 6 shown above:\n\n\n\n\n\n\n\n\n\n\nTopic\nTopic Words\nSummary\n\n\n\n\n1\nstock, inflation, market, rate, change, economy, high, world, china, covid, ukraine, russia\nDiscussions highlighting the interconnectedness of global economic and political events with financial markets\n\n\n2\nprice, crypto, rise, tax, house, work, million, bitcoin, investor, job, uk, oil, gas\nDiscussions related to oil protests in the UK, housing markets, and cryptocurrencies\n\n\n3\nnew, year, buy, people, loan, credit, help, widen (Biden), claim, report, student\nDiscussion on Biden administration’s efforts to expand student loan access and financial assistance for borrowers\n\n\n4\nmake, money, trade, business, currency, weakness, strength, payment\nDiscussions on trade and currency dynamics influencing global economic strength and business opportunities\n\n\n5\nfintechinshorts, fintechnews, finance, fund, debt, investment, bill, partner, risk, asset, app\nDiscussing the convergence of technology and finance, while underscoring the collaborative and user-centric approach of fintech as well as risk management\n\n\n\n\nTable 8: Summary of Topics in Economics Subreddits Submissions"
  },
  {
    "objectID": "nlp.html#change-in-business-questions",
    "href": "nlp.html#change-in-business-questions",
    "title": "NLP",
    "section": "",
    "text": "Although we had wanted to conduct LDA Topic Modeling on our subreddits, we were unable to do so due to compute resource restrictions in Azure ML . Given the size of our comments data, most of the compute was spent on pre-processing it through the NLP Pipeline built with johnsnowlabs. The following functions from the package were utilized on both submissions and comments data:\nDocumentAssembler(): transforms input data into Spark NLP annotated documents.\nTokenizer(): splits the text into individual words, also known as tokens.\nNormalizer(): performs various text normalization techniques, such as converting text to lowercase, removing special characters, and replacing abbreviations with their full forms.\nNorvigSweetingModel.pretrained(): applies a spell checker based on the Norvig and Sweeting algorithm to correct spelling mistakes.\nStopWordsCleaner(): removes common stop words from the text, such as “the”, “and”, “a”, etc., which are generally not helpful in determining the sentiment of a text.\nLemmatizerModel.pretrained(): reduces each word to its base form (lemma), which can improve the consistency of analysis by grouping together related words.\nFinisher(): extracts the processed text from the Spark NLP document and formats it as a regular string for further analysis.\nAfter this deliverable, we shall try to conduct LDA Topic Modeling using Spark Processing Jobs and keep that business goal open for the time being."
  },
  {
    "objectID": "business_goals.html",
    "href": "business_goals.html",
    "title": "Business Goals",
    "section": "",
    "text": "We start by visualizing each of the nine subreddits’ posts by grouping data on the subreddit and posted date (year and month features engineered from created_utc) and then aggregating the counts of posts over time. Doing so allows us to gauge periods of intense activity in specific subreddits, which could be attributed to significant political and economic events. We use the same grouped data but now aggregate on the author column to get distinct users over time for each subreddit to obtain an idea of unique user participation and differences in user diversity and engagement. We then repeat the same process for comments.\n\n\n\nUse data grouped on subreddit to gather the normalized counts of posts and comments that are gilded (the number of times a post/comment received Reddit gold), distinguished (whether the moderators or admins make the post/comment), and controversial (whether a post/comment received a large number of both upvotes and downvotes). The higher the counts of these variables, the more likely a reddit user will come across and interact with those posts or comments. The same data wrangling is done next but with author added as grouping variable. The counts of users posting or commenting across multiple subreddits are calculated, and a table with the top 50 posts (based on upvotes) from each subreddit is presented. Doing so allows us to identify the most active users across subreddits.\n\n\n\nEach submission and comment receives a score, which is calculated as the number of upvotes minus the number of downvotes as voted by the respective subreddit users. We use the data grouped by subreddit to gather the top 50 posts with the highest scores (based on upvotes and downvotes) from each subreddit. After obtaining the data, we create a HighChartJS Packed Bubble chart, where the size of each bubble corresponds to the score of individual posts, and the size of the larger bubbles represents the cumulative scores of posts from each subreddit. This vizualization allows us to identify which subreddits’ content resonates the most with its users.\n\n\n\nUse data grouped on subreddit, month, and day of the week and then aggregate the counts of posts. Create a HighChartJS Heatmap chart for each subreddit, where a greater color intensity corresponds with more posts. Doing so helps us identify distinct seasonal patterns to understand when users are most active in each subreddit.\n\n\n\nUse datasets that cover all pairwise combinations of political subreddits, joined based on common authors to gather the counts of authors posting in multiple subreddits. Create a HighChartJS Dependency Wheel chart based on the number of authors for each pairwise subreddit combination. The chart provides insights into which subreddits have the most shared users, which could be attributed to similar content or user demographics.\n\n\n\nWe first group politics and economics subreddit submissions and filter subreddits for post body (selftext) and for post title (two-part analysis). Then, using regex, we identify posts that mention keywords pertaining to politics and economics and gather the counts for each grouped subreddit that contains the keywords mentioned above. Next, we independently calculate the cosine similarity and Jaccard similarity measures for the counts for both post titles and post bodies, resulting in two tables (one for post body and another for post title). A high cosine similarity implies that the two documents have similar distributions of words, even though the counts of individual words may be different. To correctly measure the similarity between the counts of words in the two documents, we use another similarity metric, the Jaccard index. By selecting keywords that are most similar across the two subreddits, we can identify context similarities between the two subreddits and the distribution of the keywords across the two subreddits.\n\n\n\nCombine data on the number of posts and comment scores (derived from upvotes and downvotes) with external GDP data gathered from the FRED python package. Perform a time window shift of GDP data (since it’s lagged by three months). Visualize the subreddit post activity with the GDP data with HighChartJS. With the help of this time-series visualization, we can identify patterns in GDP growth and its effects on the activity of the r/Economics subreddit.\n\n\n\n\n\n\nUncover most discussed words: We create four tables of select top 100 common words corresponding to politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments. For analyzing submissions, we group politics and economics subreddits separately and count common words on the feature-engineered column of combined post body (selftext) and post title. We also use regex to replace certain stopwords, such as “https”, “www”, and “jpg”, that the NLP pipeline could not remove. Once the new feature is obtained, it is turned into an array of words, flattened, grouped by word to get the counts, and sorted and filtered to output a .csv file of the top 100 words. For analyzing comments, we group politics and economics subreddits separately and count common words in the comment body, so no new column was created for comments. We also use the regex mentioned above to replace certain stopwords and utilize the same process to obtain the counts of common words.\nUncover most important words: We create three tables of the top 10 most important words, measured by TF-IDF score, corresponding to politics subreddits submissions, economics subreddits submissions, and politics subreddits comments (economics subreddits comments are left out due to allocation of compute capacity). Similar to the process for obtaining the most discussed words, we group politics and economics subreddits separately and run the HashingTF and IDF functions on the feature engineered column of combined post body (selftext) and post title. For analysing comments, we group politics and economics subreddits separately and run the HashingTF and IDF functions on comment body, so no new column was created for comments.\nVisualize text lengths and average number of words over time: Similar to the process for obtaining the most discussed words, we group politics and economics subreddits separately and find the length of the selftext column for submissions and body column for comments. We then create four matplotlib histogram visualizations of the distribution of text lengths corresponding to politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments. Finally, we also create a matplotlib lineplot visualization of the average number of words over time for politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments in the same plot. To obtain average number of words over time, we group politics and economics subreddits separately and find the size after performing a split of the selftext column for submissions and body column for comments. We then group the data on year and month and aggregate each month’s average number of words.\n\n\n\nUsing SageMaker processing jobs and spark nlp, we generated positive, neutral, or negative sentiment indicators. Considering that Reddit is a widely used social media site and forum, we used a pre-trained sentiment detection model typically used for Twitter. For submissions, the post’s title and body were concatenated into a concat_submissions column, which was used for sentiment generation. Similarly, for comments, the post’s body was used for sentiment generation.\nWith the generated sentiment data for submissions and comments from each subreddit, we combined the political subreddits’ data with the U.S. GDP. We then generated plots showing changes in user activity and overall sentiments from each political subreddit of interest alongside changes in the U.S. GDP.\n\n\n\nBy utilizing both the pyspark and gensim libraries, we ensure comprehensive topic modeling analysis, and the interactive visualization powered by the pyLDAvis package provides a clear representation of the prevalent subjects and themes, enabling informed insights into the content across the targeted subreddits. We specifically chose to perform topic modeling on the submissions instead of comments to reveal which topics users want to initiate a discussion on rather than what they go on to discuss and possibly divert from the initial post. Topic modeling, with gensim, for submissions was performed by grouping politics and economics subreddits separately and combining the selftext and title columns to create a new column called nlpbodytext, like we did for Business Goal 8. We then create a gensim dictionary and corpus, and run the LdaModel function to obtain the topics and their corresponding keywords. Finally, we create an interactive visualization of the topics and their corresponding keywords using the pyLDAvis package.\n\n\n\n\n\n\nEach submission (and comment) receives a score derived from the number of up-votes minus the number of down-votes as voted by subreddit readers. These scores drive engagement as more popular posts are pushed to the top by Reddit’s algorithm. Given this, we will create supervised learning models, specifically Linear Regression and Random Forest Regressor, to predict the score of a submission based on provided features, such as whether a submission has adult content and the subreddit the submission belongs to, and engineered features, such as submission title_length, day, month, and year, to name a few. We are not concerned with comments for this business goal since we aim to analyze the initial impact and reception of a post on the platform. A score prediction model could help Reddit users improve their posts by predicting their potential engagement. This would enable authors to edit their posts before publishing to maximize visibility and interaction.\n\n\n\nUnlike submissions, the controversiality feature in comments is determined based on both interactions, specifically the number of up-votes minus the number of down-votes, and the balance of these interactions. We will create supervised learning models, specifically Random Forest Classifier, Logistic Regression, and Gradient Boosting Classifier, to predict the controversiality of a comment based on provided features, the score of the comment, the subreddit the comment belongs to, whether the comment is distinguished (the moderators or admins comment) or gilded (number of times the comment received Reddit gold), and engineered features, including day, month, and year. This analysis aids in understanding the dynamics of comment engagement and controversy, offering Reddit a powerful tool for content moderation and user engagement strategies.\n\n\n\nUsing the Word2Vec function, we create word embeddings of the cleaned_body column, which is the selftext column with stopwords removed and lemmatized. We then create a VectorAssembler to combine the word embeddings with other provided and engineered columns, including day, month, year, distinguished, score, gilded, num_comments, score, over_18, is_spoiler, and is_video columns. We then perform PCA on the combined features and condense the features into 2 principal components, facilitating visualization. Then, using the elbow method to obtain k, the number of clusters, we create a KMeans model and fit the data to obtain the clusters. Better than the elbow method is the silhouette score, which measures how similar an object is to its cluster compared to other clusters. We then use the silhouette score to determine the optimal number of clusters, a hyperparameter-tuning excercise. Finally, we create a KMeans model with the optimal number of clusters and fit the data to obtain the clusters. This analysis is valuable for Reddit and its users as it systematically segments submissions into clusters based on their content. We capture the semantic meaning of submission bodies by leveraging word embeddings created with the Word2Vec function. Including various features, such as temporal information, user distinctions, scores, and content characteristics, enriches the clustering process, offering a nuanced understanding of subreddit categorization.\n\n\n\n\nWe would like to disclose that we employed Grammarly, Inc. to assist with grammar and proofreading for this section."
  },
  {
    "objectID": "ml.html#executive-summary",
    "href": "ml.html#executive-summary",
    "title": "ML",
    "section": "",
    "text": "The ML (Machine Learning) section delves into applying various machine learning models to predict certain aspects of Reddit posts. First, for submissions, we wanted to see if there were certain characteristics that indicated the score that a post would get. In order to do so, we employed regression models, such as Linear Regression and Random Forest Regressor, for determining the score of submissions. Second, we wanted to determine what aspects of reddit users’ comments resulted in it being flagged as controversial. We used classification models, such as Logistic Regression, Gradient Boosting Trees, and Random Forest, to predict the controversiality indicator of comments. Lastly, as an exploratory excercise, we implemented KMeans clustering on the word embeddings of the submissions to gauge whether subreddits can be segmented based on content. These models were chosen for their suitability in addressing three distinct types of problems:\n\nRegression for quantitatively predicting a post’s score, and\nClassification for determining the binary outcome of a post’s controversial nature.\nClustering for understanding whether subreddits submissions can be segmented based on content and other features.\n\nThroughout the analysis, we faced challenges like class imbalances in our features and response. We created multiple models for the regression task, by including and excluding predictors, and found that the Linear Regression model with all predictors performed best. Yet, the best regression model still could explain only 43% of the variability in the response, score. Classification, on the other hand, yielded better results with the highest accuracy, 84%, outputted by the Random Forest Classifier. For Clustering, we had to reduce the dimensionality of the feature set using Principal Component Analysis (PCA), given that the Word2Vec model returned thousands of columns. These results provided valuable insights into the models’ performances and the complex nature of predictive modeling with massive social media data."
  },
  {
    "objectID": "ml.html#analysis-report",
    "href": "ml.html#analysis-report",
    "title": "ML",
    "section": "",
    "text": "One of the questions we tried to answer is how accurately we’d be able to predict a submission’s score based on several features. This task dealt with approximately 600,000 rows of submissions. In Reddit nomenclature, a submission’s score refers to the number of upvotes minus the number of downvotes. To answer this question, we test two models: Linear Regression and Random Forest.\nThe main features being used for this predictive model are the following:\n\nis_video: Whether a submission has a video.\nspoiler: Whether a submission has spoilers.\nover_18: Whether a submission has content over 18.\ntitle_length: Length of title, which was feature engineered from the title column.\nsubreddit: Subreddit to which a post belongs.\n\nOur initial hypothesis was that regression will lead to very poor results, because the response, score is highly skewed (see table and plot below).\n\n\n\n\n\nColumn\n1%\n25%\n50%\n75%\n99%\n\n\n\n\nscore\n0.0\n1.0\n1.0\n29.0\n110685.0\n\n\n\n\nTable 1: Percentile Distribution of score\n\n\n\n \n\n\n\n\nFigure 1: Distribution of score below 75th Percentile\n\n\n\nThe primary factor contributing to the poor performance is inherent to the response variable, score. Firstly, a vast majority of scores are centered around 0 and 1 and significantly lower frequencies relative to the maximum score of more than 110,685. Figure 1 above illustrates the distribution of this variable, highlighting its right-skewed distribution. Despite attempts to address this skewness through log transforms, no substantial improvement was observed, as even the transformed data remained skewed.\n\n\nThe two main models used were Linear Regression and Random Forest Regressor. The resulting \\(R^2\\) values were 0.005 and 0.007 respectively, which reinforced our hypothesis before we ran the models.\nAnother pivotal reason why the models performed poorly is due to the high class imbalance in three categorical features, is_video, is_spoiler, and over_18. Below is an example of the value counts fot is_spoiler (the other two can be found in the Appendix section):\n\n\n\n\n\nis_spoiler\ncount\n\n\n\n\nfalse\n598,852\n\n\ntrue\n1,056\n\n\n\n\nTable 2: Value Counts of is_spoiler\n\n\n\n \n\n\n\nInitially, we didn’t include time-related features (day, month, year) as well as the features num_comments and gilded, but later incorporated them to explore any temporal effects and user as well as moderator induced behavior on the score. Adding these features significantly increased the model’s performance on the test set! The \\(R^2\\) values for the Linear Regression and Random Forest models both improved, indicating that time did play a significant role in determining the score of a submission. The test set \\(R^2\\) values for the Linear Regression and Random Forest models were 0.429 and 0.386 and the test set RMSE values were 402.07 and 417.101 respectively.\nThe Random Forest model performed slightly worse than the Linear Regression model, which was surprising given that Random Forests are generally more robust to outliers and noise. These metrics are shown in the tables below:\n\n\n\n\n\nEvaluation Metric\nValue\n\n\n\n\n\\(R^2\\)\n0.429\n\n\nRMSE\n402.07\n\n\n\n\nTable 3: Linear Regression Test Set Model Metrics\n\n\n\n \n\n\n\n\n\nEvaluation Metric\nValue\n\n\n\n\n\\(R^2\\)\n0.386\n\n\nRMSE\n417.101\n\n\n\n\nTable 4: Random Forest Regressor Test Set Model Metrics\n\n\n\nIn fact, through the use of the featureImportances attribute of the Random Forest model, we were able to determine that the most important features were gilded, day, title_length, subreddit_vec, and year. This is shown in the plot below:\n\n\n\n\nFigure 2: Random Forest Feature Importance\n\n\n\nWe also plotted the residuals of both regression models to see if there were any patterns in the errors. The plots below shows that a decent amount of the errors are centered around 0, which is a good sign. However, there are some outliers that are worth investigating further. Specifically, from Linear Regression’s residual plot, we see that its predicted values are much higher than the predicted values of the Random Forest model. This is likely due to the fact that Linear Regression is more sensitive to outliers than Random Forests. However, this could also be the reason why Linear Regression performed better than Random Forests because it was able to capture the extreme values of the response variable.\n\n\n\n\nFigure 3: Linear Regression Test Set Residuals\n\n\n\n\n\n\n\nFigure 4: Random Forest Test Set Residuals\n\n\n\nTo improve the performance of Random Forest Regressor, we could conduct grid search hyper-parameter tuning. We did try increasing numTrees from 30 to 100 and found that the \\(R^2\\) value increased to 0.39, by 1%. However, this is not a significant improvement and we believe that the Random Forest Regressor is overfitting the data.\n\n\n\n\nAlongside submissions analysis, we wanted to look into the truly interactive element of Reddit – the comments section. Much like the scores on submissions, comments also have a score that refers to the number of upvotes minus the number of downvotes. However, the controverisality marker is a binary indicator regarding these upvotes and downvotes. According to this Reddit post, controversiality is determined based on both interactions and the balance of these interactions. In other words, a comment needs to have sufficiently large number of upvotes and downvotes, as well as having a balance of upvotes and downvotes (which would result in a score close to 0).\nTo prepare the comments for analysis, further pre-processing was conducted, such as excluding comments that have been removed or deleted and separating our the created_utc column into individual years, months, and days. Most importantly, since controversiality is a severly imbalanced target variable (approximately 93% non-controversial and 7% controversial), we added a weight column for controversiality to balance class weights and ensure our models pay more attention to controversial comments by up-weighting its importance during training.\nWe created a Pipeline with the following steps:\n\nStringIndexer(): Converts the columns controversiality, distinguished, and subreddit into numerical representations.\nVectorAssembler(): Combines the feature columns distinguished_ix, subreddit_ix, year, month, day, score, and gilded into a single features column.\npipeline_model: Represents the ML model being used in the pipeline. Changes based on the argument passed into the Spark Processing Job. Possible options are Random Forests, Logistic Regression, and Gradient Boosted Trees.\nIndexToString(): Converts the numeric predictions from the model back to the original binary indicators of controversiality.\n\n \n\n\nThe three models fit were Random Forest, Logistic Regression, and Gradient Boosting. The confusion matrices for each of the models are in the appendix section below, but here is a summary of the findings:\n\nRandom Forest: The Random Forest model performed the best out of the three models. It had the highest accuracy, precision, recall, and F1-score. However, it has the lowest number of True Positives, which is not ideal given that we want to be able to predict controversial comments, and comprises on performance by classifying the highest number of True Negatives, cases where the actual label is non-controversial and the model predicts that, and classifying the fewest False Negatives, cases where the actual label is controversial and the model fails to predicts that, than the other algorithms.\nLogistic Regression: The Logistic Regression model performed the worst out of the three models. It had the lowest accuracy, precision, recall, and F1-score. However, opposite to Random Forest, it has the highest number of True Positives, which is ideal given that we want to be able to predict controversial comments. Yet, it classifies the highest number of False Negatives, cases where the actual label is controversial and the model fails to predicts that, than the other algorithms.\nGradient Boosted Trees: The Gradient Boosted Trees model performed in the middle out of the three models. It had the second highest accuracy, precision, recall, and F1-score. It has the second highest number of True Positives, which is ideal given that we want to be able to predict controversial comments. Yet, it classifies the second highest number of False Negatives, cases where the actual label is controversial and the model fails to predicts that, than the other algorithms.\n\nThe AUC-ROC curve of each of the models can be found below:\n\n\n\n\n\n\nFigure 5: Test Set AUC-ROC Curves\n\n\n\nGiven that the dataset is heavily imbalanced, we believe the AUC-ROC plot is a better measure of a model’s ability to distinguish between controversial and non-controversial comments. When looking solely at the figure above, Gradient Boosted Trees performs best (albeit marginally), followed by Random Forests, and lastly Logistic Regression.\nWhen accounting for the evaluation metrics, confusion matrices, and the AUC-ROC plot, Random Forests seems to perform the best overall, followed by Gradient Boosted Trees, and lastly Logistic Regression.\n\n\n\n\nThe final task we wanted to tackle was to determine what subreddit a submission belongs to based on the submission body and other features. This task also dealt with approximately 600,000 rows of submissions. In order to do so, we employed clustering models, such as K-Means, for determining the subreddit of a submission. The features used for this unsupervised model are the following: day, month, year, distinguished, score, gilded, num_comments, score, over_18, is_spoiler, and is_video. Apart from these features, we also used the word embeddings of the submission body, which were generated using the Word2Vec algorithm. Once all the features were combined, we used VectorAssembler to combine them into a single features column, scaled the features using StandardScaler, and then reduced the dimensionality of the features using PCA to obtain two principal components.\nThe figure below showcases the elbow method, which was one of the ways to determine the optimal number of clusters.\n\n\n\n\nFigure 6: Elbow method\n\n\n\nThe optimal number of clusters was determined to be four based on distortion and six based on inertia. Therefore, we decided to go with five clusters as a compromise between the two methods:\n\n\n\n\nFigure 7: K-Means Clustering with 5 Clusters\n\n\n\nNot satisfied with the elbow method, we also decided to use the silhouette score to determine the optimal number of clusters. The silhouette score is a measure of how similar an object is to its own cluster compared to other clusters. The score ranges from -1 to 1, where a score of 1 indicates that the object is far away from its neighboring cluster and very close to the cluster it is assigned to. The higher the score for a cluster, the more distinct and well-separated the clusters are. In contrast, a score close to -1 suggests overlapping clusters, indicating that the object could be assigned to a neighboring cluster. The figure below shows the silhouette score for different number of clusters:\n\n\n\n\nFigure 8: Silhouette Score vs Number of Clusters\n\n\n\nWe see that the silhouette score is highest for four clusters, which is in line with the elbow method. In fact, the highest silhouette score is 0.91, corresponding to two clusters. This reconciles well with the types of subreddits we are analyzing, which are either political or economics related. The silhouette plot below coupled with the scatter plot on the right shows the clusters for nine clusters:\n\n\n\n\nFigure 9: Silhouette Analysis (9 clusters)\n\n\n\nSilhouette plots look to have an edge over the elbow method as one can evaluate clusters on multiple criteria, including scores below average Silhouette score (red vertical line), wide fluctuations in the size of the plot, and non-uniform thickness. Therefore, it is highly likely that one can end up determining the most optimal number of clusters in K-means using the above plots.\nMost clusters have scores above the average Silhouette score (red vertical line) and the thickness of the silhouette plots suggests that the cluster sizes are highly different or non-uniform. Our aim is to choose those n_clusters that correspond to uniform thickness of the clusters’ Silhouette plot. In the appendix below, for n_clusters=4, the silhouette score is lower than that of n_clusters=9, but the thickness of the silhouette plots is still non-uniform. Unfortunately, we have no n_clusters value that showcases that all the clusters are more or less of similar thickness and, hence, are of similar sizes as can also be verified from the labelled scatter plot on the right. Therefore, we decided to go with two clusters, given that it has the highest silhouette score and the clusters are of similar thickness, as can be seen from the silhouette plot below:\n\n\n\n\nFigure 10: Silhouette Analysis (2 clusters)\n\n\n\nIn conclusion, given the complexity of the data, we believe that the K-Means clustering algorithm did not perform well. The sheer number of overlap in principal components between the political and economics subreddits made it difficult for the algorithm to cluster the data. In addition, the fact that the data is highly imbalanced also made it difficult for the algorithm to cluster the data. In the future, we would like to explore other clustering algorithms, such as DBSCAN, to see if we can get better results."
  },
  {
    "objectID": "business_goals.html#exploratory-data-analysis-eda-goals",
    "href": "business_goals.html#exploratory-data-analysis-eda-goals",
    "title": "Business Goals",
    "section": "",
    "text": "We start by visualizing each of the nine subreddits’ posts by grouping data on the subreddit and posted date (year and month features engineered from created_utc) and then aggregating the counts of posts over time. Doing so allows us to gauge periods of intense activity in specific subreddits, which could be attributed to significant political and economic events. We use the same grouped data but now aggregate on the author column to get distinct users over time for each subreddit to obtain an idea of unique user participation and differences in user diversity and engagement. We then repeat the same process for comments.\n\n\n\nUse data grouped on subreddit to gather the normalized counts of posts and comments that are gilded (the number of times a post/comment received Reddit gold), distinguished (whether the moderators or admins make the post/comment), and controversial (whether a post/comment received a large number of both upvotes and downvotes). The higher the counts of these variables, the more likely a reddit user will come across and interact with those posts or comments. The same data wrangling is done next but with author added as grouping variable. The counts of users posting or commenting across multiple subreddits are calculated, and a table with the top 50 posts (based on upvotes) from each subreddit is presented. Doing so allows us to identify the most active users across subreddits.\n\n\n\nEach submission and comment receives a score, which is calculated as the number of upvotes minus the number of downvotes as voted by the respective subreddit users. We use the data grouped by subreddit to gather the top 50 posts with the highest scores (based on upvotes and downvotes) from each subreddit. After obtaining the data, we create a HighChartJS Packed Bubble chart, where the size of each bubble corresponds to the score of individual posts, and the size of the larger bubbles represents the cumulative scores of posts from each subreddit. This vizualization allows us to identify which subreddits’ content resonates the most with its users.\n\n\n\nUse data grouped on subreddit, month, and day of the week and then aggregate the counts of posts. Create a HighChartJS Heatmap chart for each subreddit, where a greater color intensity corresponds with more posts. Doing so helps us identify distinct seasonal patterns to understand when users are most active in each subreddit.\n\n\n\nUse datasets that cover all pairwise combinations of political subreddits, joined based on common authors to gather the counts of authors posting in multiple subreddits. Create a HighChartJS Dependency Wheel chart based on the number of authors for each pairwise subreddit combination. The chart provides insights into which subreddits have the most shared users, which could be attributed to similar content or user demographics.\n\n\n\nWe first group politics and economics subreddit submissions and filter subreddits for post body (selftext) and for post title (two-part analysis). Then, using regex, we identify posts that mention keywords pertaining to politics and economics and gather the counts for each grouped subreddit that contains the keywords mentioned above. Next, we independently calculate the cosine similarity and Jaccard similarity measures for the counts for both post titles and post bodies, resulting in two tables (one for post body and another for post title). A high cosine similarity implies that the two documents have similar distributions of words, even though the counts of individual words may be different. To correctly measure the similarity between the counts of words in the two documents, we use another similarity metric, the Jaccard index. By selecting keywords that are most similar across the two subreddits, we can identify context similarities between the two subreddits and the distribution of the keywords across the two subreddits.\n\n\n\nCombine data on the number of posts and comment scores (derived from upvotes and downvotes) with external GDP data gathered from the FRED python package. Perform a time window shift of GDP data (since it’s lagged by three months). Visualize the subreddit post activity with the GDP data with HighChartJS. With the help of this time-series visualization, we can identify patterns in GDP growth and its effects on the activity of the r/Economics subreddit."
  },
  {
    "objectID": "business_goals.html#natural-language-nlp-processing-goals",
    "href": "business_goals.html#natural-language-nlp-processing-goals",
    "title": "Business Goals",
    "section": "",
    "text": "Uncover most discussed words: We create four tables of select top 100 common words corresponding to politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments. For analysing submissions, we group politics and economics subreddits separately and count common words on the feature engineered column of combined post body (selftext) and post title. We also use regex to replace certain stopwords, such as “https”, “www”, and “jpg”, that the NLP pipeline could not remove. Once the new feature is obtained, it is turned into an array of words, flattened, grouped by word to get the counts, and sorted and filtered to output a .csv file of the top 100 words. For analysing comments, we group politics and economics subreddits separately and count common words in the comment body, so no new column was created for comments. We also use the aforementioned regex to replace certain stopwords and utilize the same process to obtain the counts of common words.\nUncover most important words: We create three tables of the top 10 most important words, measured by TF-IDF score, corresponding to politics subreddits submissions, economics subreddits submissions, and politics subreddits comments (economics subreddits comments are left out due to allocation of compute capacity). Similar to the process for obtaining the most discussed words, we group politics and economics subreddits separately and run the HashingTF and IDF functions on the feature engineered column of combined post body (selftext) and post title. For analysing comments, we group politics and economics subreddits separately and run the HashingTF and IDF functions on comment body, so no new column was created for comments.\nVisualize text lengths as well as average number of words over time: Similar to the process for obtaining the most discussed words, we group politics and economics subreddits separately and find the length of the selftext column for submissions and body column for comments. We then create four matplotlib histogram visualizations of the distribution of text lengths corresponding to politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments. Finally, we also create a matplotlib lineplot visualization of the average number of words over time for politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments in the same plot. To obtain average number of words over time, we group politics and economics subreddits separately and find the size after performing a split of the selftext column for submissions and body column for comments. We then group the data on year and month and aggregate the average number of words for each month.\n\n\n\nImplement sentiment analysis using NLP to determine the emotional context of posts and comments in each subreddit.\n\n\n\nBy utilizing both the pyspark and gensim libraries, we ensure comprehensive topic modeling analysis, and the interactive visualization powered by the pyLDAvis package provides a clear representation of the prevalent subjects and themes, enabling informed insights into the content across the targeted subreddits. We specifically chose to perform topic modeling on the submissions instead of comments to reveal which topics users want to initiate a discussion on, rather than what they go on to discuss and possibly divert from the initial post. Topic modeling, with gensim, for submissions was performed by grouping politics and economics subreddits separately and combining the selftext and title columns to create a new column called nlpbodytext, like we did for Business Goal 8. We then create a gensim dictionary and corpus, and run the LdaModel function to obtain the topics and their corresponding keywords. Finally, we create an interactive visualization of the topics and their corresponding keywords using the pyLDAvis package."
  },
  {
    "objectID": "business_goals.html#machine-learning-ml-goals",
    "href": "business_goals.html#machine-learning-ml-goals",
    "title": "Business Goals",
    "section": "",
    "text": "Each submission (and comment) receives a score derived from the number of up-votes minus the number of down-votes as voted by subreddit readers. These scores drive engagement as more popular posts are pushed to the top by Reddit’s algorithm. Given this, we will create supervised learning models, specifically Linear Regression and Random Forest Regressor, to predict the score of a submission based on provided features, such as whether a submission has adult content and the subreddit the submission belongs to, and engineered features, such as submission title_length, day, month, and year, to name a few. We are not concerned with comments for this business goal since we aim to analyze the initial impact and reception of a post on the platform. A score prediction model could help Reddit users improve their posts by predicting their potential engagement. This would enable authors to edit their posts before publishing to maximize visibility and interaction.\n\n\n\nUnlike submissions, the controversiality feature in comments is determined based on both interactions, specifically the number of up-votes minus the number of down-votes, and the balance of these interactions. We will create supervised learning models, specifically Random Forest Classifier, Logistic Regression, and Gradient Boosting Classifier, to predict the controversiality of a comment based on provided features, the score of the comment, the subreddit the comment belongs to, whether the comment is distinguished (the moderators or admins comment) or gilded (number of times the comment received Reddit gold), and engineered features, including day, month, and year. This analysis aids in understanding the dynamics of comment engagement and controversy, offering Reddit a powerful tool for content moderation and user engagement strategies.\n\n\n\nUsing the Word2Vec function, we create word embeddings of the cleaned_body column, which is the selftext column with stopwords removed and lemmatized. We then create a VectorAssembler to combine the word embeddings with other provided and engineered columns, including day, month, year, distinguished, score, gilded, num_comments, score, over_18, is_spoiler, and is_video columns. We then perform PCA on the combined features and condense the features into 2 principal components, facilitating visualization. Then, using the elbow method to obtain k, the number of clusters, we create a KMeans model and fit the data to obtain the clusters. Better than the elbow method is the silhouette score, which measures how similar an object is to its cluster compared to other clusters. We then use the silhouette score to determine the optimal number of clusters, a hyperparameter-tuning excercise. Finally, we create a KMeans model with the optimal number of clusters and fit the data to obtain the clusters. This analysis is valuable for Reddit and its users as it systematically segments submissions into clusters based on their content. We capture the semantic meaning of submission bodies by leveraging word embeddings created with the Word2Vec function. Including various features, such as temporal information, user distinctions, scores, and content characteristics, enriches the clustering process, offering a nuanced understanding of subreddit categorization."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "In an era shaped by digital discourse and diverse perspectives, where clashing opinions abound, the need for understanding online communities and their interactions is paramount. This project embarks on a comprehensive analysis of Reddit’s political and economic space. Our analysis will be further augmented by incorporating external data from the Federal Reserve Economic Data (FRED) to unravel multifaceted insights across socio-political, economic, and sentiment-driven dimensions. In doing so, we endeavor to offer a holistic perspective on the unfolding narratives of online political discourse.\nAt the heart of this exploration lies an intricate tapestry of ideological diversity represented through Reddit’s various subreddits. The nine chosen subreddits, seven related to politics and two related to finance/economics, cover the four quadrants and the center of the political compass, thereby encapsulating a broad spectrum of beliefs. These political subreddits serve as hubs for ideological exchange and mirror the heterogeneous landscapes of societal viewpoints growing online. Examining these subreddits goes beyond the mere enumeration of user activities; it seeks to decipher the nuanced interplay between the online public discourse and the socio-economic climate, reflecting the dynamism and intricacies of contemporary societal discourses.\n\n\n\n\nFigure 1: Political Compass\n\n\n\nBy scrutinizing these subreddits through various analytical lenses, encompassing activity metrics, sentiment analysis, topic modeling, and alignment with external economic indicators, this project seeks to uncover patterns, trends, and correlations. The fusion of Reddit’s user-generated content with data from external economic sources enables us to discern the pulse of online ideological discussions and their connections and implications within broader socio-economic contexts. Through this, we aim to offer insights into the complex interactions that characterize Reddit’s political and economic spheres, shedding light on the evolving nature of digital discourse and its reflections on contemporary societal dynamics.\nA blend of robust Cloud Computing technologies and platforms aided our study. Given the scale of Reddit data, the backbone of our analysis relied on Amazon Web Services (AWS) and Microsoft Azure ML, harnessing the power of Spark for efficient data processing and analysis. Leveraging the capabilities of SparkML and SparkNLP, we executed various tasks encompassing natural language processing and machine learning. We employed a combination of models to achieve comprehensive insights, including pre-trained models sourced from JohnSnowLabs and custom models trained on our data.\n\n\n\n\n\nSubreddit\nSubmissions\nComments\n\n\n\n\nr/Ask_politics\n5,903\n60,149\n\n\nr/changemyview\n64,632\n3,909,587\n\n\nr/finance\n28,904\n137,118\n\n\nr/Economics\n40,604\n1,428,423\n\n\nr/Conservative\n343,938\n5,231,661\n\n\nr/socialism\n40,094\n371,369\n\n\nr/Liberal\n11,086\n96,396\n\n\nr/Libertarian\n51,153\n2,706,903\n\n\nr/centrist\n13,594\n92,1871\n\n\n\n\nTable 1: Number of Comments and Submissions on Select Subreddits from January 2021 to March 2023.\n\n\n\n\n\n\n\n\n\nRaunak Advani\n\n\n\n\n\nTegveer Ghura\n\n\n\n\n\nEric Lim\n\n\n\n\n\nAnthony Moubarak\n\n\n\n\n\nWe would like to disclose that we employed Grammarly, Inc. to assist with grammar and proofreading for this section."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Introduction",
    "section": "",
    "text": "In an era shaped by digital discourse and diverse perspectives, where clashing opinions abound, the need for understanding online communities and their interactions is paramount. This project embarks on a comprehensive analysis of Reddit’s political and economic space. Our analysis will be further augmented by incorporating external data from the Federal Reserve Economic Data (FRED) to unravel multifaceted insights across socio-political, economic, and sentiment-driven dimensions. In doing so, we endeavor to offer a holistic perspective on the unfolding narratives of online political discourse.\nAt the heart of this exploration lies an intricate tapestry of ideological diversity represented through Reddit’s various subreddits. The nine chosen subreddits, seven related to politics and two related to finance/economics, cover the four quadrants and the center of the political compass, thereby encapsulating a broad spectrum of beliefs. These political subreddits serve as hubs for ideological exchange and mirror the heterogeneous landscapes of societal viewpoints growing online. Examining these subreddits goes beyond the mere enumeration of user activities; it seeks to decipher the nuanced interplay between the online public discourse and the socio-economic climate, reflecting the dynamism and intricacies of contemporary societal discourses.\n\n\n\n\nFigure 1: Political Compass\n\n\n\nBy scrutinizing these subreddits through various analytical lenses, encompassing activity metrics, sentiment analysis, topic modeling, and alignment with external economic indicators, this project seeks to uncover patterns, trends, and correlations. The fusion of Reddit’s user-generated content with data from external economic sources enables us to discern the pulse of online ideological discussions and their connections and implications within broader socio-economic contexts. Through this, we aim to offer insights into the complex interactions that characterize Reddit’s political and economic spheres, shedding light on the evolving nature of digital discourse and its reflections on contemporary societal dynamics.\nA blend of robust Cloud Computing technologies and platforms aided our study. Given the scale of Reddit data, the backbone of our analysis relied on Amazon Web Services (AWS) and Microsoft Azure ML, harnessing the power of Spark for efficient data processing and analysis. Leveraging the capabilities of SparkML and SparkNLP, we executed various tasks encompassing natural language processing and machine learning. We employed a combination of models to achieve comprehensive insights, including pre-trained models sourced from JohnSnowLabs and custom models trained on our data.\n\n\n\n\n\nSubreddit\nSubmissions\nComments\n\n\n\n\nr/Ask_politics\n5,903\n60,149\n\n\nr/changemyview\n64,632\n3,909,587\n\n\nr/finance\n28,904\n137,118\n\n\nr/Economics\n40,604\n1,428,423\n\n\nr/Conservative\n343,938\n5,231,661\n\n\nr/socialism\n40,094\n371,369\n\n\nr/Liberal\n11,086\n96,396\n\n\nr/Libertarian\n51,153\n2,706,903\n\n\nr/centrist\n13,594\n92,1871\n\n\n\n\nTable 1: Number of Comments and Submissions on Select Subreddits from January 2021 to March 2023."
  },
  {
    "objectID": "discussion.html",
    "href": "discussion.html",
    "title": "Discussion",
    "section": "",
    "text": "No feedback was provided.\n\n\n\n\nData quality checks are not explicitly on the website.\n\nAs a part of the EDA section, our group necessarily performed data cleaning and sanity checks. These were local to our codebase. However, for the sake of transparency, the data quality checks that we performed have been added under the Code/Preprocessing section.\n\nBusiness goals are vague and not directly mapped.\n\nThis feedback spills over into the NLP and ML sections as well. As such, we’ve reworded the business goals and technical summaries to be less vague. Furthermore, we’ve directly mapped our analyses in all of our sections to the aforementioned business goals.\n\nMany plots are repetitive.\n\nThis feedback spills over into the NLP and ML sections as well. Because we deal with multiple political and economic subreddits, having multiple similar-looking plots was an inevitability. However, for the sake of narrative flow and clarity, we’ve created appendices in each section for references to repetitive plots.\n\nExpand shared user base analysis to LDA Topic Modeling using either cosine similarity or LDA Topic Modeling to further explore the business goal on common authors in political subreddits.\n\nThis was an interesting avenue to further expand one of our business goals. We, therefore, implemented an LDA model on a filtered dataset of common users in political subreddits to identify whether users who comment on multiple subreddits also tend to engage on similar topics. Selecting five as the desired number of topics, we found that users do engage in other, unique topics across subreddits, with the topics being mainly political and very few economy related topics. The visualization and table below summarizes the topics that users engage in across subreddits:\n\n\n\n\n\nFigure 1: Interactive Visualization of Submissions Topics Across Shared User Subreddits\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nTopic Words\nSummary\n\n\n\n\n1\npeople, think, want, good, right, cmv (change my view), life, use, change, believe\nExploring perspectives and beliefs in political discourse to understand and potentially alter their own opinions. This topic is also seen in the LDA visualization for politics submissions.\n\n\n2\nwiden (Biden), Trump, cmv (change my view), democrat, republican, gun, school vote, party, bill, crime\nDiscussions related to political parties and the broader national context. This topic is also seen in the LDA visualization for politics submissions.\n\n\n3\nright, white, support, war, system, capitol, society, kill, russia\nDiscussions related to right-wing politics, Jan. 6 Capitol attack, and Russia-Ukraine war. This is a unique topic from previously analyzed economics and politics submissions!\n\n\n4\ncovid, law, police, ban, news, court, liberal, vaccine, joe, china, abortion, supreme, inflation, rule, freedom\nDiscussions related to left-wing politics, such as the Supreme Court overturning Roe v. Wade, COVID-19, and police brutality. Another unique topic!\n\n\n5\nnew, conservative, libertarian, socialism, twitter, death, mask, racism, drug, economy, property, musk\nA mix of different political subreddits discussing multifarious societal issues, such as social media and twitter, racism, drugs, and the economy. Another unique topic!\n\n\n\n\nTable 1: Summary of Submissions Topics Across Shared User Subreddits\n\n\n\n\n\n\nNo feedback was provided.\n\n\n\n\nModel performances are a concern, especially with the volatility of user interactions online.\n\nUnfortunately, our models definitely were underwhelming. However, this seems like an inevitability given the distributions of our feature variables and the skewedness of many of our target variables.\n\n\n\n\nDefault website formatting and styling are too mundane.\n\nThe given website starter template was useful but wasn’t very aesthetic or well-organized as the project progressed. As such, we’ve added the sidebar with splits per section, and subsections for each large section if necessary. Additionally, we’ve customized the color theme, font, and other minor aesthetic features."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Conclusion\nOur robust exploration of subreddits gave us a holistic understanding of the nuances of analysing and interpreting big data, particularly in the context of politics, finance, and economics. Through exploratory data analysis (EDA), natural language processing (NLP), and machine learning (ML), we gained insights into the trends, sentiments, and important features related to our data.\nThe exploratory data analysis (EDA) of Reddit’s political and economic subreddits offered a comprehensive view of online political and economic discourse, significantly enhancing our understanding of user behavior and the dynamics within these online communities. Our preliminary findings indicate that r/Conservative is the most active political subreddit, with a high number of posts and comments, and a strong resonance with its audience, as evidenced by its high average post score. In contrast, r/AskPolitics stands out for its diversity, suggesting a broader user base. This analysis also uncovered interesting patterns, such as Thursdays being the most active day for posting across various subreddits, and a general decrease in posting activity as the year progresses, aligning with common online engagement trends.\nIn delving deeper into the content, we found that post titles are more indicative of subreddit themes than post bodies, which are often links to external articles. Additionally, incorporating U.S. GDP data revealed a correlation between economic conditions and engagement levels on r/Economics. During economic downturns, there was a noticeable increase in subreddit activity, with a corresponding decrease in average post scores, indicating that users turn to these online communities for discussion and information in times of economic uncertainty. This EDA thus provides valuable insights into the patterns and preferences of Reddit users, particularly in the realms of politics and economics, offering a lens into how online discourse mirrors and reacts to real-world events.\nNext, looking at our Natural Language Processing (NLP) section, we focused on analyzing trends in submission posts and comments across nine Reddit subreddits. Utilizing the johnsnowlabs sparkNLP library, this section delved into sentiment analysis, assigning each submission and comment a sentiment category: positive, negative, or neutral. This approach unveiled significant overlaps in key words across both political and economic subreddits, indicating shared themes and concerns among these communities.\nA critical aspect of the NLP analysis was the exploration of sentiment trends within these subreddits. Interestingly, subreddits like r/Conservative, r/Libertarian, and r/centrist demonstrated a predominance of positive sentiments in comments, whereas r/Economics and r/Liberal leaned towards more negative sentiments. This polarity was also evident in submissions, with r/AskPolitics and r/Conservative featuring higher negative sentiments, while r/Economics and r/Liberal showed more positivity. These findings suggest a complex landscape of online discourse, where positivity prevails, but negativity and polarization are still significant. The scant presence of neutral sentiments implies that Reddit users in these communities generally take definitive stances on issues rather than remaining neutral.\nFurthermore, the integration of external data from the FRED python package enriched the analysis, particularly in the r/Economics subreddit. The frequency of terms like “recession” spiked in tandem with major economic events, indicating the subreddit’s responsiveness to real-world economic shifts. This highlights how online discussions are not isolated but are deeply intertwined with global events, reflecting and reacting to them.\n\n\n\n\n\nFigure 1: Number of r/Economics Submissions vs GDP\n\n\n\nThe Machine Learning (ML) analysis employed regression, classification, and clustering to predict submission scores, comment controversiality, and segment submissions based on content, respectively. The regression analysis revealed that the score of a submission is mostly explained by the day it was posted, the length of its title, the subreddit it belonged to, and whether it was gilded (number of times it received Reddit gold). The classification analysis unveiled that although only 7% of comments are tagged as controversial, Logistic Regression predicted more of them correctly than the ensemble models, but at the expense of misclassifying more non-controversial comments as controversial. The clustering analysis revealed that submissions content of all nine subreddits has considerable overlap, making it difficult to segment them based on content and other features.\n\n\n\n\n\nFigure 2: Test Set AUC-ROC Curves\n\n\n\n\n\nNext steps\nOur project has several limitations, related to our data and analysis, that can be addressed in future work. First, our project is catered to the U.S audience and we treat our findings to be native to the U.S; however, Reddit is open to the world, so there will be a few comments and submissions that were posted outside the U.S. Although this did not hamper our work, we do admit some bias may be present in our findings. To treat this issue, we could broaden the project to provide a more global perspective on online discourse by expanding the analysis to include subreddits from other countries.\nWe could also focus on more specific and significant topics, such as the COVID-19 pandemic, which has had a profound impact on the economy and politics. This would allow us to explore how online discourse has evolved in response to this global crisis. Additionally, in terms of specific implementations in the project, given greater computational power, training a sentiment model ourself specific to our given dataset may prove to give more insightful results, rather than relying on a pretrained Twitter model. Furthermore, for both regression and classification, it may be worth creating stacked models and conducting hyperparameter tuning to hopefully yield better results.\n\n\nAppendix\nThis section expands on the updates we made to our analysis since the intermediate deliverables. Specifically, we added a new section on Machine Learning (ML) to include clustering analysis of submissions, and we incorporated professor Amit’s feedback on diving deeper in our shared user interaction analysis, as showcased in the EDA section.\nAlthough the ML section was complete with the submission of the intermediate deliverable, we were still looking forward to adding a clustering analysis of submissions, given some overlapping topics from the LDA Topic Modeling section. It also gave us an opportunity to display unique visualizations, including the Principal Components by Subreddit, the Silhouette Score by Number of Clusters, and the performance of KMeans, all of which augmented our understanding of subreddit content.\nProfessor Amit’s feedback on our EDA section was also very helpful, as it encouraged us to dive deeper into our analysis of shared users across political subreddits. We were able to add a new LDA Topic Modeling visualization and a table summarizing the topics. This is further discussed and showcased in the Discussion section"
  },
  {
    "objectID": "eda.html#appendix",
    "href": "eda.html#appendix",
    "title": "EDA",
    "section": "",
    "text": "Appendix 1: Line Chart of number of posts by distinct number of users (per subreddit)\n\n\n\n\n\n\n\n\n\nAppendix 2: Line Chart of number of comments by distinct number of users (per subreddit)\n\n\n\n\n\n\n\n\n\n\nAppendix 3: Calendar Heatmap of r/Liberterian Reddit Posts\n\n\n\n\n\n\n\n\n\nAppendix 4: Calendar Heatmap of r/centrist Reddit Posts\n\n\n\n\n\n\n\n\n\nAppendix 5: Calendar Heatmap of r/socialism Reddit Posts"
  },
  {
    "objectID": "business_goals.html#natural-language-processing-nlp-goals",
    "href": "business_goals.html#natural-language-processing-nlp-goals",
    "title": "Business Goals",
    "section": "",
    "text": "Uncover most discussed words: We create four tables of select top 100 common words corresponding to politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments. For analyzing submissions, we group politics and economics subreddits separately and count common words on the feature-engineered column of combined post body (selftext) and post title. We also use regex to replace certain stopwords, such as “https”, “www”, and “jpg”, that the NLP pipeline could not remove. Once the new feature is obtained, it is turned into an array of words, flattened, grouped by word to get the counts, and sorted and filtered to output a .csv file of the top 100 words. For analyzing comments, we group politics and economics subreddits separately and count common words in the comment body, so no new column was created for comments. We also use the regex mentioned above to replace certain stopwords and utilize the same process to obtain the counts of common words.\nUncover most important words: We create three tables of the top 10 most important words, measured by TF-IDF score, corresponding to politics subreddits submissions, economics subreddits submissions, and politics subreddits comments (economics subreddits comments are left out due to allocation of compute capacity). Similar to the process for obtaining the most discussed words, we group politics and economics subreddits separately and run the HashingTF and IDF functions on the feature engineered column of combined post body (selftext) and post title. For analysing comments, we group politics and economics subreddits separately and run the HashingTF and IDF functions on comment body, so no new column was created for comments.\nVisualize text lengths and average number of words over time: Similar to the process for obtaining the most discussed words, we group politics and economics subreddits separately and find the length of the selftext column for submissions and body column for comments. We then create four matplotlib histogram visualizations of the distribution of text lengths corresponding to politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments. Finally, we also create a matplotlib lineplot visualization of the average number of words over time for politics subreddits submissions, economics subreddits submissions, politics subreddits comments, and economics subreddits comments in the same plot. To obtain average number of words over time, we group politics and economics subreddits separately and find the size after performing a split of the selftext column for submissions and body column for comments. We then group the data on year and month and aggregate each month’s average number of words.\n\n\n\nUsing SageMaker processing jobs and spark nlp, we generated positive, neutral, or negative sentiment indicators. Considering that Reddit is a widely used social media site and forum, we used a pre-trained sentiment detection model typically used for Twitter. For submissions, the post’s title and body were concatenated into a concat_submissions column, which was used for sentiment generation. Similarly, for comments, the post’s body was used for sentiment generation.\nWith the generated sentiment data for submissions and comments from each subreddit, we combined the political subreddits’ data with the U.S. GDP. We then generated plots showing changes in user activity and overall sentiments from each political subreddit of interest alongside changes in the U.S. GDP.\n\n\n\nBy utilizing both the pyspark and gensim libraries, we ensure comprehensive topic modeling analysis, and the interactive visualization powered by the pyLDAvis package provides a clear representation of the prevalent subjects and themes, enabling informed insights into the content across the targeted subreddits. We specifically chose to perform topic modeling on the submissions instead of comments to reveal which topics users want to initiate a discussion on rather than what they go on to discuss and possibly divert from the initial post. Topic modeling, with gensim, for submissions was performed by grouping politics and economics subreddits separately and combining the selftext and title columns to create a new column called nlpbodytext, like we did for Business Goal 8. We then create a gensim dictionary and corpus, and run the LdaModel function to obtain the topics and their corresponding keywords. Finally, we create an interactive visualization of the topics and their corresponding keywords using the pyLDAvis package."
  },
  {
    "objectID": "discussion.html#project-plans",
    "href": "discussion.html#project-plans",
    "title": "Discussion",
    "section": "",
    "text": "No feedback was provided."
  },
  {
    "objectID": "discussion.html#eda",
    "href": "discussion.html#eda",
    "title": "Discussion",
    "section": "",
    "text": "Data quality checks are not explicitly on the website.\n\nAs a part of the EDA section, our group necessarily performed data cleaning and sanity checks. These were local to our codebase. However, for the sake of transparency, the data quality checks that we performed have been added under the Code/Preprocessing section.\n\nBusiness goals are vague and not directly mapped.\n\nThis feedback spills over into the NLP and ML sections as well. As such, we’ve reworded the business goals and technical summaries to be less vague. Furthermore, we’ve directly mapped our analyses in all of our sections to the aforementioned business goals.\n\nMany plots are repetitive.\n\nThis feedback spills over into the NLP and ML sections as well. Because we deal with multiple political and economic subreddits, having multiple similar-looking plots was an inevitability. However, for the sake of narrative flow and clarity, we’ve created appendices in each section for references to repetitive plots.\n\nExpand shared user base analysis to LDA Topic Modeling using either cosine similarity or LDA Topic Modeling to further explore the business goal on common authors in political subreddits.\n\nThis was an interesting avenue to further expand one of our business goals. We, therefore, implemented an LDA model on a filtered dataset of common users in political subreddits to identify whether users who comment on multiple subreddits also tend to engage on similar topics. Selecting five as the desired number of topics, we found that users do engage in other, unique topics across subreddits, with the topics being mainly political and very few economy related topics. The visualization and table below summarizes the topics that users engage in across subreddits:\n\n\n\n\n\nFigure 1: Interactive Visualization of Submissions Topics Across Shared User Subreddits\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic\nTopic Words\nSummary\n\n\n\n\n1\npeople, think, want, good, right, cmv (change my view), life, use, change, believe\nExploring perspectives and beliefs in political discourse to understand and potentially alter their own opinions. This topic is also seen in the LDA visualization for politics submissions.\n\n\n2\nwiden (Biden), Trump, cmv (change my view), democrat, republican, gun, school vote, party, bill, crime\nDiscussions related to political parties and the broader national context. This topic is also seen in the LDA visualization for politics submissions.\n\n\n3\nright, white, support, war, system, capitol, society, kill, russia\nDiscussions related to right-wing politics, Jan. 6 Capitol attack, and Russia-Ukraine war. This is a unique topic from previously analyzed economics and politics submissions!\n\n\n4\ncovid, law, police, ban, news, court, liberal, vaccine, joe, china, abortion, supreme, inflation, rule, freedom\nDiscussions related to left-wing politics, such as the Supreme Court overturning Roe v. Wade, COVID-19, and police brutality. Another unique topic!\n\n\n5\nnew, conservative, libertarian, socialism, twitter, death, mask, racism, drug, economy, property, musk\nA mix of different political subreddits discussing multifarious societal issues, such as social media and twitter, racism, drugs, and the economy. Another unique topic!\n\n\n\n\nTable 1: Summary of Submissions Topics Across Shared User Subreddits"
  },
  {
    "objectID": "discussion.html#nlp",
    "href": "discussion.html#nlp",
    "title": "Discussion",
    "section": "",
    "text": "No feedback was provided."
  },
  {
    "objectID": "discussion.html#ml",
    "href": "discussion.html#ml",
    "title": "Discussion",
    "section": "",
    "text": "Model performances are a concern, especially with the volatility of user interactions online.\n\nUnfortunately, our models definitely were underwhelming. However, this seems like an inevitability given the distributions of our feature variables and the skewedness of many of our target variables."
  },
  {
    "objectID": "discussion.html#website-results",
    "href": "discussion.html#website-results",
    "title": "Discussion",
    "section": "",
    "text": "Default website formatting and styling are too mundane.\n\nThe given website starter template was useful but wasn’t very aesthetic or well-organized as the project progressed. As such, we’ve added the sidebar with splits per section, and subsections for each large section if necessary. Additionally, we’ve customized the color theme, font, and other minor aesthetic features."
  },
  {
    "objectID": "eda.html#introduction",
    "href": "eda.html#introduction",
    "title": "EDA",
    "section": "",
    "text": "In the digital era, online forums like Reddit have become pivotal platforms for political and economic discourse. This exploratory data analysis (EDA) section focuses on major political and economic subreddits, aiming to uncover insights into user behavior and the dynamics of online discussions. By analyzing posting patterns, user engagement, and content trends within these communities, we seek to provide a comprehensive view of the landscape of online political discourse.\nOur methodology involves examining various aspects of subreddit activities, such as post frequency, comment counts, user interactions, and content analysis. This approach enables us to identify patterns and anomalies that may indicate broader trends in online political and economic conversations. This project is not just an exercise in data analysis; it’s an endeavor to understand the pulse of digital communities, revealing how they reflect and influence real-world events and sentiments."
  },
  {
    "objectID": "ml.html#appendix",
    "href": "ml.html#appendix",
    "title": "ML",
    "section": "",
    "text": "over_18\ncount\n\n\n\n\nfalse\n595,936\n\n\ntrue\n3,972\n\n\n\n\nAppendix 1: Value Counts of over_18\n\n\n\n \n\n\n\n\n\nis_video\ncount\n\n\n\n\nfalse\n596876\n\n\ntrue\n3032\n\n\n\n\nAppendix 2: Value Counts of is_video\n\n\n\n \n\n\n\n\nAppendix 3: Test Set Confusion Matrix - Random Forest\n\n\n\n \n\n\n\n\n\nEvaluation Metric\nValue\n\n\n\n\nTest Accuracy\n0.841\n\n\nTest Error\n0.159\n\n\nPrecision\n0.958\n\n\nRecall\n0.867\n\n\nF1-score\n0.868\n\n\n\n\nAppendix 4: Random Forest Model Test Set Metrics\n\n\n\n \n\n\n\n\nAppendix 5: Test Set Confusion Matrix - Gradient Boosted Trees\n\n\n\n \n\n\n\n\n\nEvaluation Metric\nValue\n\n\n\n\nTest Accuracy\n0.667\n\n\nTest Error\n0.333\n\n\nPrecision\n0.971\n\n\nRecall\n0.662\n\n\nF1-score\n0.749\n\n\n\n\nAppendix 6: Gradient Boosted Trees Model Test Set Metrics\n\n\n\n \n\n\n\n\nAppendix 7: Test Set Confusion Matrix - Logistic Regression\n\n\n\n \n\n\n\n\n\nEvaluation Metric\nValue\n\n\n\n\nTest Accuracy\n0.451\n\n\nTest Error\n0.549\n\n\nPrecision\n0.960\n\n\nRecall\n0.427\n\n\nF1-score\n0.561\n\n\n\n\nAppendix 8: Logistic Regression Model Test Set Metrics\n\n\n\n\n\n\n\nAppendix 9: Silhouette Plot for 4 clusters\n\n\n\n\n\n\n\nAppendix 10: Silhouette Plot for 6 clusters\n\n\n\n\n\n\n\nAppendix 11: Silhouette Plot for 8 clusters"
  },
  {
    "objectID": "nlp.html#appendix",
    "href": "nlp.html#appendix",
    "title": "NLP",
    "section": "",
    "text": "Rank\nWord\nWord Count\n\n\n\n\n1\nfintechinshorts\n7,003\n\n\n…\n…\n…\n\n\n5\nbank\n3,810\n\n\n…\n…\n…\n\n\n10\nmarket\n2,885\n\n\n11\ninflation\n2,784\n\n\n12\nstock\n2,537\n\n\n…\n…\n…\n\n\n17\nrate\n16,373\n\n\n18\nmoney\n2,121\n\n\n19\nman\n2,116\n\n\n20\nget\n2,074\n\n\n21\nprice\n1,958\n\n\n22\ngood\n1,943\n\n\n23\neconomy\n1,937\n\n\n24\nreview\n1,850\n\n\n25\nfinance\n1,790\n\n\n…\n…\n…\n\n\n30\neconomic\n1,448\n\n\n31\ncrypto\n1,429\n\n\n32\ntrade\n1,429\n\n\n33\nworld\n1,429\n\n\n…\n…\n…\n\n\n38\ncovid\n1,310\n\n\n39\nbuy\n1,301\n\n\n40\nglobal\n1,298\n\n\n41\nweakness\n1,296\n\n\n…\n…\n…\n\n\n51\nrise\n1,234\n\n\n52\nhelp\n1,225\n\n\n53\npay\n1,225\n\n\n54\nwork\n1,203\n\n\n55\ntime\n1,200\n\n\n56\nfund\n1,124\n\n\n…\n…\n…\n\n\n96\naccount\n854\n\n\n97\ngrowth\n854\n\n\n98\nfall\n851\n\n\n99\ncrisis\n846\n\n\n100\nrecession\n833\n\n\n\n\nAppendix 1: Select Top 100 Frequent Words in Economics Subreddits Submissions\n\n\n\n\n\n\n\n\n\n\nRank\nWord\nWord Count\n\n\n\n\n1\npeople\n3,916,872\n\n\n2\nlike\n2,429,385\n\n\n3\nsay\n2,388,813\n\n\n4\nget\n2,383,236\n\n\n5\nthink\n2,212,302\n\n\n…\n…\n…\n\n\n28\nstate\n938,986\n\n\n29\nwe\n927,029\n\n\n30\nyear\n828,483\n\n\n31\ngovernment\n828,131\n\n\n…\n…\n…\n\n\n48\nTrump\n655,714\n\n\n49\nactually\n652,720\n\n\n50\nlook\n650,575\n\n\n51\nvote\n644,104\n\n\n52\ncountry\n627,094\n\n\n…\n…\n…\n\n\n59\npay\n583,674\n\n\n60\npost\n573,795\n\n\n61\nlaw\n572,163\n\n\n62\nreason\n565,496\n\n\n…\n…\n…\n\n\n77\nchild\n506,417\n\n\n78\nmoney\n504,222\n\n\n79\nargument\n504,091\n\n\n80\nlive\n500,994\n\n\n…\n…\n…\n\n\n98\nyes\n433,362\n\n\n99\nwrong\n428,341\n\n\n100\nkeep\n427,201\n\n\n\n\nAppendix 2: Select Top 100 Frequent Words in Politics Subreddits Comments\n\n\n\n\n\n\n\n\n\n\nRank\nWord\nWord Count\n\n\n\n\n1\npeople\n274,244\n\n\n2\nmake\n215,993\n\n\n3\nget\n209,560\n\n\n4\ngo\n194,391\n\n\n…\n…\n…\n\n\n7\npay\n143,362\n\n\n8\nmoney\n141,992\n\n\n9\nwork\n141,738\n\n\n10\nthink\n138,345\n\n\n11\nsay\n134,141\n\n\n12\nprice\n128,054\n\n\n13\ntax\n127,177\n\n\n…\n…\n…\n\n\n19\ninflation\n110,184\n\n\n20\nrate\n109,396\n\n\n21\ntime\n106,155\n\n\n22\nmarket\n103,920\n\n\n23\neven\n103,606\n\n\n24\nhouse\n103,563\n\n\n25\nhigh\n99,101\n\n\n…\n…\n…\n\n\n37\njob\n81,568\n\n\n38\nbuy\n80,449\n\n\n39\ncompany\n79,214\n\n\n40\nway\n78,705\n\n\n41\nincrease\n78,237\n\n\n42\ngovernment\n78,082\n\n\n…\n…\n…\n\n\n52\ncountry\n64,732\n\n\n53\ninterest\n64,663\n\n\n54\nlot\n64,473\n\n\n55\nhome\n64,098\n\n\n56\neconomy\n64,068\n\n\n57\nright\n63,889\n\n\n58\nmany\n62,990\n\n\n59\nwage\n62,931\n\n\n60\nbank\n62,018\n\n\n61\nrule\n61,256\n\n\n…\n…\n…\n\n\n98\nreal\n44,515\n\n\n99\nchina\n44,042\n\n\n100\nevery\n44,012\n\n\n\n\nAppendix 3: Select Top 100 Frequent Words in Economics Subreddits Comments\n\n\n\n\n\n\n\n\n\n\nRank\nWord\nTF-IDF Value\n\n\n\n\n1\nblackfriday\n9.762917158451936\n\n\n2\ncraic\n9.762917158451936\n\n\n3\nboo\n9.762917158451936\n\n\n4\nbookings\n9.762917158451936\n\n\n5\nacn\n9.762917158451936\n\n\n6\nbookkeeper\n9.762917158451936\n\n\n7\nadm\n9.762917158451936\n\n\n8\nbrave\n9.762917158451936\n\n\n9\naerospace\n9.762917158451936\n\n\n10\nbroadcom\n9.762917158451936\n\n\n\n\nAppendix 4: Top 10 Important Words in Economics Subreddits Submissions\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\nWord\nWord Count\n\n\n\n\n1\nemancipation\n99.52226201944997\n\n\n2\nsabine\n99.44126423729803\n\n\n3\nspikeproteins\n98.70374062769325\n\n\n4\nwilleford\n98.03656936906297\n\n\n5\ntetrapod\n97.42748973013556\n\n\n6\nmikayla\n97.42748973013556\n\n\n7\nsubsource\n96.34843497134476\n\n\n8\narbitration\n95.59369519372572\n\n\n9\ndelgado\n95.06230029904857\n\n\n10\nadvocateforrightsandknowledgeofamericansarkaghostio\n94.2598337269131\n\n\n\n\nAppendix 5: Top 10 Important Words in Politics Subreddits Comments\n\n\n\n\n\n\n\nAppendix 6: Histogram of Text Length for Economics Subreddits Submissions\n\n\n\n\n\n\n\nAppendix 7: Histogram of Text Length for Politics Subreddits Comments\n\n\n\n\n\n\n\nAppendix 8: Histogram of Text Length for Economics Subreddits Comments\n\n\n\n\n\n\n\nAppendix 9: Line Chart of Number of Words Over Time in Economics and Politics Subreddits Submissions and Comments\n\n\n\n\n\n\n\n\nAppendix 10: Line Chart of Number of Words Over Time in Economics and Politics Subreddits Submissions and Comments\n\n\n\n\n\n\n\n\nAppendix 11: Line Chart of Number of Words Over Time in Economics and Politics Subreddits Submissions and Comments\n\n\n\n\n\n\n\n\nAppendix 12: Line Chart of Number of Words Over Time in Economics and Politics Subreddits Submissions and Comments"
  },
  {
    "objectID": "index.html#about-the-team",
    "href": "index.html#about-the-team",
    "title": "Introduction",
    "section": "",
    "text": "Raunak Advani\n\n\n\n\n\nTegveer Ghura\n\n\n\n\n\nEric Lim\n\n\n\n\n\nAnthony Moubarak"
  },
  {
    "objectID": "business_goals.html#acknowledgment",
    "href": "business_goals.html#acknowledgment",
    "title": "Business Goals",
    "section": "",
    "text": "We would like to disclose that we employed Grammarly, Inc. to assist with grammar and proofreading for this section."
  },
  {
    "objectID": "index.html#acknowledgment",
    "href": "index.html#acknowledgment",
    "title": "Introduction",
    "section": "",
    "text": "We would like to disclose that we employed Grammarly, Inc. to assist with grammar and proofreading for this section."
  }
]